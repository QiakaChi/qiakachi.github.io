![](https://cdn.nlark.com/yuque/0/2025/png/39039688/1756188334137-779c69de-bd11-4627-bda3-8351c18a08c0.png)

# Agent-Q
这篇论文的agent，不是传统的 Observation-Thinker-Actor 划分，而是 Observation-Actor（包括 Plan-Thought-Env Action-Explanation）。

模型可视为 POMDP（部分可观测马尔可夫决策过程）：(O, S, A, T, R, μ₀, γ)。

## Obervation（O）
+ **对象**：用户指令和浏览器状态（HTML DOM）。

## Actor（A）
 论文将计划（plan）、推理（thought）、环境动作（env action）和解释（explanation）这四个部分统一作为 composite action。训练时**LLM会同时学习这四个子输出**。

+ **MCTS 的搜索空间**：只对 env action 做树状探索和评估。  
+ **DPO 优化的数据**：是基于 MCTS 轨迹得到的偏好对，但在训练时会一起优化 plan / thought / explanation，因为它们被建模为和 env action 同属于一个 composite action。  

### Plan
+ 初始步骤的顺序执行计划。 仅在**初始观察**之后的第一个动作会生成。

### Thought
+ 在后续的每一步，模型都会生成一个类似CoT的推理步骤（`atht`），这相当于一个内部的思考过程，解释了为什么选择这个动作。 

### **Env Action**
+  唯一**直接与环境（浏览器）交互**的部分， 包括点击网页元素（CLICK）、输入文本（TYPE）、滚动页面（SCROLL）、向用户请求信息（ASK USER）等。

### Explanation
+  在执行完环境动作之后生成。给出对刚才动作的解释，用于表示 agent 的意图和当前状态。虽然不影响环境，但会影响之后的决策。  

## 图表说明
![图2：我们向智能体提供以下输入格式，包括系统提示、执行历史、当前观察（以DOM表示）以及包含目标的用户查询。我们将智能体的输出格式分为一个总体的分步计划、思考、命令和状态代码。](https://cdn.nlark.com/yuque/0/2025/png/39039688/1756204206086-59dd04f0-5fef-4b03-ac46-58a4ececcef5.png)

<details class="lake-collapse"><summary id="u838da6cf"><span class="ne-text">【AI生成】图2解释</span></summary><p id="u640ba4ce" class="ne-p"><span class="ne-text">这张图是理解Agent Q智能体</span><strong><span class="ne-text">在推理/测试阶段（Inference Phase）如何工作</span></strong><span class="ne-text">的关键。它清晰地展示了智能体的“输入-输出”流程，即LLM如何根据当前环境信息进行推理并生成可执行的动作。</span></p><h4 id="sIFbg"><strong><span class="ne-text">左侧：智能体输入 (Agent Input)</span></strong></h4><p id="u64f7ca14" class="ne-p"><span class="ne-text">这是智能体在做出决策前接收到的所有信息。这些信息共同构成了其“上下文”或“记忆”。</span></p><ol class="ne-ol"><li id="u7857cd58" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">&lt;SYSTEM_PROMPT&gt;</span></code><strong><span class="ne-text"> (系统提示)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="uc5634cc2" data-lake-index-type="0"><span class="ne-text">这是一个固定的、预先设定的指令，用于指导智能体的整体行为和角色。例如，它可能告诉模型：“你是一个专业的餐厅预订助手，需要遵循严格的步骤来完成任务。”虽然图中没有显示具体内容，但它是模型行为的基础。</span></li></ul></ul><ol start="2" class="ne-ol"><li id="uaa048fc7" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">&lt;EXECUTION HISTORY&gt;</span></code><strong><span class="ne-text"> (执行历史)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="ueeac0431" data-lake-index-type="0"><span class="ne-text">这是智能体到目前为止已经采取的所有动作的记录。它包含了之前的状态和操作序列。</span></li><li id="u97cd7e3b" data-lake-index-type="0"><span class="ne-text">在图中，它被简化为一个文本块，其中包含了</span><strong><span class="ne-text">用户查询</span></strong><span class="ne-text">（USER QUERY）和</span><strong><span class="ne-text">当前观察</span></strong><span class="ne-text">（CURRENT OBSERVATION）。</span></li><li id="ufc749b41" data-lake-index-type="0"><span class="ne-text">用户查询：</span><code class="ne-code"><span class="ne-text">Book a reservation for the restaurant Coconi's on OpenTable for 2 people on June 17 2024 at 7:00pm</span></code><span class="ne-text">。这是智能体需要完成的最终目标。</span></li><li id="u2b9d6697" data-lake-index-type="0"><span class="ne-text">当前观察：</span><code class="ne-code"><span class="ne-text">CURRENT OBSERVATION:</span></code><span class="ne-text"> 后面跟着一个网页截图。这代表了智能体当前所处的环境状态——在OpenTable网站上，位于Coconi's餐厅的页面。</span></li></ul></ul><ol start="3" class="ne-ol"><li id="u84a9a8b6" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">CURRENT OBSERVATION:</span></code><strong><span class="ne-text"> (当前观察)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="uc90042bb" data-lake-index-type="0"><span class="ne-text">这是智能体感知到的</span><strong><span class="ne-text">实时环境状态</span></strong><span class="ne-text">。</span></li><li id="u041cdde6" data-lake-index-type="0"><span class="ne-text">根据论文第3.1节，这个观察是以</span><strong><span class="ne-text">HTML DOM格式</span></strong><span class="ne-text">表示的。图中的截图就是DOM的一个可视化呈现。</span></li><li id="u9a49ea1f" data-lake-index-type="0"><span class="ne-text">它包含了所有与任务相关的视觉和交互元素，如餐厅名称、日期选择器、时间选择器、人数选择器、按钮等。智能体需要解析这个DOM来理解环境。</span></li></ul></ul><h4 id="xQ2uo"><strong><span class="ne-text">右侧：智能体输出 (Agent Output)</span></strong></h4><p id="uad960308" class="ne-p"><span class="ne-text">这是智能体基于输入信息进行推理后生成的响应。它是一个结构化的、复合的动作，旨在指导执行器（Executor）。</span></p><ol class="ne-ol"><li id="udb97a8ad" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">PLAN:</span></code><strong><span class="ne-text"> (计划)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="uadb1d440" data-lake-index-type="0"><span class="ne-text">这是智能体为完成整个任务而制定的</span><strong><span class="ne-text">宏观策略</span></strong><span class="ne-text">。</span></li><li id="u6feb126e" data-lake-index-type="0"><span class="ne-text">它是一个分步列表，列出了从当前状态到最终成功所需执行的一系列主要步骤。</span></li><li id="u68cfe763" data-lake-index-type="0"><span class="ne-text">在图中，计划是：</span></li></ul></ul><ol class="ne-list-wrap"><ol class="ne-list-wrap"><ol ne-level="2" class="ne-ol"><li id="ud5dc8b52" data-lake-index-type="0"><span class="ne-text">选择预订日期（2024年5月22日）。</span></li><li id="ua7b7bb06" data-lake-index-type="0"><span class="ne-text">选择预订时间（晚上7点）。</span></li><li id="u2744d97f" data-lake-index-type="0"><span class="ne-text">选择用餐人数（4人）。</span></li><li id="ud43d7593" data-lake-index-type="0"><span class="ne-text">点击“查找餐桌”按钮完成预订。</span></li></ol></ol></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u9cb0ff25" data-lake-index-type="0"><span class="ne-text">这个计划是</span><strong><span class="ne-text">高层级的</span></strong><span class="ne-text">，它为后续的思考和具体动作提供了方向。</span></li></ul></ul><ol start="2" class="ne-ol"><li id="u908f34d8" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">THOUGHT:</span></code><strong><span class="ne-text"> (思考)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u1e5f6717" data-lake-index-type="0"><span class="ne-text">这是智能体在执行计划时的</span><strong><span class="ne-text">内部推理过程</span></strong><span class="ne-text">。</span></li><li id="u5c0abde6" data-lake-index-type="0"><span class="ne-text">它解释了为什么选择当前的行动，以及对当前状态的理解。</span></li><li id="ufa873d0e" data-lake-index-type="0"><span class="ne-text">在图中，思考是：</span></li></ul></ul><div class="ne-quote"><p id="u02b53b5e" class="ne-p"><span class="ne-text">“我目前在OpenTable上的Coconi's餐厅页面，我需要选择预订的日期和时间，并选择用餐人数。我将专注于选择日期、时间和用餐人数。”</span></p></div><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u49863eae" data-lake-index-type="0"><span class="ne-text">这个思考是</span><strong><span class="ne-text">低层级的</span></strong><span class="ne-text">，它聚焦于当前步骤的细节。它表明模型正在分析当前的DOM，识别出需要操作的元素（日期、时间、人数），并决定优先处理日期。</span></li></ul></ul><ol start="3" class="ne-ol"><li id="uf4d25c6e" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">COMMANDS:</span></code><strong><span class="ne-text"> (命令)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u9e1c3d57" data-lake-index-type="0"><span class="ne-text">这是智能体生成的</span><strong><span class="ne-text">具体、可执行的指令</span></strong><span class="ne-text">，也是整个输出中最关键的部分。</span></li><li id="u3bb59793" data-lake-index-type="0"><span class="ne-text">它直接传递给执行器（Executor）去操作浏览器。</span></li><li id="ubd312391" data-lake-index-type="0"><span class="ne-text">在图中，命令是：</span></li></ul></ul><div class="ne-quote"><p id="ufe3e8f2c" class="ne-p"><code class="ne-code"><span class="ne-text">CLICK &lt;select&gt;Date&lt;/select&gt;</span></code></p></div><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="ud714f00b" data-lake-index-type="0"><span class="ne-text">这个命令明确指出了要执行的操作（</span><code class="ne-code"><span class="ne-text">CLICK</span></code><span class="ne-text">）和要点击的目标（</span><code class="ne-code"><span class="ne-text">&lt;select&gt;Date&lt;/select&gt;</span></code><span class="ne-text">）。这里的</span><code class="ne-code"><span class="ne-text">&lt;select&gt;Date&lt;/select&gt;</span></code><span class="ne-text">是DOM中代表“日期选择器”的元素的HTML标签和ID（或类似标识符）。</span></li><li id="u914d5d93" data-lake-index-type="0"><span class="ne-text">执行器会解析这个命令，在浏览器中找到对应的元素并触发点击事件。</span></li></ul></ul><ol start="4" class="ne-ol"><li id="ucf62140b" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">STATUS:</span></code><strong><span class="ne-text"> (状态码)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u7031f68b" data-lake-index-type="0"><span class="ne-text">这是一个简单的状态指示，告诉系统下一步该做什么。</span></li><li id="u8fc5d380" data-lake-index-type="0"><span class="ne-text">在图中，状态码是 </span><code class="ne-code"><span class="ne-text">CONTINUE</span></code><span class="ne-text">，意味着任务仍在进行中，需要继续下一个步骤。</span></li><li id="u4d6be41a" data-lake-index-type="0"><span class="ne-text">其他可能的状态码可能是 </span><code class="ne-code"><span class="ne-text">SUCCESS</span></code><span class="ne-text">（任务成功）、</span><code class="ne-code"><span class="ne-text">FAILURE</span></code><span class="ne-text">（任务失败）或 </span><code class="ne-code"><span class="ne-text">ASK_USER</span></code><span class="ne-text">（需要用户介入）。</span></li></ul></ul><h4 id="kko35"><strong><span class="ne-text">总结</span></strong></h4><p id="u2cdfaef6" class="ne-p"><span class="ne-text">图2的核心意义在于，它展示了一个</span><strong><span class="ne-text">结构化、可解释的AI Agent架构</span></strong><span class="ne-text">：</span></p><ul class="ne-ul"><li id="u9d2176fe" data-lake-index-type="0"><strong><span class="ne-text">输入</span></strong><span class="ne-text">：智能体接收一个完整的上下文，包括长期目标（用户查询）、历史记录和当前环境快照（DOM）。</span></li><li id="u7605945c" data-lake-index-type="0"><strong><span class="ne-text">输出</span></strong><span class="ne-text">：智能体通过多层推理，生成一个包含</span><strong><span class="ne-text">计划</span></strong><span class="ne-text">（宏观）、</span><strong><span class="ne-text">思考</span></strong><span class="ne-text">（微观）和</span><strong><span class="ne-text">命令</span></strong><span class="ne-text">（执行）的复合输出。</span></li><li id="u2d012f92" data-lake-index-type="0"><strong><span class="ne-text">作用</span></strong><span class="ne-text">：这种结构化输出使得智能体的行为变得</span><strong><span class="ne-text">透明和可控</span></strong><span class="ne-text">。研究人员可以轻松地检查每个部分（计划、思考、命令）来调试和理解模型的决策过程。同时，</span><code class="ne-code"><span class="ne-text">COMMANDS</span></code><span class="ne-text"> 部分确保了与真实世界的接口是标准化和可执行的。</span></li></ul></details>

# 训练阶段
## MCTS（Monte Carlo Tree Search）
### 为什么使用MCTS
只使用DPO进行实验后发现：DPO模型在WebShop中表现出贪婪搜索行为（几乎不使用“下一页”按钮进行探索）。为此引入MCTS引导Agent进行探索，

### MCTS工作原理
+ **作用**：在训练阶段作为“数据生成器”（每个节点计算Q值），引导Agent在环境中探索。
+ **不调整参数**：MCTS本身不直接更新模型参数，而是为DPO提供训练数据。
+ **Q值计算**：
    - MCTS本身的Q值更新：![image](https://cdn.nlark.com/yuque/__latex/b86110638bbec1449abd3a91638f80f1.svg)
    - AI过程监督产生的Q值：![image](https://cdn.nlark.com/yuque/__latex/bf6a5ddabb93fe57fd4551a901256cce.svg)（文中未给出具体的计算公式）

#### 选择
+ **UCB1**：
    - UCB1平衡探索与利用。
    - 最终Q值将代入UCB1公式![image](https://cdn.nlark.com/yuque/__latex/3292d28e74b12c68cca1f550007183be.svg)
    - 使用UCB1公式选择节点（动作）并执行，到达新节点（网页）。
+ **AI过程监督**：
    - 由于网页环境奖励稀疏，引入一个基于LLM的“批评模型”（critic）。
    - 该模型对当前节点下可能的K个动作进行**打分和排名**，提供中间奖励（intermediate reward）。
    - 解决了信用分配（credit assignment）问题，指导搜索走向更优路径。
    - 生成![image](https://cdn.nlark.com/yuque/__latex/bd24ae7d139a07a4e2263a809667c4c2.svg)。

##### 图表说明
![](https://cdn.nlark.com/yuque/0/2025/png/39039688/1756204786109-262196d9-1ce5-4c3c-b44b-b99479c52bfe.png)

<details class="lake-collapse"><summary id="ub0be35df"><span class="ne-text">【AI生成】图4解释</span></summary><p id="u101db347" class="ne-p"><span class="ne-text">这张图是理解Agent Q框架中</span><strong><span class="ne-text">AI过程监督</span></strong><span class="ne-text">（AI Process Supervision）这一核心创新的关键。它展示了在MCTS的“选择”（Selection）阶段，如何利用一个“批评模型”（LLM Critic）来解决网页环境中的稀疏奖励问题。</span></p><h4 id="ykWAU"><strong><span class="ne-text">整体流程概览</span></strong></h4><p id="uee2465f9" class="ne-p"><span class="ne-text">图4描绘了一个循环过程：</span></p><ol class="ne-ol"><li id="ud0883164" data-lake-index-type="0"><strong><span class="ne-text">策略（Actor）提出动作</span></strong><span class="ne-text">：在当前网页状态（Agent Input）下，主模型（LLM Actor）生成多个可能的下一步动作。</span></li><li id="uff53a399" data-lake-index-type="0"><strong><span class="ne-text">批评模型（Critic）进行排序</span></strong><span class="ne-text">：另一个独立的模型（LLM Critic）对这些动作进行评估和排序，判断哪个动作更有可能成功。</span></li><li id="u57f58510" data-lake-index-type="0"><strong><span class="ne-text">利用排序结果</span></strong><span class="ne-text">：这个排序结果被用来指导MCTS的后续步骤，既在</span><strong><span class="ne-text">探索时</span></strong><span class="ne-text">（inference-time search）帮助选择更优路径，也在</span><strong><span class="ne-text">训练时</span></strong><span class="ne-text">（policy training）用于构建数据。</span></li></ol><h4 id="gy7kq"><strong><span class="ne-text">详细分解</span></strong></h4><ol class="ne-ol"><li id="udad3f218" data-lake-index-type="0"><strong><span class="ne-text">左侧：Agent Input (智能体输入)</span></strong></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="uef2a76a0" data-lake-index-type="0"><span class="ne-text">这是与图2相同的输入格式，包含了用户查询（</span><code class="ne-code"><span class="ne-text">Book a reservation for the restaurant Fogo de Chao...</span></code><span class="ne-text">）和当前观察到的OpenTable主页截图。</span></li><li id="uc4cd20f1" data-lake-index-type="0"><span class="ne-text">当前状态是：用户需要在OpenTable上找到特定餐厅，但尚未输入任何信息。</span></li></ul></ul><ol start="2" class="ne-ol"><li id="uc504307b" data-lake-index-type="0"><strong><span class="ne-text">中间：LLM Actor (主模型/策略)</span></strong></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="ufd552770" data-lake-index-type="0"><span class="ne-text">这是执行任务的“大脑”，即经过初步训练的LLM。</span></li><li id="u042039b8" data-lake-index-type="0"><span class="ne-text">它根据输入，生成了三个可能的下一步动作（</span><code class="ne-code"><span class="ne-text">Proposed Action 1</span></code><span class="ne-text">, </span><code class="ne-code"><span class="ne-text">Proposed Action 2</span></code><span class="ne-text">, </span><code class="ne-code"><span class="ne-text">Proposed Action 3</span></code><span class="ne-text">）：</span></li></ul></ul><ul class="ne-list-wrap"><ul class="ne-list-wrap"><ul ne-level="2" class="ne-ul"><li id="ue24beac9" data-lake-index-type="0"><strong><span class="ne-text">Action 1 (橙色)</span></strong><span class="ne-text">：点击日期选择器。这是一个合理的动作，但不够具体。</span></li><li id="uc775b561" data-lake-index-type="0"><strong><span class="ne-text">Action 2 (绿色)</span></strong><span class="ne-text">：在搜索栏中输入“Terra - Eataly”。这是一个非常直接且高效的行动，能快速定位目标餐厅。</span></li><li id="u0f3d2ee4" data-lake-index-type="0"><strong><span class="ne-text">Action 3 (红色)</span></strong><span class="ne-text">：导航回OpenTable首页。这是一个错误的动作，因为当前已经在主页上，这会浪费步骤。</span></li></ul></ul></ul><ol start="3" class="ne-ol"><li id="ub369e6e3" data-lake-index-type="0"><strong><span class="ne-text">右侧：LLM Critic (批评模型)</span></strong></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u4d5d88fd" data-lake-index-type="0"><span class="ne-text">这是“自我监督”的关键。它是一个</span><strong><span class="ne-text">与主模型（Actor）完全相同的基础LLM模型</span></strong><span class="ne-text">（例如，都是LLaMA-3 70B）。它不负责执行任务，只负责“思考”和“评判”。</span></li><li id="u4fb8bf56" data-lake-index-type="0"><span class="ne-text">它接收主模型提出的三个动作，并基于其自身的“直觉”对它们进行分析和排序。</span></li><li id="u5603b4e8" data-lake-index-type="0"><strong><span class="ne-text">批评模型的思考</span></strong><span class="ne-text">（灰色框）：</span></li></ul></ul><div class="ne-quote"><p id="u53ffa838" class="ne-p"><span class="ne-text">“在分析当前浏览器状态后，我注意到我们正在OpenTable网站上。此页面显示了带有可用预订时间的餐厅列表。页面上的相关元素是……”<br /></span><span class="ne-text">“最有可能成功的命令是在搜索栏中输入搜索词‘Terra - Eataly’。”</span></p></div><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u7147fa5b" data-lake-index-type="0"><span class="ne-text">这个思考过程表明，批评模型能够理解当前环境（在主页上，有搜索栏），并能评估不同动作的合理性。它认为</span><strong><span class="ne-text">Action 2</span></strong><span class="ne-text"> 是最佳选择。</span></li></ul></ul><ol start="4" class="ne-ol"><li id="u6575e193" data-lake-index-type="0"><strong><span class="ne-text">结果：动作排序与反馈</span></strong></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="uf7c0a3b2" data-lake-index-type="0"><span class="ne-text">批评模型的判断结果体现在</span><strong><span class="ne-text">右侧的重新排列</span></strong><span class="ne-text">上：</span></li></ul></ul><ul class="ne-list-wrap"><ul class="ne-list-wrap"><ul ne-level="2" class="ne-ul"><li id="u5a636958" data-lake-index-type="0"><strong><span class="ne-text">Action 2 (绿色)</span></strong><span class="ne-text"> 被排在第一位，因为它被判定为“最有可能成功的命令”。</span></li><li id="ucd8ef47a" data-lake-index-type="0"><strong><span class="ne-text">Action 1 (橙色)</span></strong><span class="ne-text"> 被排在第二位。</span></li><li id="u8eced427" data-lake-index-type="0"><strong><span class="ne-text">Action 3 (红色)</span></strong><span class="ne-text"> 被排在第三位，因为它是一个无效或低效的动作。</span></li></ul></ul></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="ufe45a758" data-lake-index-type="0"><span class="ne-text">这个排序就是所谓的“</span><strong><span class="ne-text">AI过程监督</span></strong><span class="ne-text">”（AI Process Feedback）。它为每个动作提供了一个</span><strong><span class="ne-text">中间奖励</span></strong><span class="ne-text">（intermediate reward），即使该动作最终没有导致任务成功。</span></li></ul></ul><h4 id="Z7PoA"><strong><span class="ne-text">核心意义与应用</span></strong></h4><ul class="ne-ul"><li id="uc61f3294" data-lake-index-type="0"><strong><span class="ne-text">解决稀疏奖励问题</span></strong><span class="ne-text">：在真实的网页环境中，只有在任务完成时才能获得“成功/失败”的最终奖励。如果一个动作（如Action 3）在早期就犯了错误，整个轨迹都会失败，模型无法知道是哪个具体的动作导致了失败。AI过程监督通过批评模型提供的即时反馈，解决了这个问题。它告诉MCTS：“虽然你没成功，但你的Action 2是正确的，而Action 3是错的。”</span></li><li id="uee5ba3b8" data-lake-index-type="0"><strong><span class="ne-text">指导MCTS搜索</span></strong><span class="ne-text">：在MCTS的“选择”阶段，这个排序可以作为启发式信息，优先探索那些被批评模型评为“好”的动作路径，从而加速搜索过程。</span></li><li id="u34aa9121" data-lake-index-type="0"><strong><span class="ne-text">构建训练数据</span></strong><span class="ne-text">：在训练阶段，这个排序结果被用来计算一个加权的Q值（见公式(10)），以构建用于DPO训练的“偏好对”。例如，在某个节点上，如果Action 2被排在第一，而Action 3被排在最后，那么这对动作就可以构成一个“优选” vs “劣选”的对比对，用于训练主模型。</span></li></ul><p id="ua2077dfd" class="ne-p"><span class="ne-text">总而言之，图4生动地展示了Agent Q如何利用一个</span><strong><span class="ne-text">相同的、强大的LLM作为“自我批评者”</span></strong><span class="ne-text">，来为复杂的网页交互任务提供细粒度的、实时的反馈，从而极大地提升了学习效率和最终性能。</span></p></details>

#### 扩展
+ **动作生成**：使用基础LLM策略（`πθ`）在当前节点（网页状态）上采样K个可能的动作（如 `CLICK[ELEMENT ID]`, `TYPE[CONTENT]`）。
+ **执行**：选择一个动作在浏览器环境中执行，进入下一个网页状态，创建新的子节点。

#### 模拟
+ **过程**：从新创建的叶节点开始，使用当前的策略模型 `πθ` 进行rollout，直到任务成功或失败。
+ **目的**：快速评估该分支的最终奖励R（成功时R=1，失败时R=0）。

#### 回溯
+ **更新**：将模拟得到的最终奖励（R=1/0）从叶节点反向传播到根节点。
+ **更新内容**：更新路径上每个 (state, action) 对的访问次数![image](https://cdn.nlark.com/yuque/__latex/bf090727f949cab997be83612c034605.svg)和MCTS的平均![image](https://cdn.nlark.com/yuque/__latex/1fb9da5332b0e6cc22f9870dbc3b0f57.svg)：
    - ![image](https://cdn.nlark.com/yuque/__latex/b86110638bbec1449abd3a91638f80f1.svg)
    - ![image](https://cdn.nlark.com/yuque/__latex/942e97d96b5d3d83d1ea4b9c744a7dac.svg)

## DPO（Direct Preference Optimization）
### 为什么选用DPO
+ 强化微调 (Reinforced Fine-Tuning, RFT)简单但性能通常低于标准RL。
+ DPO是经典RLHF优化流程的一种离线RL替代方案，无需在线采样。

### DPO工作原理
+ **作用**：训练时，LLM会通过微调（fine-tuning）的方式，调整其所有参数 `θ`，朝着使损失函数 DPO loss 最小化的方向进行优化。
+ **范围**：DPO loss 是对整个![image](https://cdn.nlark.com/yuque/__latex/c61a8b387e1cb6c40608f4ae65d6f6a6.svg) 的 log-likelihood（plan/thought/env/expl 部分全部拼在一起）做梯度更新的，因此虽然MCTS的数据针对env action，但DPO最终可以调整LLM的plan、thought和explanation。
+ **数据来源**：使用MCTS生成的轨迹，构造偏好对（preferred vs. dispreferred）。

#### 训练时优化的不是单独的Env Action
被训练的策略模型（policy）即 LLM with parameters，输出是一个完整的 step action ![image](https://cdn.nlark.com/yuque/__latex/c61a8b387e1cb6c40608f4ae65d6f6a6.svg) ：

+ 第一步：![image](https://cdn.nlark.com/yuque/__latex/3102b155970f1f5aa975f9ac0ae6acfd.svg)
+ 之后步骤：![image](https://cdn.nlark.com/yuque/__latex/194baee433b44914be17dcdb48cd0cf4.svg)

在优化时，DPO loss 是对整个 ![image](https://cdn.nlark.com/yuque/__latex/c61a8b387e1cb6c40608f4ae65d6f6a6.svg) 的 log-likelihood（plan/thought/env/expl 部分全部拼在一起）做梯度更新的。

##### 公式<font style="color:rgb(28, 31, 35);"> (1) </font>说明
+ 公式<font style="color:rgb(28, 31, 35);"> (1) </font>是论文中用于定义**复合动作**（composite action）的联合似然性（joint likelihood）的核心，解释了模型![image](https://cdn.nlark.com/yuque/__latex/f3c3603e6d7e76a264931deaf6f8087f.svg)在给定历史![image](https://cdn.nlark.com/yuque/__latex/ca57d16acf441271e4e8283f344a3f70.svg)的情况下，生成一个完整动作![image](https://cdn.nlark.com/yuque/__latex/a91c1b75795cab51f42aa19035aa8324.svg)的概率是如何计算的。
+ 该公式的结果![image](https://cdn.nlark.com/yuque/__latex/ecaf8eacd3fc55d2b47aafcf8f583c40.svg)将在后续的DPO优化中使用。

<details class="lake-collapse"><summary id="u4428fbe9"><span class="ne-text">【相关原文】</span><span class="ne-text" style="color: rgb(0,0,0); font-size: 16px">3.1. Agent Formulation, page 6</span></summary><div class="ne-quote"><p id="u9453dfd4" class="ne-p"><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">We denote the step action </span><span id="iErDj" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/bc182742c2b1bc1b41fbfb75b8a65514.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px"> as a tuple of plan, thought, environment and explanation actions for the first step and thought, environment and explanation actions for subsequent steps. When optimizing models we consider the joint likelihood </span><span id="aJo35" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/59321d5d413c91083bcc2a8cbf1d72cd.svg"></span></p><p id="u13d34b08" class="ne-p"><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">for the initial action and </span><span id="yMMEN" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/abccd6e5bd0f8077568899bd3cf238e2.svg"></span></p><p id="ue9a5fc96" class="ne-p"><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">for subsequent actions, unlike some prior works Zhai et al. (2024), which down-weight the reasoning likelihood.</span></p></div></details>
1. 初始动作的联合似然 (Initial Action)

<font style="color:rgb(28, 31, 35);">对于第一步（t=1），模型需要同时生成四个组件。公式 (1) 将这四个部分的似然性相加：</font>

![image](https://cdn.nlark.com/yuque/__latex/f83c8830518c028419e4537f7687ca53.svg)

<details class="lake-collapse"><summary id="u99898270"><span class="ne-text" style="font-size: 16px">t=1时似然公式的参数说明</span></summary><ul class="ne-ul"><li id="uf41c05ce" data-lake-index-type="0"><span id="PBmNh" class="ne-math" style="font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/55f656d5b9c2f0412c30fc94a120ee1b.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">: </span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="uc0524659" data-lake-index-type="0"><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">生成</span><strong><span class="ne-text">计划</span></strong><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">（Plan）的概率。</span></li><li id="ucfab8118" data-lake-index-type="0"><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">在第一步，模型根据用户指令和初始网页状态</span><span id="Yovuq" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/a20b4773e10b72f1fb7c7b6a27d9a6df.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">，生成一个高层次的、分步骤的任务执行方案。</span></li></ul></ul><ul class="ne-ul"><li id="u7dde6514" data-lake-index-type="0"><span id="qFNzs" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/8b7dac22fa73b0df859f29be0a4f3193.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">：</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u59fdbcc7" data-lake-index-type="0"><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">生成</span><strong><span class="ne-text">推理</span></strong><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">（Thought）的概率。</span></li><li id="u51e331b3" data-lake-index-type="0"><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">模型在知道计划后，生成下一步的具体思考过程，解释为什么选择某个动作。</span></li></ul></ul><ul class="ne-ul"><li id="u8d573d02" data-lake-index-type="0"><span id="bOQwP" class="ne-math" style="font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/7153af9a9a730b856aa93e1597f3976c.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">：</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u4e899019" data-lake-index-type="0"><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">生成</span><strong><span class="ne-text">环境动作</span></strong><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">（Environment Action）的概率。</span></li><li id="u12ce523c" data-lake-index-type="0"><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">这是真正与浏览器交互的指令（如 </span><code class="ne-code"><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">&quot;CLICK[123]&quot;</span></code><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">）。它的生成依赖于当前的状态</span><span id="zmmDn" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/a20b4773e10b72f1fb7c7b6a27d9a6df.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">、已生成的推理 </span><span id="YXZa9" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/5507d98423209ae86cb23c8082d3ace2.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">和计划</span><span id="XsD4V" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/6fcf06795aafc2628ceb4b78c41af110.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">。</span></li></ul></ul><ul class="ne-ul"><li id="u80183486" data-lake-index-type="0"><span id="sEfNL" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/eefdf3a4bd33b821c613e4acae283dcc.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">：</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="uda13f8f4" data-lake-index-type="0"><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">生成</span><strong><span class="ne-text">解释</span></strong><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">（Explanation）的概率。</span></li><li id="uca628dcc" data-lake-index-type="0"><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">模型在生成了环境动作后，会提供一个解释，说明这个动作的意图是什么。</span></li></ul></ul></details>

> 这里使用的是对数似然性 (log π)，而不是直接的似然性 (π)。这是为了数值稳定性和方便数学运算（将乘法转换为加法）。
>

2. **后续动作的联合似然 (Subsequent Actions)**

<font style="color:rgb(28, 31, 35);">对于第 t 步（t > 1），模型不再需要生成“计划”，因为它已经有一个初步的计划。因此，后续动作的联合似然性只包含三个部分：</font>

![image](https://cdn.nlark.com/yuque/__latex/25010f50aabaae172a47938f8db28c5f.svg)

<details class="lake-collapse"><summary id="u97084ef1"><span class="ne-text" style="font-size: 16px">t&gt;1时似然公式的参数说明</span></summary><ul class="ne-ul"><li id="u60e72b3e" data-lake-index-type="0"><span id="OTnir" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/7a5910184dfc77d17f9b0ec3fa0cf07e.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">：根据当前的历史</span><span id="xyQMj" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/8525a674a3e1a1e11cff744db4ae150f.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">（包括之前的动作和当前的网页状态），生成当前步骤的</span><strong><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">thought</span></strong><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">。 </span></li><li id="ud1d4f919" data-lake-index-type="0"><span id="NvifE" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/3508bfc38711ac1c5af7f71c4437d15b.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">：根据当前历史</span><span id="NSbYe" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/ca57d16acf441271e4e8283f344a3f70.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">和刚刚生成的推理</span><span id="Yrtpq" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/0c817eac39997f4d7257c81be6432c2a.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">，生成</span><strong><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">env action</span></strong><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">。 </span></li><li id="u4189e91b" data-lake-index-type="0"><span id="Hkg2e" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/0605562371d29a74b6eef27de343dd44.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">$: 根据当前历史</span><span id="oJrI8" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/ca57d16acf441271e4e8283f344a3f70.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">、生成的环境动作</span><span id="YdFDK" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/b89a6102a8b369c36440f13496c0d0c3.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">和推理</span><span id="L4AWq" class="ne-math" style="color: rgb(28, 31, 35); font-size: 16px"><img src="https://cdn.nlark.com/yuque/__latex/431d7167a9911835cb074153ada1e2c4.svg"></span><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">，生成</span><strong><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">explanation</span></strong><span class="ne-text" style="color: rgb(28, 31, 35); font-size: 16px">。</span></li></ul></details>

#### 从env action的偏好传导到其他部分
MCTS的偏好对针对于env action候选；在此基础上，DPO从env action偏好传导到其他部分。

1. **数据构造**
    - MCTS rollout 给出的是“(history ![image](https://cdn.nlark.com/yuque/__latex/ca57d16acf441271e4e8283f344a3f70.svg), env action ![image](https://cdn.nlark.com/yuque/__latex/58611e81f48c0c52f4a66e1324e3d7bb.svg)) 的偏好”。
    - 但训练时我们不单独输入 env action，而是把 **完整的 composite action**（包含当时生成的 plan/thought/env/expl）当作候选轨迹。
2. **DPO Loss 计算**
+ 单步DPO：
    - ![image](https://cdn.nlark.com/yuque/__latex/f783f2f922a1655c9ef329f8424452ff.svg)

<details class="lake-collapse"><summary id="ucd70aea2"><span class="ne-text">单步 DPO loss 参数说明</span></summary><ul class="ne-ul"><li id="u889a90c1" data-lake-index-type="0"><span id="WCHbf" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/79e8b7111590579ef5d47c021221d73a.svg"></span><span class="ne-text">：训练时要最小化这个损失函数。</span></li><li id="ude84e9f6" data-lake-index-type="0"><span id="vGqls" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/7fbcd62054bd83770c4508841ae9c9a4.svg"></span><span class="ne-text">：被训练的策略模型（policy），即 LLM with parameters。</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="uf7cd4687" data-lake-index-type="0"><span id="FlSR6" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/ed5a4aa5e092e303a69c608582c70db9.svg"></span><span class="ne-text">：被训练的参数，它输出给定历史</span><span id="MJwu8" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/ec566ee46ac6807dbae34c04467b725b.svg"></span><span class="ne-text">下，生成某个 composite action </span><span id="SVHlT" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/2097518d227bec7bfc7b6d82278fdba1.svg"></span><span class="ne-text">的概率。</span></li></ul></ul><ul class="ne-ul"><li id="u69951107" data-lake-index-type="0"><span id="nizGW" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/7080b878ae30a6cc7a7433613ebd0b2d.svg"></span><span class="ne-text">：参考模型（reference policy）。通常就是初始的 SFT 模型，用来稳定训练，防止策略无限制偏离原始分布。</span></li><li id="u5af2de1d" data-lake-index-type="0"><span id="JbNjB" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/5903f53f4f8c62f99df77e4e3ccbc4e3.svg"></span><span class="ne-text">：数据集，包含很多个偏好对 (preference pairs)。每个样本形如 </span><span id="FdtXX" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/1bf96e3d27cff5818277b1175d45681d.svg"></span><span class="ne-text">。</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u114f3825" data-lake-index-type="0"><span id="lWJrW" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/ec566ee46ac6807dbae34c04467b725b.svg"></span><span class="ne-text">：当时的历史（环境状态 + 已有动作）。</span></li><li id="u218e346e" data-lake-index-type="0"><span id="XWnYp" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/f79a6111a6ed5f7107289647dd62cb3f.svg"></span><span class="ne-text">：较优的动作（来自 MCTS Q 值高的 env action 轨迹）。</span></li><li id="ud805af8a" data-lake-index-type="0"><span id="VzJaB" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/4d9869b1f130813ae03e427bc797dc43.svg"></span><span class="ne-text">：较劣的动作。</span></li></ul></ul><ul class="ne-ul"><li id="u6a77cd52" data-lake-index-type="0"><span id="XSix1" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/cd931d3eba9b88130de39881cc91f803.svg"></span><span class="ne-text">：对数据集里的所有偏好对取平均（数学上的“期望”）。所以最终 loss 是在整个训练集上的平均损失。</span></li><li id="u3acc0de5" data-lake-index-type="0"><span id="zupDi" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/6100158802e722a88c15efc101fc275b.svg"></span><span class="ne-text">：缩放系数，控制 policy 与参考模型之间的偏差有多敏感。</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u026a7af4" data-lake-index-type="0"><span id="g21st" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/6100158802e722a88c15efc101fc275b.svg"></span><span class="ne-text"> 大 → 更强调区分优劣动作；</span></li><li id="u9504861c" data-lake-index-type="0"><span id="m47tc" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/6100158802e722a88c15efc101fc275b.svg"></span><span class="ne-text"> 小 → 更温和地调整。</span></li></ul></ul><ul class="ne-ul"><li id="u1c8e02f0" data-lake-index-type="0"><span id="QGbuI" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/40e869c1d13997553ef0cf976e09b2b5.svg"></span><span class="ne-text">：当前模型在历史 </span><span id="SLSEs" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/7a5b8967d37d84b58992cc8ac1bbc2c6.svg"></span><span class="ne-text"> 下输出优动作 </span><span id="cFPWE" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/f79a6111a6ed5f7107289647dd62cb3f.svg"></span><span class="ne-text"> 的概率。</span></li><li id="u41f4a739" data-lake-index-type="0"><span id="eeksp" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/9a690a604c1d023ce0e5b12fb546f210.svg"></span><span class="ne-text">：同理，是劣动作的概率。</span></li><li id="ub86b588a" data-lake-index-type="0"><span id="cQsG4" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/07e1edf789c9dd7ea7b15f1e67c9812e.svg"></span><span class="ne-text">：这里的 </span><span id="CtGoB" class="ne-math"><img src="https://cdn.nlark.com/yuque/__latex/788df1ba344b3092def7590d1be6b4d4.svg"></span><span class="ne-text"> 是 sigmoid 函数。log-sigmoid 的形式和对比学习类似，确保：</span></li></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="uab88fc25" data-lake-index-type="0"><span class="ne-text">如果优动作相对参考模型的概率比劣动作更大 → loss 变小；</span></li><li id="ua4e273d8" data-lake-index-type="0"><span class="ne-text">如果模型错误地更偏向劣动作 → loss 变大，强迫模型纠正。</span></li></ul></ul></details>

+ 轨迹级DPO：
    - ![image](https://cdn.nlark.com/yuque/__latex/86b7383d304c5415d89bf0f6542bbbda.svg)
+ 虽然偏好标签来自 env action，但训练信号被应用到了 **plan/thought/expl + env** 整体序列的 likelihood。
+ <font style="color:rgb(17, 24, 39);">采用无策略重放缓冲区，缓解计算资源，避免独立参考模型。</font>
3. **条件传导**
    - 因为 env action 的选择是**依赖于 plan/thought/expl 的上下文**（模型是自回归的），如果 thought/expl 给出的“上下文”更有助于生成更优的 env action，它们的概率分布就会被调整。
    - 换句话说，DPO 在优化 env action 的同时，**顺带推动模型去产生那些“能更好引导 env action” 的 plan/thought/expl**。

##### 打个比方
假设有两个 rollout：

+ **坏轨迹**：
    - thought: “我应该立即点确认”
    - env action: `CLICK [wrong button]`
+ **好轨迹**：
    - thought: “先检查餐厅是否有空位”
    - env action: `CLICK [availability button]`

MCTS 会给 **第二条 env action** 更高的 Q 值。  
→ DPO 偏好对就标记第二条轨迹更优。  
→ 优化时，整个序列 (thought + env + explanation) 的 log-likelihood 都会被提升。  
→ 于是模型学到“这种 thought 更可能导致好 env action”，以后也更倾向生成这种思路。

## 通过强化学习提升零样本性能
+ **目标**：利用MCTS生成的轨迹数据，通过DPO离线训练，提升模型的内在能力。
+ **理论基础**：Theorem 1 表明，若偏好对（preference pairs）基于最优Q值生成，则DPO优化等价于最优RL策略。
+ **偏好对构造**：
    - 在MCTS树同一父节点（状态）的每个节点，比较不同子节点（动作）的Q值。
    - 如果两个动作的Q值差异超过阈值 ![image](https://cdn.nlark.com/yuque/__latex/59c2f1d2daa5a03d3b55b477dfe07c27.svg)，则构造一个偏好对。
    - **最终Q值**：采用加权平均![image](https://cdn.nlark.com/yuque/__latex/32c3ca1f40b8813dc0e830fe55cf1c47.svg)
        * ![image](https://cdn.nlark.com/yuque/__latex/8f7523fa11cf0319316d47fbb8ec7fa1.svg)：通过MCTS回溯得到的。
        * ![image](https://cdn.nlark.com/yuque/__latex/bd24ae7d139a07a4e2263a809667c4c2.svg)：通过AI过程监督得到的。
    - 偏好对![image](https://cdn.nlark.com/yuque/__latex/31c551f1147316a6fde2db1bf76436dd.svg)：当处于![image](https://cdn.nlark.com/yuque/__latex/cd7da655072cb606d866aed0b8354535.svg)状态时，应该选择![image](https://cdn.nlark.com/yuque/__latex/b2a4de046f8b90bbacb8b750e9af7a0d.svg)，而不是![image](https://cdn.nlark.com/yuque/__latex/7270d280a3feecbc2dcb385a14b42e2b.svg)。
+ **结果**：训练后的模型（Agent Q）即使在不使用MCTS的情况下（零样本），其性能也远超基线。

### 图表说明
![图1：我们使用蒙特卡洛树搜索（MCTS）来指导轨迹收集，并通过直接偏好优化（DPO）迭代地提升模型性能。我们从左侧开始，从数据集的任务列表中采样一个用户查询。我们使用UCB1作为启发式方法，迭代地扩展搜索树，以平衡不同动作的探索与利用。我们将树中每个节点累积获得的奖励存储起来，在本图中，更深的绿色表示更高的奖励，更深的红色表示更低的奖励。为了构建偏好数据集，我们计算MCTS平均Q值和由反馈语言模型生成的分数的加权得分，以构造用于DPO的对比对。策略被优化，并且可以被迭代地改进。](https://cdn.nlark.com/yuque/0/2025/png/39039688/1756203796679-dc0fe408-54be-4ae0-980c-c5bcbc38f422.png)

<details class="lake-collapse"><summary id="u969dd33e"><span class="ne-text">【AI生成】图1解释</span></summary><p id="uab7c21ee" class="ne-p"><span class="ne-text">这张图是理解Agent Q框架核心思想的“全景图”。它清晰地展示了</span><strong><span class="ne-text">训练阶段</span></strong><span class="ne-text">（Training Phase）的整个流程，特别是如何将</span><strong><span class="ne-text">MCTS</span></strong><span class="ne-text">和</span><strong><span class="ne-text">DPO</span></strong><span class="ne-text">结合起来。</span></p><h4 id="DvgWj"><strong><span class="ne-text">整体流程概览</span></strong></h4><p id="ub6a1e640" class="ne-p"><span class="ne-text">整个过程是一个循环：</span></p><ol class="ne-ol"><li id="u1c08d6a0" data-lake-index-type="0"><strong><span class="ne-text">起点</span></strong><span class="ne-text">：从一个具体的任务开始（如“在OpenTable上预订餐厅”）。</span></li><li id="u7404d9fa" data-lake-index-type="0"><strong><span class="ne-text">MCTS探索</span></strong><span class="ne-text">：使用MCTS算法在可能的行动路径上进行探索，生成大量的成功和失败的交互轨迹。</span></li><li id="uae55f086" data-lake-index-type="0"><strong><span class="ne-text">数据构建</span></strong><span class="ne-text">：利用MCTS探索过程中产生的信息（Q值、AI反馈），构建一个“偏好对”数据集。</span></li><li id="ufe71b1be" data-lake-index-type="0"><strong><span class="ne-text">DPO训练</span></strong><span class="ne-text">：使用这个偏好对数据集，通过DPO算法对基础LLM模型进行微调，使其学习到更优的决策策略。</span></li><li id="u92c45450" data-lake-index-type="0"><strong><span class="ne-text">迭代</span></strong><span class="ne-text">：用新训练好的模型再次进行MCTS探索，如此循环往复，不断迭代优化模型。</span></li></ol><h4 id="qUlsy"><strong><span class="ne-text">详细分解</span></strong></h4><ol class="ne-ol"><li id="uc107a26a" data-lake-index-type="0"><strong><span class="ne-text">左侧：MCTS 搜索树 (The Search Tree)</span></strong></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u6bc151ba" data-lake-index-type="0"><strong><span class="ne-text">根节点 (Root Node)</span></strong><span class="ne-text">：代表初始状态，即用户的任务指令（“Booking a Reservation on Open Table”）。</span></li><li id="ub7fa5806" data-lake-index-type="0"><strong><span class="ne-text">树枝 (Branches)</span></strong><span class="ne-text">：代表了在执行任务时可能采取的不同动作序列。</span></li><li id="u4e6a3f9c" data-lake-index-type="0"><strong><span class="ne-text">节点 (Nodes)</span></strong><span class="ne-text">：代表了网页上的一个具体状态或决策点。例如，“导航到错误的餐厅”、“选择错误的日期”等。</span></li><li id="uf348f21a" data-lake-index-type="0"><strong><span class="ne-text">颜色编码 (Color Coding)</span></strong><span class="ne-text">：</span></li></ul></ul><ul class="ne-list-wrap"><ul class="ne-list-wrap"><ul ne-level="2" class="ne-ul"><li id="u1c98919c" data-lake-index-type="0"><strong><span class="ne-text">深绿色 (Darker Green)</span></strong><span class="ne-text">：表示该路径（或该节点）的累积奖励高，意味着这条路径更有可能通向成功。</span></li><li id="u0177c0d3" data-lake-index-type="0"><strong><span class="ne-text">深红色 (Darker Red)</span></strong><span class="ne-text">：表示该路径（或该节点）的累积奖励低，意味着这条路径很可能导致失败。</span></li><li id="u748ac48c" data-lake-index-type="0"><span class="ne-text">这个颜色编码直观地展示了MCTS如何通过反复模拟和回溯，为每条路径“打分”，从而引导搜索向更成功的方向发展。</span></li></ul></ul></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="ub3886e3a" data-lake-index-type="0"><strong><span class="ne-text">路径示例</span></strong><span class="ne-text">：</span></li></ul></ul><ul class="ne-list-wrap"><ul class="ne-list-wrap"><ul ne-level="2" class="ne-ul"><li id="u19eb3822" data-lake-index-type="0"><strong><span class="ne-text">成功路径 (SUCCESS)</span></strong><span class="ne-text">：</span><code class="ne-code"><span class="ne-text">1 -&gt; 3 -&gt; 5 -&gt; 6 -&gt; 7</span></code><span class="ne-text">。这条路径经过一系列正确的操作，最终完成了预订，因此被标记为“SUCCESS”。</span></li><li id="u015128d3" data-lake-index-type="0"><strong><span class="ne-text">失败路径 (FAILURE)</span></strong><span class="ne-text">：</span><code class="ne-code"><span class="ne-text">1 -&gt; 2</span></code><span class="ne-text"> 和 </span><code class="ne-code"><span class="ne-text">3 -&gt; 4</span></code><span class="ne-text">。这些路径因为选择了错误的餐厅或日期而失败，因此被标记为“FAILURE”。</span></li></ul></ul></ul><ol start="2" class="ne-ol"><li id="u92979bb8" data-lake-index-type="0"><strong><span class="ne-text">右侧：AI过程监督 (AI Process Supervision)</span></strong></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="ub64d72f0" data-lake-index-type="0"><span class="ne-text">这个部分展示了MCTS是如何在探索过程中获得“中间奖励”的。</span></li><li id="u134459a1" data-lake-index-type="0"><strong><span class="ne-text">背景</span></strong><span class="ne-text">：在真实的网页环境中，只有在任务完成时才会得到最终的“成功/失败”奖励（稀疏奖励）。这使得模型很难判断中间步骤的好坏（信用分配问题）。</span></li><li id="u66361722" data-lake-index-type="0"><strong><span class="ne-text">解决方案 - AI反馈</span></strong><span class="ne-text">：作者引入了一个“反馈语言模型”（Feedback Language Model），它也是一个LLM（可能是和主模型相同的模型）。</span></li><li id="u7c0c21c8" data-lake-index-type="0"><strong><span class="ne-text">工作方式</span></strong><span class="ne-text">：</span></li></ul></ul><ol class="ne-list-wrap"><ol class="ne-list-wrap"><ol ne-level="2" class="ne-ol"><li id="u25ec07bc" data-lake-index-type="0"><span class="ne-text">当MCTS到达一个决策点（比如要点击哪个按钮）时，它会从主模型那里采样出多个可能的动作（K个）。</span></li><li id="u29fbbb75" data-lake-index-type="0"><span class="ne-text">然后，</span><strong><span class="ne-text">反馈语言模型</span></strong><span class="ne-text">会被询问：“在当前情况下，你认为这些动作哪个更有助于完成任务？”</span></li><li id="u2bbfc14d" data-lake-index-type="0"><span class="ne-text">反馈模型会根据其自身的“直觉”对这些动作进行排序和打分。</span></li></ol></ol></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="ub69b03c4" data-lake-index-type="0"><strong><span class="ne-text">目的</span></strong><span class="ne-text">：这个由AI生成的“过程监督”分数（Process Feedback Score）作为一个</span><strong><span class="ne-text">中间奖励</span></strong><span class="ne-text">，被用来指导MCTS的搜索。它让MCTS能“提前知道”某个动作是否看起来“正确”，从而优先探索那些被AI认为是好主意的路径。</span></li></ul></ul><ol start="3" class="ne-ol"><li id="u3dafa1d5" data-lake-index-type="0"><strong><span class="ne-text">连接与整合 (Connecting the Dots)</span></strong></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="ua3d3b457" data-lake-index-type="0"><span class="ne-text">图中的箭头表明了MCTS搜索的结果（即所有探索过的路径及其Q值、AI反馈分数）被用来构建一个“偏好对”数据集。</span></li><li id="uada7184b" data-lake-index-type="0"><span class="ne-text">具体来说，对于搜索树中的每一个节点，如果两个子节点的Q值差异足够大（超过阈值 </span><code class="ne-code"><span class="ne-text">θthreshold</span></code><span class="ne-text">），就会形成一个“优选”（preferred）和“劣选”（dispreferred）的对比对。</span></li><li id="u01a5a799" data-lake-index-type="0"><span class="ne-text">这个对比对的数据集就是</span><strong><span class="ne-text">DPO算法的输入</span></strong><span class="ne-text">。DPO会利用这些数据来训练主模型，让模型学会在相同的历史下，生成“优选”响应的概率远高于“劣选”响应的概率。</span></li></ul></ul><h4 id="IOPWX"><strong><span class="ne-text">总结</span></strong></h4><p id="u23816a0e" class="ne-p"><span class="ne-text">图1的核心意义在于，它展示了一个</span><strong><span class="ne-text">闭环的、自我强化的学习系统</span></strong><span class="ne-text">：</span></p><ul class="ne-ul"><li id="ufedc035f" data-lake-index-type="0"><strong><span class="ne-text">MCTS</span></strong><span class="ne-text"> 是一个强大的“探索者”和“数据生成器”，它通过结合</span><strong><span class="ne-text">AI过程监督</span></strong><span class="ne-text">来解决稀疏奖励的问题，生成高质量的训练数据。</span></li><li id="u6614ad93" data-lake-index-type="0"><strong><span class="ne-text">DPO</span></strong><span class="ne-text"> 是一个高效的“学习者”，它利用MCTS生成的“谁更好”的偏好数据，直接优化模型的参数。</span></li><li id="u0faeca6b" data-lake-index-type="0"><span class="ne-text">两者结合，使得模型能够从自己的探索经验中持续学习，不断提升在复杂、多步骤任务中的决策能力。</span></li></ul></details>

![](https://cdn.nlark.com/yuque/0/2025/png/39039688/1756205136774-e59fe088-e2c9-4894-b7b7-5986de99b53c.png)

<details class="lake-collapse"><summary id="u4416c6c0"><span class="ne-text">【AI生成】算法1解释</span></summary><div class="ne-quote"><p id="u178298e4" class="ne-p"><strong><span class="ne-text">算法1：MCTS引导的直接偏好优化（MCTS Guided Direct Preference Optimization）</span></strong></p><p id="ud7731271" class="ne-p"><strong><span class="ne-text">输入</span></strong><span class="ne-text">：</span><code class="ne-code"><span class="ne-text">πθ0</span></code><span class="ne-text">：初始LLM策略，</span><code class="ne-code"><span class="ne-text">DT</span></code><span class="ne-text">：代理必须在环境中完成的任务数据集，</span><code class="ne-code"><span class="ne-text">N</span></code><span class="ne-text">：迭代次数，</span><code class="ne-code"><span class="ne-text">B</span></code><span class="ne-text">：每次迭代的样本数，</span><code class="ne-code"><span class="ne-text">T</span></code><span class="ne-text">：MCTS树深度，</span><code class="ne-code"><span class="ne-text">ℬ</span></code><span class="ne-text">：重放缓冲区（replay buffer），</span><code class="ne-code"><span class="ne-text">θthreshold</span></code><span class="ne-text">：公式(10)中的值阈值，</span><code class="ne-code"><span class="ne-text">K</span></code><span class="ne-text">：为MCTS采样的动作数量</span></p><p id="u6dbff0de" class="ne-p"><strong><span class="ne-text">输出</span></strong><span class="ne-text">：</span><code class="ne-code"><span class="ne-text">πθN</span></code><span class="ne-text">，训练好的LLM策略</span></p><p id="u2536243f" class="ne-p"><strong><span class="ne-text">for i = 1 to N do</span></strong><span class="ne-text"><br /></span><span class="ne-text">    </span><code class="ne-code"><span class="ne-text">πref ← πθi</span></code><span class="ne-text">, </span><code class="ne-code"><span class="ne-text">πθi ← πθi−1</span></code><span class="ne-text"><br /></span><span class="ne-text">    从 </span><code class="ne-code"><span class="ne-text">DT</span></code><span class="ne-text"> 中采样一批大小为 </span><code class="ne-code"><span class="ne-text">B</span></code><span class="ne-text"> 的任务<br /></span><span class="ne-text">    </span><strong><span class="ne-text">for each task in batch do</span></strong><span class="ne-text"><br /></span><span class="ne-text">        初始化根节点 </span><code class="ne-code"><span class="ne-text">h0</span></code><span class="ne-text"><br /></span><span class="ne-text">        </span><strong><span class="ne-text">for t = 1 to T do</span></strong><span class="ne-text"><br /></span><span class="ne-text">            </span><strong><span class="ne-text">选择（Selection）</span></strong><span class="ne-text">：使用树策略（UCB1；7）从根节点遍历到叶节点<br /></span><span class="ne-text">            </span><strong><span class="ne-text">轨迹回放（Trajectory Rollout）</span></strong><span class="ne-text">：从选定节点的轨迹开始，使用 </span><code class="ne-code"><span class="ne-text">πθi</span></code><span class="ne-text"> 回放轨迹，直到达到终止状态<br /></span><span class="ne-text">            </span><strong><span class="ne-text">回溯（Backpropagation）</span></strong><span class="ne-text">：自底向上地反向传播价值估计（8）<br /></span><span class="ne-text">        </span><strong><span class="ne-text">end for</span></strong><span class="ne-text"><br /></span><span class="ne-text">        从回放中收集轨迹并存储在重放缓冲区 </span><code class="ne-code"><span class="ne-text">ℬ</span></code><span class="ne-text"> 中<br /></span><span class="ne-text">    </span><strong><span class="ne-text">end for</span></strong><span class="ne-text"><br /></span><span class="ne-text">    构造偏好对 </span><code class="ne-code"><span class="ne-text">DP = {(ht, awt, alt)}T−1 t=1</span></code><span class="ne-text">，其中 </span><code class="ne-code"><span class="ne-text">ht ∼ DP</span></code><span class="ne-text">。对于每个步骤 </span><code class="ne-code"><span class="ne-text">t</span></code><span class="ne-text"> 的节点，比较每对子节点，并构造生成的动作对 </span><code class="ne-code"><span class="ne-text">(aw, al)</span></code><span class="ne-text">，如果采取该动作的价值差异 </span><code class="ne-code"><span class="ne-text">|Q(ht, aw) - Q(ht, al)| &gt; θthreshold</span></code><span class="ne-text">，其中 </span><code class="ne-code"><span class="ne-text">Q(ht, aw)</span></code><span class="ne-text"> 和 </span><code class="ne-code"><span class="ne-text">Q(ht, al)</span></code><span class="ne-text"> 是通过公式(10)计算的。<br /></span><span class="ne-text">    使用公式(5)中的DPO目标函数，结合 </span><code class="ne-code"><span class="ne-text">DP</span></code><span class="ne-text"> 和 </span><code class="ne-code"><span class="ne-text">πref</span></code><span class="ne-text">，优化LLM策略 </span><code class="ne-code"><span class="ne-text">πθi</span></code><span class="ne-text"><br /></span><strong><span class="ne-text">end for</span></strong></p></div><hr id="wkyzw" class="ne-hr"><p id="u0f720212" class="ne-p"><span class="ne-text">这个算法是Agent Q框架的核心，它将</span><strong><span class="ne-text">MCTS搜索</span></strong><span class="ne-text">和</span><strong><span class="ne-text">DPO训练</span></strong><span class="ne-text">紧密地结合在一起，形成一个</span><strong><span class="ne-text">迭代优化</span></strong><span class="ne-text">的闭环。它的主要目标是利用MCTS探索产生的“经验”来不断改进LLM策略。</span></p><h4 id="ZWUUT"><strong><span class="ne-text">整体流程概览</span></strong></h4><p id="u8b5dbf61" class="ne-p"><span class="ne-text">整个过程是一个循环，共进行 </span><code class="ne-code"><span class="ne-text">N</span></code><span class="ne-text"> 次迭代：</span></p><ol class="ne-ol"><li id="u43032446" data-lake-index-type="0"><strong><span class="ne-text">准备阶段</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u8c3b53b7" data-lake-index-type="0"><span class="ne-text">在每次迭代 </span><code class="ne-code"><span class="ne-text">i</span></code><span class="ne-text"> 开始时，将上一次迭代训练好的模型 </span><code class="ne-code"><span class="ne-text">πθi−1</span></code><span class="ne-text"> 设置为当前的参考模型 </span><code class="ne-code"><span class="ne-text">πref</span></code><span class="ne-text">。</span></li><li id="u1bfc7dbe" data-lake-index-type="0"><span class="ne-text">将当前要训练的模型初始化为 </span><code class="ne-code"><span class="ne-text">πθi</span></code><span class="ne-text">，它在本次迭代中会根据新数据被更新。</span></li></ul></ul><ol start="2" class="ne-ol"><li id="ua8609158" data-lake-index-type="0"><strong><span class="ne-text">数据收集阶段（MCTS）</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u6253f200" data-lake-index-type="0"><span class="ne-text">从任务数据集 </span><code class="ne-code"><span class="ne-text">DT</span></code><span class="ne-text"> 中随机抽取一批任务（</span><code class="ne-code"><span class="ne-text">B</span></code><span class="ne-text"> 个）。</span></li><li id="u54e55d6a" data-lake-index-type="0"><span class="ne-text">对于每一个任务，执行一个完整的MCTS搜索过程（见第5.1节）：</span></li></ul></ul><ul class="ne-list-wrap"><ul class="ne-list-wrap"><ul ne-level="2" class="ne-ul"><li id="u1db96cf7" data-lake-index-type="0"><strong><span class="ne-text">选择（Selection）</span></strong><span class="ne-text">：使用UCB1公式从根节点（任务初始状态）向下遍历，直到找到一个叶节点（未探索的决策点）。</span></li><li id="u901acf35" data-lake-index-type="0"><strong><span class="ne-text">轨迹回放（Trajectory Rollout）</span></strong><span class="ne-text">：从这个叶节点开始，使用当前的LLM策略 </span><code class="ne-code"><span class="ne-text">πθi</span></code><span class="ne-text"> 进行模拟（rollout），直到任务成功或失败，得到一个完整的交互轨迹。</span></li><li id="u675b5bd5" data-lake-index-type="0"><strong><span class="ne-text">回溯（Backpropagation）</span></strong><span class="ne-text">：将这个轨迹的最终奖励（R=1/0）从叶节点反向传播到根节点，更新路径上所有 </span><code class="ne-code"><span class="ne-text">(状态, 动作)</span></code><span class="ne-text"> 对的访问次数 </span><code class="ne-code"><span class="ne-text">N(ht, ait)</span></code><span class="ne-text"> 和平均Q值 </span><code class="ne-code"><span class="ne-text">Q(ht, ait)</span></code><span class="ne-text">。</span></li></ul></ul></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u1e77d369" data-lake-index-type="0"><span class="ne-text">所有这些由MCTS生成的轨迹都被收集起来，并存入一个</span><strong><span class="ne-text">重放缓冲区</span></strong><span class="ne-text"> </span><code class="ne-code"><span class="ne-text">ℬ</span></code><span class="ne-text">。</span></li></ul></ul><ol start="3" class="ne-ol"><li id="u4a354b77" data-lake-index-type="0"><strong><span class="ne-text">数据处理与训练阶段（DPO）</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="ub5b15a93" data-lake-index-type="0"><strong><span class="ne-text">构建偏好对</span></strong><span class="ne-text">：从重放缓冲区 </span><code class="ne-code"><span class="ne-text">ℬ</span></code><span class="ne-text"> 中提取数据，为DPO训练构建“对比对”。具体来说，对于MCTS树中每个时间步 </span><code class="ne-code"><span class="ne-text">t</span></code><span class="ne-text"> 的节点 </span><code class="ne-code"><span class="ne-text">ht</span></code><span class="ne-text">，会比较其所有子节点（即不同的可能动作）。如果两个动作 </span><code class="ne-code"><span class="ne-text">aw</span></code><span class="ne-text"> 和 </span><code class="ne-code"><span class="ne-text">al</span></code><span class="ne-text"> 的Q值差异超过预设的阈值 </span><code class="ne-code"><span class="ne-text">θthreshold</span></code><span class="ne-text">，就认为 </span><code class="ne-code"><span class="ne-text">aw</span></code><span class="ne-text"> 是更优的，从而构成一个“优选” vs “劣选”的偏好对 </span><code class="ne-code"><span class="ne-text">(ht, aw, al)</span></code><span class="ne-text">。</span></li><li id="ub76b676e" data-lake-index-type="0"><strong><span class="ne-text">模型训练</span></strong><span class="ne-text">：使用这些构建好的偏好对数据集 </span><code class="ne-code"><span class="ne-text">DP</span></code><span class="ne-text"> 和参考模型 </span><code class="ne-code"><span class="ne-text">πref</span></code><span class="ne-text">，通过DPO的损失函数（公式5）来优化当前的LLM策略 </span><code class="ne-code"><span class="ne-text">πθi</span></code><span class="ne-text">。这一步会更新模型的内部参数 </span><code class="ne-code"><span class="ne-text">θ</span></code><span class="ne-text">。</span></li></ul></ul><h4 id="lJNPh"><strong><span class="ne-text">关键细节解析</span></strong></h4><ul class="ne-ul"><li id="ub3b96873" data-lake-index-type="0"><strong><span class="ne-text">重放缓冲区（Replay Buffer </span></strong><code class="ne-code"><span class="ne-text">ℬ</span></code><strong><span class="ne-text">）</span></strong><span class="ne-text">：这是一个非常重要的组件。它允许算法在一次MCTS搜索中收集大量轨迹，并在后续的多个DPO训练迭代中重复使用这些数据，从而提高了数据利用效率，避免了每次都重新进行昂贵的MCTS搜索。</span></li><li id="u230a6789" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">πref</span></code><strong><span class="ne-text"> 的作用</span></strong><span class="ne-text">：</span><code class="ne-code"><span class="ne-text">πref</span></code><span class="ne-text"> 是一个固定的参考模型，在本次迭代中不会被改变。它在DPO的损失函数中作为“基准”，用来衡量新策略 </span><code class="ne-code"><span class="ne-text">πθi</span></code><span class="ne-text"> 相对于旧策略的改进程度，防止模型在训练过程中偏离太远（out-of-distribution drift）。</span></li><li id="u0c527fc8" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">θthreshold</span></code><strong><span class="ne-text"> 的作用</span></strong><span class="ne-text">：这个阈值用于过滤掉那些Q值差异很小、难以判断优劣的行动对。只保留那些差距明显的对比对，可以提高训练信号的质量，使DPO学习到更清晰的偏好。</span></li><li id="u85969121" data-lake-index-type="0"><strong><span class="ne-text">迭代优化</span></strong><span class="ne-text">：这是算法的精髓。经过一轮DPO训练后，模型 </span><code class="ne-code"><span class="ne-text">πθi</span></code><span class="ne-text"> 会变得更强。在下一轮迭代中，这个更强的模型 </span><code class="ne-code"><span class="ne-text">πθi</span></code><span class="ne-text"> 又会被用作新的 </span><code class="ne-code"><span class="ne-text">πθi−1</span></code><span class="ne-text"> 来进行MCTS搜索。这意味着新的MCTS搜索会基于一个更聪明的“大脑”进行，从而能探索出更优的路径，产生更高质量的训练数据，进而让模型变得更强大。这个正向循环是Agent Q性能提升的关键。</span></li></ul><h4 id="kr1bP"><strong><span class="ne-text">总结</span></strong></h4><p id="u5cf76302" class="ne-p"><span class="ne-text">算法1完美地诠释了论文的核心思想：“</span><strong><span class="ne-text">用搜索来指导学习，用学习来增强搜索</span></strong><span class="ne-text">”。MCTS作为一个强大的探索者，负责生成高质量的数据（轨迹和偏好对）；DPO作为一个高效的学习者，负责利用这些数据来优化模型。两者通过迭代的方式相互促进，共同推动LLM策略的持续进化。</span></p></details>
