
# 实验（推理阶段）
## 虚拟环境
### WebShop介绍（摘自WebShop论文）
+ **环境**：一个模拟的电子商务平台，用于基准测试。
+ **任务场景**：搜索页（search）、结果页（results）、商品详情页（item）、进一步的详情页（item-detail）等。
+ **是否多跳**：
    - 涉及多页面导航：Agent 需从搜索页面开始，通过点击搜索结果、进入商品页面，进一步定制选项后进行购买，跨越多个页面完成任务。但都在同一模拟网站中进行。
    - WebShop 的任务虽多页面跳转，但限定在同一站点／模拟环境中。
+ **任务评估**：
    - 通过解析HTML内容来获取agent执行操作后的结果数据。
    - ![image](https://cdn.nlark.com/yuque/__latex/83da07dc473935db7cd534f4331b3647.svg)
    - 由类型、属性、选项、价格匹配组成。
    - r＝1时任务成功。

### 实验结果
+ **基础模型**（xLAM-v0.1-r）零样本成功率：28.6%。
+ **基础模型+RFT**（仅使用成功轨迹）：提升至31.3%。
+ **基础模型+DPO**（成功 vs 失败轨迹）：提升至40.6%，<u>说明DPO优于RFT</u>。
+ **基础模型+MCTS**（推理阶段启动MCTS）：提升至48.4%，<u>说明推理时使用MCTS有巨大性能提升</u>。
+ **Agent Q**（训练阶段包含MCTS+AI监督+DPO，零样本）：41.5%。
+ **Agent Q+MCTS**（推理阶段启用MCTS）：提升至50.5%，<u>说明推理时使用MCTS有巨大性能提升</u>。

### 图表说明
![图3：不同方法在WebShop任务（Yao et al. (2022)）上的成功率。所有模型都基于xLAM-v0.1-r Zhang et al. (2024c)。在xLAM-v0.1-r上进行RFT和DPO训练，性能分别从28.6%提升到31.3%和37.5%。然而，这些方法仍然落后于平均人类表现的50.0%。我们的方法，Agent Q + MCTS，在基线模型上实现了显著的增益（相对提升76.57%），在WebShop上以50.5%的成功率超越了平均水平的人类表现。](https://cdn.nlark.com/yuque/0/2025/png/39039688/1756204539903-e13e9b83-d921-4468-b621-37e757447e9d.png)

<details class="lake-collapse"><summary id="u569dcfec"><span class="ne-text">【AI生成】图3解释</span></summary><p id="u020c43ce" class="ne-p"><span class="ne-text">这张图是衡量Agent Q框架性能的核心图表之一。它清晰地展示了在</span><strong><span class="ne-text">模拟环境（WebShop）</span></strong><span class="ne-text"> 中，不同方法的性能对比，突出了Agent Q+MCTS组合的巨大优势。</span></p><h4 id="S7H69"><strong><span class="ne-text">整体解读</span></strong></h4><p id="uf7ed7bb4" class="ne-p"><span class="ne-text">这是一个条形图，横轴是不同的方法或模型，纵轴是它们在WebShop基准测试上的</span><strong><span class="ne-text">成功率为百分比</span></strong><span class="ne-text">。这个成功率达到</span><strong><span class="ne-text">50%</span></strong><span class="ne-text"> 是一个关键的参考点，因为它代表了“平均人类”的表现水平。</span></p><h4 id="hEw86"><strong><span class="ne-text">各条形柱详解</span></strong></h4><ol class="ne-ol"><li id="u790ce1b3" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">xLAM-v0.1-r</span></code><strong><span class="ne-text"> (深紫色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u77ea81bb" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：这是所有其他方法的基线模型。它是经过监督微调（SFT）的LLM，但没有经过额外的强化学习或搜索优化。</span></li><li id="u33769b07" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">28.6%</span></strong><span class="ne-text">。这表明即使是强大的预训练模型，在没有进一步优化的情况下，其零样本能力也有限。</span></li></ul></ul><ol start="2" class="ne-ol"><li id="udebab515" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">RFT</span></code><strong><span class="ne-text"> (深紫色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="ub06686d8" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：Reinforced Fine-Tuning（强化微调）。这是一种基于结果监督（outcome-based supervision）的方法，它使用成功轨迹来训练模型。</span></li><li id="ubf8be3f2" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">31.3%</span></strong><span class="ne-text">。相比基线模型有小幅提升（+2.7%），但效果有限。</span></li></ul></ul><ol start="3" class="ne-ol"><li id="u7e156270" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">GPT-4</span></code><strong><span class="ne-text"> (深蓝色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="udf27afc0" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：作为另一个强大的基线模型进行比较。</span></li><li id="u52b76a39" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">37.5%</span></strong><span class="ne-text">。虽然比xLAM-v0.1-r强，但仍然远低于人类水平。</span></li></ul></ul><ol start="4" class="ne-ol"><li id="u139d609b" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">DPO</span></code><strong><span class="ne-text"> (深蓝色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u3b174ab2" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：Direct Preference Optimization（直接偏好优化）。这是一种更先进的RLHF方法，利用成功和失败的轨迹来构建偏好对，从而训练模型。</span></li><li id="u569450a5" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">40.6%</span></strong><span class="ne-text">。比RFT和GPT-4都好，证明了DPO的有效性，但仍落后于人类。</span></li></ul></ul><ol start="5" class="ne-ol"><li id="u8082553d" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">DPO+BeamSearch</span></code><strong><span class="ne-text"> (青色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u5f185ec1" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：在DPO训练的基础上，结合了束搜索（Beam Search）这种推理时的规划技术。</span></li><li id="u53b4ed36" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">41.5%</span></strong><span class="ne-text">。与纯DPO相比，提升非常小，说明简单的规划技术在这个任务上效果有限。</span></li></ul></ul><ol start="6" class="ne-ol"><li id="uc11c8787" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">AgentQ</span></code><strong><span class="ne-text"> (绿色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u8a145498" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：这是指经过</span><strong><span class="ne-text">Agent Q框架</span></strong><span class="ne-text">（即MCTS+AI过程监督+DPO）训练后的模型，但</span><strong><span class="ne-text">在评估时没有启用MCTS搜索</span></strong><span class="ne-text">（即零样本模式）。</span></li><li id="u3c39a4c3" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">48.4%</span></strong><span class="ne-text">。这个成绩非常关键，它表明仅仅通过训练（DPO），模型的能力就得到了巨大提升，已经接近人类平均水平。</span></li></ul></ul><ol start="7" class="ne-ol"><li id="u8766f74e" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">xLAM + MCTS</span></code><strong><span class="ne-text"> (浅绿色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u448f984f" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：将</span><strong><span class="ne-text">未训练的基线模型</span></strong><span class="ne-text">（xLAM-v0.1-r）与</span><strong><span class="ne-text">推理时的MCTS搜索</span></strong><span class="ne-text">相结合。</span></li><li id="u8b91933e" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">50.0%</span></strong><span class="ne-text">。这个结果非常重要，它证明了</span><strong><span class="ne-text">MCTS搜索本身</span></strong><span class="ne-text">就能带来巨大的性能提升，使一个未经强化训练的模型达到人类平均水平。</span></li></ul></ul><ol start="8" class="ne-ol"><li id="u36923bae" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">Human (average)</span></code><strong><span class="ne-text"> (浅绿色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u7d6b9488" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：平均人类的表现。</span></li><li id="u45aa5c41" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">50.0%</span></strong><span class="ne-text">。作为基准线。</span></li></ul></ul><ol start="9" class="ne-ol"><li id="uc5cb8624" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">AgentQ + MCTS</span></code><strong><span class="ne-text"> (亮黄色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u28d20892" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：这是</span><strong><span class="ne-text">最终的、最强的组合</span></strong><span class="ne-text">。它将</span><strong><span class="ne-text">经过训练的Agent Q模型</span></strong><span class="ne-text">与</span><strong><span class="ne-text">推理时的MCTS搜索</span></strong><span class="ne-text">结合起来。</span></li><li id="u8c74bcff" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">50.5%</span></strong><span class="ne-text">。这是所有方法中最高的，</span><strong><span class="ne-text">超过了平均人类的表现</span></strong><span class="ne-text">。</span></li></ul></ul><ol start="10" class="ne-ol"><li id="ua2968edf" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">Human (expert)</span></code><strong><span class="ne-text"> (亮黄色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="uf40af1c3" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：专家人类的表现。</span></li><li id="u1e952c95" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">59.6%</span></strong><span class="ne-text">。这是人类能达到的最高水平，为未来的研究设定了一个更高的目标。</span></li></ul></ul><h4 id="LmXO3"><strong><span class="ne-text">核心结论</span></strong></h4><ul class="ne-ul"><li id="u980ba91c" data-lake-index-type="0"><strong><span class="ne-text">训练很重要</span></strong><span class="ne-text">：</span><code class="ne-code"><span class="ne-text">AgentQ</span></code><span class="ne-text">（48.4%） vs </span><code class="ne-code"><span class="ne-text">xLAM-v0.1-r</span></code><span class="ne-text">（28.6%）的对比，证明了通过MCTS生成数据并用DPO训练模型，能极大地提升模型的内在能力。</span></li><li id="ua4e08dbe" data-lake-index-type="0"><strong><span class="ne-text">搜索很重要</span></strong><span class="ne-text">：</span><code class="ne-code"><span class="ne-text">xLAM + MCTS</span></code><span class="ne-text">（50.0%） vs </span><code class="ne-code"><span class="ne-text">xLAM-v0.1-r</span></code><span class="ne-text">（28.6%）的对比，证明了在推理时使用MCTS搜索能有效弥补模型推理能力的不足。</span></li><li id="u8aa31fce" data-lake-index-type="0"><strong><span class="ne-text">两者结合是关键</span></strong><span class="ne-text">：</span><code class="ne-code"><span class="ne-text">AgentQ + MCTS</span></code><span class="ne-text">（50.5%）的成绩，是</span><strong><span class="ne-text">训练</span></strong><span class="ne-text">（提升模型能力）和</span><strong><span class="ne-text">搜索</span></strong><span class="ne-text">（在执行时进行规划）协同作用的结果。它不仅超越了基线模型，还超越了平均人类表现，展现了该框架的强大潜力。</span></li></ul><p id="u3c9670ec" class="ne-p"><span class="ne-text">总而言之，图3有力地证明了Agent Q框架在模拟环境中的有效性，特别是当将训练和推理时的搜索能力结合起来时，能够产生超越人类平均水平的性能。</span></p></details>

## 真实世界
### OpenTable介绍
+ **环境**：一个真实世界的餐厅预订网站。
+ **挑战**：真实、动态、步骤多（平均13.9步，远超WebShop的6.8步）。
+ **成功判定**：使用 **GPT-4-V** 作为评估器，基于**最终状态截图**和**执行历史**，根据四项标准（时间、人数、信息、点击按钮）给出二元（0/1）成功信号。
+ **成功判定**：使用**GPT-4-V** 作为评估器，根据**最终状态截图**和**历史**给出二元（0/1）成功信号。
+ **动作空间**：CLICK, GOTO, TYPE, SUBMIT, CLEAR, SCROLL, ASK USER等。

### 实验结果
+ 基础模型（LLaMA-3 70B Instruct）零样本成功率：18.6%。
+ RFT：提升至67.2%。
+ DPO：提升至71.8%，<u>说明DPO优于RFT</u>。
+ RFT+MCTS（推理）：提升至84.3%，<u>说明推理时使用MCTS有巨大性能提升</u>。
+ Agent Q：提升至81.7%。
+ Agent Q+MCTS（推理）：提升至95.4%，<u>说明推理时使用MCTS有巨大性能提升</u>。

### 图表说明
![图5：在轨迹结束时，会调用一个GPT-4-V评估器，根据最终观察和动作历史来提供关于智能体表现的反馈，以确定成功得分。该模型被提示以浓缩的执行历史和最终状态的截图作为输入。成功指标是一个二元的0/1值。](https://cdn.nlark.com/yuque/0/2025/png/39039688/1756219529302-6972d7e8-dca2-4379-bc98-efe476596f2a.png)

<details class="lake-collapse"><summary id="u25f62daa"><span class="ne-text">【AI生成】图5解释</span></summary><p id="u46065691" class="ne-p"><span class="ne-text">这张图是理解Agent Q框架如何进行</span><strong><span class="ne-text">最终任务评估</span></strong><span class="ne-text">（Final Task Evaluation）的关键。它展示了在</span><strong><span class="ne-text">训练阶段</span></strong><span class="ne-text">（Training Phase），特别是当使用MCTS探索生成数据时，如何为一个完整的交互序列（trajectory）打上“成功”或“失败”的标签。</span></p><h4 id="d5IXK"><strong><span class="ne-text">整体流程概览</span></strong></h4><p id="u3132ca89" class="ne-p"><span class="ne-text">整个过程是：</span></p><ol class="ne-ol"><li id="uede9b230" data-lake-index-type="0"><strong><span class="ne-text">执行轨迹</span></strong><span class="ne-text">：智能体从用户查询开始，经过一系列思考、规划和行动，最终到达一个网页状态。</span></li><li id="u7a9b7a79" data-lake-index-type="0"><strong><span class="ne-text">调用评估器</span></strong><span class="ne-text">：在轨迹的终点，系统会自动调用一个外部的、强大的语言模型（GPT-4-V）来评判这个轨迹的最终结果。</span></li><li id="ub890dbb4" data-lake-index-type="0"><strong><span class="ne-text">获取反馈</span></strong><span class="ne-text">：GPT-4-V根据提供的信息，给出一个二元的成功分数（0或1）和一段文字说明。</span></li></ol><h4 id="LQ7nU"><strong><span class="ne-text">详细分解</span></strong></h4><ol class="ne-ol"><li id="u793e9c07" data-lake-index-type="0"><strong><span class="ne-text">左侧：最终观察 (FINAL OBSERVATION)</span></strong></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="ua6e4bef7" data-lake-index-type="0"><span class="ne-text">这是智能体执行完所有操作后，浏览器显示的最终页面截图。</span></li><li id="u3f6bdfe0" data-lake-index-type="0"><span class="ne-text">在图中，我们可以看到：</span></li></ul></ul><ul class="ne-list-wrap"><ul class="ne-list-wrap"><ul ne-level="2" class="ne-ul"><li id="u7f192cd4" data-lake-index-type="0"><span class="ne-text">智能体成功预订了餐厅 </span><strong><span class="ne-text">Fogo de Chao</span></strong><span class="ne-text">。</span></li><li id="u99cb3c09" data-lake-index-type="0"><span class="ne-text">预订的日期是 </span><strong><span class="ne-text">8月28日</span></strong><span class="ne-text">（Wed, Aug 28, 2024），但用户请求的是 </span><strong><span class="ne-text">8月29日</span></strong><span class="ne-text">。</span></li><li id="u87c83f43" data-lake-index-type="0"><span class="ne-text">预订的时间是 </span><strong><span class="ne-text">晚上7点</span></strong><span class="ne-text">，这与用户请求一致。</span></li></ul></ul></ul><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u10ebd351" data-lake-index-type="0"><span class="ne-text">这个观察是</span><strong><span class="ne-text">真实的、视觉化的</span></strong><span class="ne-text">，它包含了所有最终的、可验证的信息。</span></li></ul></ul><ol start="2" class="ne-ol"><li id="u0ac14fd4" data-lake-index-type="0"><strong><span class="ne-text">中间：LLM Critic (批评模型)</span></strong></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u3220442f" data-lake-index-type="0"><span class="ne-text">这里的“LLM Critic”实际上指的是</span><strong><span class="ne-text">GPT-4-V</span></strong><span class="ne-text">，它是论文中用于评估任务完成情况的工具。</span></li><li id="u30711b0f" data-lake-index-type="0"><span class="ne-text">它接收两个关键输入：</span></li></ul></ul><ul class="ne-list-wrap"><ul class="ne-list-wrap"><ul ne-level="2" class="ne-ul"><li id="ud1c5487f" data-lake-index-type="0"><strong><span class="ne-text">最终观察</span></strong><span class="ne-text">：即上面的截图。</span></li><li id="u86f272e4" data-lake-index-type="0"><strong><span class="ne-text">浓缩的执行历史</span></strong><span class="ne-text">：虽然图中没有直接展示，但论文描述了这一点。这个历史记录了智能体在整个任务中采取的所有关键步骤（如点击了哪个按钮、输入了什么内容等），以便评估器了解整个过程。</span></li></ul></ul></ul><ol start="3" class="ne-ol"><li id="u9a2430aa" data-lake-index-type="0"><strong><span class="ne-text">右侧：评估结果 (Score &amp; Feedback)</span></strong></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u856ac15d" data-lake-index-type="0"><strong><span class="ne-text">Score: 0.0</span></strong><span class="ne-text">：这是最关键的输出。它是一个</span><strong><span class="ne-text">二元（binary）</span></strong><span class="ne-text"> 的成功指标。</span><code class="ne-code"><span class="ne-text">0.0</span></code><span class="ne-text"> 表示</span><strong><span class="ne-text">失败</span></strong><span class="ne-text">，</span><code class="ne-code"><span class="ne-text">1.0</span></code><span class="ne-text"> 表示</span><strong><span class="ne-text">成功</span></strong><span class="ne-text">。</span></li><li id="uc65e672c" data-lake-index-type="0"><strong><span class="ne-text">反馈文本</span></strong><span class="ne-text">：“The agent booked a reservation for the correct restaurant, but incorrect date and time.” （智能体为正确的餐厅预订了座位，但日期和时间不正确。）</span></li><li id="ub8fb8176" data-lake-index-type="0"><strong><span class="ne-text">核心逻辑</span></strong><span class="ne-text">：尽管智能体完成了大部分工作（找到了正确的餐厅并提交了预订），但由于</span><strong><span class="ne-text">日期错误</span></strong><span class="ne-text">，任务被视为失败。这体现了评估标准的严格性——必须满足所有预设条件才算成功。</span></li></ul></ul><h4 id="FR2IT"><strong><span class="ne-text">为什么需要GPT-4-V？</span></strong></h4><ul class="ne-ul"><li id="u9e5b7112" data-lake-index-type="0"><strong><span class="ne-text">解决真实环境的挑战</span></strong><span class="ne-text">：在OpenTable这样的真实网站上，无法通过程序化的方式（programatically）读取后台数据库来验证预订是否真正成功。例如，系统无法直接知道预订的日期是否正确。</span></li><li id="u4f85b14a" data-lake-index-type="0"><strong><span class="ne-text">视觉理解能力</span></strong><span class="ne-text">：GPT-4-V作为一个多模态大模型，具备强大的</span><strong><span class="ne-text">视觉理解能力</span></strong><span class="ne-text">。它能够“看懂”截图中的所有元素（如文本、按钮、图标），并准确地提取出关键信息（如餐厅名称、日期、时间）。</span></li><li id="u523d8679" data-lake-index-type="0"><strong><span class="ne-text">综合判断</span></strong><span class="ne-text">：它不仅能识别信息，还能将这些信息与用户查询进行对比，并做出最终的判断。这比简单的文本匹配要复杂得多。</span></li></ul><h4 id="Qr7bC"><strong><span class="ne-text">总结</span></strong></h4><p id="u7c0bfa75" class="ne-p"><span class="ne-text">图5的核心意义在于，它展示了Agent Q框架如何利用一个</span><strong><span class="ne-text">强大的外部模型（GPT-4-V）作为“裁判”</span></strong><span class="ne-text"> 来解决真实世界任务评估的难题。这种基于视觉的、综合性的评估方式，确保了训练数据的质量，使得DPO算法能够学习到真正有用的策略，而不仅仅是学会如何在模拟环境中“作弊”。</span></p></details>

![图6：不同方法在OpenTable上的成功率。除非另有说明，所有模型均基于LLaMA-3-70B-Instruct Touvron et al. (2023)。使用DPO和RFT结合MCTS，性能分别从18.6%提升到71.8%和84.3%。我们证明了Agent Q本身实现了81.7%的成功率，而Agent Q + MCTS显著优于所有其他技术，在OpenTable上达到了95.4%的性能。](https://cdn.nlark.com/yuque/0/2025/png/39039688/1756219634737-588e4f97-7f51-4d36-bad4-f2de679dd47c.png)

<details class="lake-collapse"><summary id="u41c9461a"><span class="ne-text">【AI生成】图6解释</span></summary><p id="u6d16243d" class="ne-p"><span class="ne-text">这张图是论文最核心、最令人印象深刻的实验结果之一。它展示了Agent Q框架在</span><strong><span class="ne-text">真实世界网站（OpenTable）</span></strong><span class="ne-text"> 上的巨大威力，特别是当将训练后的模型与推理时的MCTS搜索相结合时，其性能达到了惊人的高度。</span></p><h4 id="d29dd"><strong><span class="ne-text">整体解读</span></strong></h4><p id="u75adc31a" class="ne-p"><span class="ne-text">这是一个条形图，横轴是不同的方法或模型，纵轴是它们在OpenTable任务上的</span><strong><span class="ne-text">成功率为百分比</span></strong><span class="ne-text">。这个图表清晰地呈现了一个“阶梯式”的性能提升过程。</span></p><h4 id="oFwvi"><strong><span class="ne-text">各条形柱详解</span></strong></h4><ol class="ne-ol"><li id="u98586d81" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">xLAM-v0.1-r</span></code><strong><span class="ne-text"> (深紫色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u92d85ba4" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：这是基线模型，一个经过监督微调的LLM。</span></li><li id="u313abb61" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">0.0%</span></strong><span class="ne-text">。这表明该模型在没有进一步优化的情况下，完全无法完成预订任务。</span></li></ul></ul><ol start="2" class="ne-ol"><li id="u22a1b406" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">LLaMA-3-70B-Instruct Base</span></code><strong><span class="ne-text"> (深紫色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="ub2cb8370" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：这是另一个更强的基线模型，即LLaMA-3 70B Instruct模型，但未经过任何特定训练。</span></li><li id="u0ba0ca14" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">18.6%</span></strong><span class="ne-text">。这代表了该强大模型的</span><strong><span class="ne-text">零样本（zero-shot）能力</span></strong><span class="ne-text">。虽然比第一个基线好得多，但仍然远低于人类水平。</span></li></ul></ul><ol start="3" class="ne-ol"><li id="u85cd77aa" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">GPT-4o</span></code><strong><span class="ne-text"> (深蓝色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u61a83307" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：作为另一个强大的基线模型进行比较。</span></li><li id="ucb6afee9" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">62.6%</span></strong><span class="ne-text">。这显示了即使是顶级的商业模型，在这种复杂的、多步骤的真实世界任务上，其零样本表现也有限。</span></li></ul></ul><ol start="4" class="ne-ol"><li id="uad5ad5cc" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">LLaMA-3-70B-Instruct RFT</span></code><strong><span class="ne-text"> (深蓝色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u37d4815a" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：在LLaMA-3 70B Instruct模型上应用了强化微调（Reinforced Fine-Tuning, RFT）。</span></li><li id="u729c8d55" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">67.2%</span></strong><span class="ne-text">。相比基线模型有显著提升，证明了RFT的有效性。</span></li></ul></ul><ol start="5" class="ne-ol"><li id="u3e413b9d" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">DPO</span></code><strong><span class="ne-text"> (青色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u82211396" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：在LLaMA-3 70B Instruct模型上应用了直接偏好优化（Direct Preference Optimization, DPO）。</span></li><li id="uf5afc21f" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">71.8%</span></strong><span class="ne-text">。比RFT稍好，但效果提升有限。</span></li></ul></ul><ol start="6" class="ne-ol"><li id="u71aced3d" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">AgentQ (No AI Feedback)</span></code><strong><span class="ne-text"> (青色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u43a83ae8" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：这是Agent Q框架的一个变体，它使用了MCTS+DPO，但</span><strong><span class="ne-text">去除了AI过程监督</span></strong><span class="ne-text">（即批评模型）。</span></li><li id="u16871bbb" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">75.2%</span></strong><span class="ne-text">。这个结果非常重要，它证明了即使没有AI反馈，仅通过MCTS探索生成数据并用DPO训练，也能带来显著的性能提升。</span></li></ul></ul><ol start="7" class="ne-ol"><li id="u032f02b6" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">AgentQ</span></code><strong><span class="ne-text"> (绿色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u6750be1a" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：这是完整的Agent Q框架，包含了</span><strong><span class="ne-text">MCTS探索、AI过程监督和DPO训练</span></strong><span class="ne-text">。</span></li><li id="u062a76c6" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">81.7%</span></strong><span class="ne-text">。这是</span><strong><span class="ne-text">零样本</span></strong><span class="ne-text">（zero-shot）性能，即模型在不使用MCTS搜索的情况下，仅凭自身学习到的能力执行任务。这个成绩已经是一个巨大的飞跃，是基线模型的4倍多。</span></li></ul></ul><ol start="8" class="ne-ol"><li id="ua272686a" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">RFT + MCTS</span></code><strong><span class="ne-text"> (浅绿色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="uc21d7b58" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：将RFT训练的模型与</span><strong><span class="ne-text">推理时的MCTS搜索</span></strong><span class="ne-text">相结合。</span></li><li id="u8b68a0d0" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">84.3%</span></strong><span class="ne-text">。这证明了MCTS搜索本身就能极大地提升模型的决策质量，使一个未经充分训练的模型达到很高的性能。</span></li></ul></ul><ol start="9" class="ne-ol"><li id="u55d980af" data-lake-index-type="0"><code class="ne-code"><span class="ne-text">AgentQ + MCTS</span></code><strong><span class="ne-text"> (亮黄色)</span></strong><span class="ne-text">：</span></li></ol><ul class="ne-list-wrap"><ul ne-level="1" class="ne-ul"><li id="u6d232503" data-lake-index-type="0"><strong><span class="ne-text">含义</span></strong><span class="ne-text">：这是最终的、最强的组合。它将</span><strong><span class="ne-text">经过完整训练的Agent Q模型</span></strong><span class="ne-text">与</span><strong><span class="ne-text">推理时的MCTS搜索</span></strong><span class="ne-text">结合起来。</span></li><li id="u94fe7415" data-lake-index-type="0"><strong><span class="ne-text">成绩</span></strong><span class="ne-text">：</span><strong><span class="ne-text">95.4%</span></strong><span class="ne-text">。这是所有方法中最高的，标志着Agent Q框架的巅峰性能。这个成绩不仅远超所有基线模型，甚至可能接近或超过了人类专家的水平（文中提到专家人类为59.6%，但这里指的可能是平均人类）。</span></li></ul></ul><h4 id="HU2TV"><strong><span class="ne-text">核心结论</span></strong></h4><ul class="ne-ul"><li id="u943f00b1" data-lake-index-type="0"><strong><span class="ne-text">训练至关重要</span></strong><span class="ne-text">：从 </span><code class="ne-code"><span class="ne-text">Base</span></code><span class="ne-text"> (18.6%) 到 </span><code class="ne-code"><span class="ne-text">AgentQ</span></code><span class="ne-text"> (81.7%) 的巨大差距，证明了通过MCTS+DPO+AI反馈进行训练，能将一个强大的基础模型的潜力发挥到极致。</span></li><li id="ua1c69f54" data-lake-index-type="0"><strong><span class="ne-text">搜索是关键</span></strong><span class="ne-text">：从 </span><code class="ne-code"><span class="ne-text">AgentQ</span></code><span class="ne-text"> (81.7%) 到 </span><code class="ne-code"><span class="ne-text">AgentQ + MCTS</span></code><span class="ne-text"> (95.4%) 的提升，证明了在推理时启用MCTS搜索，能让模型进行更高级的规划和探索，从而获得“超人”般的性能。</span></li><li id="u6e6c6c37" data-lake-index-type="0"><strong><span class="ne-text">两者结合是王道</span></strong><span class="ne-text">：</span><code class="ne-code"><span class="ne-text">AgentQ + MCTS</span></code><span class="ne-text"> 是一个完美的组合，它结合了</span><strong><span class="ne-text">强大的内在能力</span></strong><span class="ne-text">（训练）和</span><strong><span class="ne-text">卓越的外部工具</span></strong><span class="ne-text">（搜索），共同创造了前所未有的高成功率。</span></li></ul><p id="u5add68e8" class="ne-p"><span class="ne-text">总而言之，图6有力地证明了Agent Q框架在解决复杂、真实的Web代理任务方面的巨大潜力，尤其是在将训练和推理时的搜索能力相结合时，能够实现质的飞跃。</span></p></details>
