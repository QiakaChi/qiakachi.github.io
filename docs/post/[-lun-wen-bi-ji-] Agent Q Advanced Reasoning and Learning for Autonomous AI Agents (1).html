<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    <style>body[data-ui-pending] #content {opacity:0;transition:opacity 0.3s ease;}</style><script>document.documentElement.setAttribute('data-ui-pending','true');</script><link rel='stylesheet' href='assets/GmeekBaseTheme.css'><script src='assets/GmeekCustomizeCss.js' defer></script><script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script><script src='https://blog.meekdai.com/Gmeek/plugins/lightbox.js'></script>
    <link rel="icon" href="https://avatars.githubusercontent.com/u/98450248?v=4"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="![](https://cdn.nlark.com/yuque/0/2025/png/39039688/1756188334137-779c69de-bd11-4627-bda3-8351c18a08c0.png)

# Agent-Q
这篇论文的agent，不是传统的 Observation-Thinker-Actor 划分，而是 Observation-Actor（包括 Plan-Thought-Env Action-Explanation）。">
<meta property="og:title" content="[论文笔记] Agent Q Advanced Reasoning and Learning for Autonomous AI Agents (1)">
<meta property="og:description" content="![](https://cdn.nlark.com/yuque/0/2025/png/39039688/1756188334137-779c69de-bd11-4627-bda3-8351c18a08c0.png)

# Agent-Q
这篇论文的agent，不是传统的 Observation-Thinker-Actor 划分，而是 Observation-Actor（包括 Plan-Thought-Env Action-Explanation）。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://qiakachi.github.io/post/%5B-lun-wen-bi-ji-%5D%20Agent%20Q%20Advanced%20Reasoning%20and%20Learning%20for%20Autonomous%20AI%20Agents%20%281%29.html">
<meta property="og:image" content="https://avatars.githubusercontent.com/u/98450248?v=4">
<title>[论文笔记] Agent Q Advanced Reasoning and Learning for Autonomous AI Agents (1)</title>



</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}

</style>




<body>
    <div id="header">
<h1 class="postTitle">[论文笔记] Agent Q Advanced Reasoning and Learning for Autonomous AI Agents (1)</h1>
<div class="title-right">
    <a href="https://qiakachi.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/QiakaChi/qiakachi.github.io/issues/5" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5dc5fd3ea5024e483280e42a62b3ca6cfce77361a87a35e2cc7604e162e83e6e/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313735363138383333343133372d37373963363964652d626431312d343632372d626461332d3833353163313861303863302e706e67"><img src="https://camo.githubusercontent.com/5dc5fd3ea5024e483280e42a62b3ca6cfce77361a87a35e2cc7604e162e83e6e/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313735363138383333343133372d37373963363964652d626431312d343632372d626461332d3833353163313861303863302e706e67" alt="" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1756188334137-779c69de-bd11-4627-bda3-8351c18a08c0.png" style="max-width: 100%;"></a></p>
<h1>Agent-Q</h1>
<p>这篇论文的agent，不是传统的 Observation-Thinker-Actor 划分，而是 Observation-Actor（包括 Plan-Thought-Env Action-Explanation）。</p>
<p>模型可视为 POMDP（部分可观测马尔可夫决策过程）：(O, S, A, T, R, μ₀, γ)。</p>
<h2>Obervation（O）</h2>
<ul>
<li><strong>对象</strong>：用户指令和浏览器状态（HTML DOM）。</li>
</ul>
<h2>Actor（A）</h2>
<p>论文将计划（plan）、推理（thought）、环境动作（env action）和解释（explanation）这四个部分统一作为 composite action。训练时<strong>LLM会同时学习这四个子输出</strong>。</p>
<ul>
<li><strong>MCTS 的搜索空间</strong>：只对 env action 做树状探索和评估。</li>
<li><strong>DPO 优化的数据</strong>：是基于 MCTS 轨迹得到的偏好对，但在训练时会一起优化 plan / thought / explanation，因为它们被建模为和 env action 同属于一个 composite action。</li>
</ul>
<h3>Plan</h3>
<ul>
<li>初始步骤的顺序执行计划。 仅在<strong>初始观察</strong>之后的第一个动作会生成。</li>
</ul>
<h3>Thought</h3>
<ul>
<li>在后续的每一步，模型都会生成一个类似CoT的推理步骤（<code class="notranslate">atht</code>），这相当于一个内部的思考过程，解释了为什么选择这个动作。</li>
</ul>
<h3><strong>Env Action</strong></h3>
<ul>
<li>唯一<strong>直接与环境（浏览器）交互</strong>的部分， 包括点击网页元素（CLICK）、输入文本（TYPE）、滚动页面（SCROLL）、向用户请求信息（ASK USER）等。</li>
</ul>
<h3>Explanation</h3>
<ul>
<li>在执行完环境动作之后生成。给出对刚才动作的解释，用于表示 agent 的意图和当前状态。虽然不影响环境，但会影响之后的决策。</li>
</ul>
<h2>图表说明</h2>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b077bef407c25e3451ae394ba413dd0a91c3cb653a5351dfe174fc81fdaa542f/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313735363230343230363038362d35396464303466302d356665662d346230332d616334362d3538613465636563636566352e706e67"><img src="https://camo.githubusercontent.com/b077bef407c25e3451ae394ba413dd0a91c3cb653a5351dfe174fc81fdaa542f/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313735363230343230363038362d35396464303466302d356665662d346230332d616334362d3538613465636563636566352e706e67" alt="图2：我们向智能体提供以下输入格式，包括系统提示、执行历史、当前观察（以DOM表示）以及包含目标的用户查询。我们将智能体的输出格式分为一个总体的分步计划、思考、命令和状态代码。" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1756204206086-59dd04f0-5fef-4b03-ac46-58a4ececcef5.png" style="max-width: 100%;"></a></p>
<details><summary id="user-content-u838da6cf"><span>【AI生成】图2解释</span></summary><p id="user-content-u640ba4ce"><span>这张图是理解Agent Q智能体</span><strong><span>在推理/测试阶段（Inference Phase）如何工作</span></strong><span>的关键。它清晰地展示了智能体的“输入-输出”流程，即LLM如何根据当前环境信息进行推理并生成可执行的动作。</span></p><h4 id="user-content-sifbg"><strong><span>左侧：智能体输入 (Agent Input)</span></strong></h4><p id="user-content-u64f7ca14"><span>这是智能体在做出决策前接收到的所有信息。这些信息共同构成了其“上下文”或“记忆”。</span></p><ol><li id="user-content-u7857cd58"><code class="notranslate"><span>&lt;SYSTEM_PROMPT&gt;</span></code><strong><span> (系统提示)</span></strong><span>：</span></li></ol><ul><ul><li id="user-content-uc5634cc2"><span>这是一个固定的、预先设定的指令，用于指导智能体的整体行为和角色。例如，它可能告诉模型：“你是一个专业的餐厅预订助手，需要遵循严格的步骤来完成任务。”虽然图中没有显示具体内容，但它是模型行为的基础。</span></li></ul></ul><ol start="2"><li id="user-content-uaa048fc7"><code class="notranslate"><span>&lt;EXECUTION HISTORY&gt;</span></code><strong><span> (执行历史)</span></strong><span>：</span></li></ol><ul><ul><li id="user-content-ueeac0431"><span>这是智能体到目前为止已经采取的所有动作的记录。它包含了之前的状态和操作序列。</span></li><li id="user-content-u97cd7e3b"><span>在图中，它被简化为一个文本块，其中包含了</span><strong><span>用户查询</span></strong><span>（USER QUERY）和</span><strong><span>当前观察</span></strong><span>（CURRENT OBSERVATION）。</span></li><li id="user-content-ufc749b41"><span>用户查询：</span><code class="notranslate"><span>Book a reservation for the restaurant Coconi's on OpenTable for 2 people on June 17 2024 at 7:00pm</span></code><span>。这是智能体需要完成的最终目标。</span></li><li id="user-content-u2b9d6697"><span>当前观察：</span><code class="notranslate"><span>CURRENT OBSERVATION:</span></code><span> 后面跟着一个网页截图。这代表了智能体当前所处的环境状态——在OpenTable网站上，位于Coconi's餐厅的页面。</span></li></ul></ul><ol start="3"><li id="user-content-u84a9a8b6"><code class="notranslate"><span>CURRENT OBSERVATION:</span></code><strong><span> (当前观察)</span></strong><span>：</span></li></ol><ul><ul><li id="user-content-uc90042bb"><span>这是智能体感知到的</span><strong><span>实时环境状态</span></strong><span>。</span></li><li id="user-content-u041cdde6"><span>根据论文第3.1节，这个观察是以</span><strong><span>HTML DOM格式</span></strong><span>表示的。图中的截图就是DOM的一个可视化呈现。</span></li><li id="user-content-u9a49ea1f"><span>它包含了所有与任务相关的视觉和交互元素，如餐厅名称、日期选择器、时间选择器、人数选择器、按钮等。智能体需要解析这个DOM来理解环境。</span></li></ul></ul><h4 id="user-content-xq2uo"><strong><span>右侧：智能体输出 (Agent Output)</span></strong></h4><p id="user-content-uad960308"><span>这是智能体基于输入信息进行推理后生成的响应。它是一个结构化的、复合的动作，旨在指导执行器（Executor）。</span></p><ol><li id="user-content-udb97a8ad"><code class="notranslate"><span>PLAN:</span></code><strong><span> (计划)</span></strong><span>：</span></li></ol><ul><ul><li id="user-content-uadb1d440"><span>这是智能体为完成整个任务而制定的</span><strong><span>宏观策略</span></strong><span>。</span></li><li id="user-content-u6feb126e"><span>它是一个分步列表，列出了从当前状态到最终成功所需执行的一系列主要步骤。</span></li><li id="user-content-u68cfe763"><span>在图中，计划是：</span></li></ul></ul><ol><ol><ol><li id="user-content-ud5dc8b52"><span>选择预订日期（2024年5月22日）。</span></li><li id="user-content-ua7b7bb06"><span>选择预订时间（晚上7点）。</span></li><li id="user-content-u2744d97f"><span>选择用餐人数（4人）。</span></li><li id="user-content-ud43d7593"><span>点击“查找餐桌”按钮完成预订。</span></li></ol></ol></ol><ul><ul><li id="user-content-u9cb0ff25"><span>这个计划是</span><strong><span>高层级的</span></strong><span>，它为后续的思考和具体动作提供了方向。</span></li></ul></ul><ol start="2"><li id="user-content-u908f34d8"><code class="notranslate"><span>THOUGHT:</span></code><strong><span> (思考)</span></strong><span>：</span></li></ol><ul><ul><li id="user-content-u1e5f6717"><span>这是智能体在执行计划时的</span><strong><span>内部推理过程</span></strong><span>。</span></li><li id="user-content-u5c0abde6"><span>它解释了为什么选择当前的行动，以及对当前状态的理解。</span></li><li id="user-content-ufa873d0e"><span>在图中，思考是：</span></li></ul></ul><div><p id="user-content-u02b53b5e"><span>“我目前在OpenTable上的Coconi's餐厅页面，我需要选择预订的日期和时间，并选择用餐人数。我将专注于选择日期、时间和用餐人数。”</span></p></div><ul><ul><li id="user-content-u49863eae"><span>这个思考是</span><strong><span>低层级的</span></strong><span>，它聚焦于当前步骤的细节。它表明模型正在分析当前的DOM，识别出需要操作的元素（日期、时间、人数），并决定优先处理日期。</span></li></ul></ul><ol start="3"><li id="user-content-uf4d25c6e"><code class="notranslate"><span>COMMANDS:</span></code><strong><span> (命令)</span></strong><span>：</span></li></ol><ul><ul><li id="user-content-u9e1c3d57"><span>这是智能体生成的</span><strong><span>具体、可执行的指令</span></strong><span>，也是整个输出中最关键的部分。</span></li><li id="user-content-u3bb59793"><span>它直接传递给执行器（Executor）去操作浏览器。</span></li><li id="user-content-ubd312391"><span>在图中，命令是：</span></li></ul></ul><div><p id="user-content-ufe3e8f2c"><code class="notranslate"><span>CLICK &lt;select&gt;Date&lt;/select&gt;</span></code></p></div><ul><ul><li id="user-content-ud714f00b"><span>这个命令明确指出了要执行的操作（</span><code class="notranslate"><span>CLICK</span></code><span>）和要点击的目标（</span><code class="notranslate"><span>&lt;select&gt;Date&lt;/select&gt;</span></code><span>）。这里的</span><code class="notranslate"><span>&lt;select&gt;Date&lt;/select&gt;</span></code><span>是DOM中代表“日期选择器”的元素的HTML标签和ID（或类似标识符）。</span></li><li id="user-content-u914d5d93"><span>执行器会解析这个命令，在浏览器中找到对应的元素并触发点击事件。</span></li></ul></ul><ol start="4"><li id="user-content-ucf62140b"><code class="notranslate"><span>STATUS:</span></code><strong><span> (状态码)</span></strong><span>：</span></li></ol><ul><ul><li id="user-content-u7031f68b"><span>这是一个简单的状态指示，告诉系统下一步该做什么。</span></li><li id="user-content-u8fc5d380"><span>在图中，状态码是 </span><code class="notranslate"><span>CONTINUE</span></code><span>，意味着任务仍在进行中，需要继续下一个步骤。</span></li><li id="user-content-u4d6be41a"><span>其他可能的状态码可能是 </span><code class="notranslate"><span>SUCCESS</span></code><span>（任务成功）、</span><code class="notranslate"><span>FAILURE</span></code><span>（任务失败）或 </span><code class="notranslate"><span>ASK_USER</span></code><span>（需要用户介入）。</span></li></ul></ul><h4 id="user-content-kko35"><strong><span>总结</span></strong></h4><p id="user-content-u2cdfaef6"><span>图2的核心意义在于，它展示了一个</span><strong><span>结构化、可解释的AI Agent架构</span></strong><span>：</span></p><ul><li id="user-content-u9d2176fe"><strong><span>输入</span></strong><span>：智能体接收一个完整的上下文，包括长期目标（用户查询）、历史记录和当前环境快照（DOM）。</span></li><li id="user-content-u7605945c"><strong><span>输出</span></strong><span>：智能体通过多层推理，生成一个包含</span><strong><span>计划</span></strong><span>（宏观）、</span><strong><span>思考</span></strong><span>（微观）和</span><strong><span>命令</span></strong><span>（执行）的复合输出。</span></li><li id="user-content-u2d012f92"><strong><span>作用</span></strong><span>：这种结构化输出使得智能体的行为变得</span><strong><span>透明和可控</span></strong><span>。研究人员可以轻松地检查每个部分（计划、思考、命令）来调试和理解模型的决策过程。同时，</span><code class="notranslate"><span>COMMANDS</span></code><span> 部分确保了与真实世界的接口是标准化和可执行的。</span></li></ul></details>
<h1>训练阶段</h1>
<h2>MCTS（Monte Carlo Tree Search）</h2>
<h3>为什么使用MCTS</h3>
<p>只使用DPO进行实验后发现：DPO模型在WebShop中表现出贪婪搜索行为（几乎不使用“下一页”按钮进行探索）。为此引入MCTS引导Agent进行探索，</p>
<h3>MCTS工作原理</h3>
<ul>
<li><strong>作用</strong>：在训练阶段作为“数据生成器”（每个节点计算Q值），引导Agent在环境中探索。</li>
<li><strong>不调整参数</strong>：MCTS本身不直接更新模型参数，而是为DPO提供训练数据。</li>
<li><strong>Q值计算</strong>：
<ul>
<li>MCTS本身的Q值更新：<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e0746cf5bc14d7692bae7f16494b39d9d550570f236122e113d5e2b156c4d072/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62383631313036333862626563313434396162643361393136333866383066312e737667"><img src="https://camo.githubusercontent.com/e0746cf5bc14d7692bae7f16494b39d9d550570f236122e113d5e2b156c4d072/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62383631313036333862626563313434396162643361393136333866383066312e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/b86110638bbec1449abd3a91638f80f1.svg" style="max-width: 100%;"></a></li>
<li>AI过程监督产生的Q值：<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5ec8c4a51641ee4f00d78123d0dba0bc6bd4cababa9483db9086a411734cf153/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62663661356464616262393366653537666434353531613930313235366363652e737667"><img src="https://camo.githubusercontent.com/5ec8c4a51641ee4f00d78123d0dba0bc6bd4cababa9483db9086a411734cf153/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62663661356464616262393366653537666434353531613930313235366363652e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/bf6a5ddabb93fe57fd4551a901256cce.svg" style="max-width: 100%;"></a>（文中未给出具体的计算公式）</li>
</ul>
</li>
</ul>
<h4>选择</h4>
<ul>
<li><strong>UCB1</strong>：
<ul>
<li>UCB1平衡探索与利用。</li>
<li>最终Q值将代入UCB1公式<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e8df6d4ff2649650b6700bf600a9f837752c84eb3eefc5f2859e94b144f6f010/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f33323932643238653734623132633638636361316635353030303731383362652e737667"><img src="https://camo.githubusercontent.com/e8df6d4ff2649650b6700bf600a9f837752c84eb3eefc5f2859e94b144f6f010/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f33323932643238653734623132633638636361316635353030303731383362652e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/3292d28e74b12c68cca1f550007183be.svg" style="max-width: 100%;"></a></li>
<li>使用UCB1公式选择节点（动作）并执行，到达新节点（网页）。</li>
</ul>
</li>
<li><strong>AI过程监督</strong>：
<ul>
<li>由于网页环境奖励稀疏，引入一个基于LLM的“批评模型”（critic）。</li>
<li>该模型对当前节点下可能的K个动作进行<strong>打分和排名</strong>，提供中间奖励（intermediate reward）。</li>
<li>解决了信用分配（credit assignment）问题，指导搜索走向更优路径。</li>
<li>生成<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1c3ed0e35ba1560697e2e6effbd3bdd74a8cd81cae41bb07506c96059cd0f642/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62643234616537643133396130376134653232363361383039363637633463322e737667"><img src="https://camo.githubusercontent.com/1c3ed0e35ba1560697e2e6effbd3bdd74a8cd81cae41bb07506c96059cd0f642/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62643234616537643133396130376134653232363361383039363637633463322e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/bd24ae7d139a07a4e2263a809667c4c2.svg" style="max-width: 100%;"></a>。</li>
</ul>
</li>
</ul>
<h5>图表说明</h5>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ff379ceb4194414487eabb752b8827a49dc198fb3992ec7d6ecaebe7a025cda8/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313735363230343738363130392d32363231393664392d316365352d346333632d623434622d6239393437396335326266652e706e67"><img src="https://camo.githubusercontent.com/ff379ceb4194414487eabb752b8827a49dc198fb3992ec7d6ecaebe7a025cda8/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313735363230343738363130392d32363231393664392d316365352d346333632d623434622d6239393437396335326266652e706e67" alt="" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1756204786109-262196d9-1ce5-4c3c-b44b-b99479c52bfe.png" style="max-width: 100%;"></a></p>
<details><summary id="user-content-ub0be35df"><span>【AI生成】图4解释</span></summary><p id="user-content-u101db347"><span>这张图是理解Agent Q框架中</span><strong><span>AI过程监督</span></strong><span>（AI Process Supervision）这一核心创新的关键。它展示了在MCTS的“选择”（Selection）阶段，如何利用一个“批评模型”（LLM Critic）来解决网页环境中的稀疏奖励问题。</span></p><h4 id="user-content-ykwau"><strong><span>整体流程概览</span></strong></h4><p id="user-content-uee2465f9"><span>图4描绘了一个循环过程：</span></p><ol><li id="user-content-ud0883164"><strong><span>策略（Actor）提出动作</span></strong><span>：在当前网页状态（Agent Input）下，主模型（LLM Actor）生成多个可能的下一步动作。</span></li><li id="user-content-uff53a399"><strong><span>批评模型（Critic）进行排序</span></strong><span>：另一个独立的模型（LLM Critic）对这些动作进行评估和排序，判断哪个动作更有可能成功。</span></li><li id="user-content-u57f58510"><strong><span>利用排序结果</span></strong><span>：这个排序结果被用来指导MCTS的后续步骤，既在</span><strong><span>探索时</span></strong><span>（inference-time search）帮助选择更优路径，也在</span><strong><span>训练时</span></strong><span>（policy training）用于构建数据。</span></li></ol><h4 id="user-content-gy7kq"><strong><span>详细分解</span></strong></h4><ol><li id="user-content-udad3f218"><strong><span>左侧：Agent Input (智能体输入)</span></strong></li></ol><ul><ul><li id="user-content-uef2a76a0"><span>这是与图2相同的输入格式，包含了用户查询（</span><code class="notranslate"><span>Book a reservation for the restaurant Fogo de Chao...</span></code><span>）和当前观察到的OpenTable主页截图。</span></li><li id="user-content-uc4cd20f1"><span>当前状态是：用户需要在OpenTable上找到特定餐厅，但尚未输入任何信息。</span></li></ul></ul><ol start="2"><li id="user-content-uc504307b"><strong><span>中间：LLM Actor (主模型/策略)</span></strong></li></ol><ul><ul><li id="user-content-ufd552770"><span>这是执行任务的“大脑”，即经过初步训练的LLM。</span></li><li id="user-content-u042039b8"><span>它根据输入，生成了三个可能的下一步动作（</span><code class="notranslate"><span>Proposed Action 1</span></code><span>, </span><code class="notranslate"><span>Proposed Action 2</span></code><span>, </span><code class="notranslate"><span>Proposed Action 3</span></code><span>）：</span></li></ul></ul><ul><ul><ul><li id="user-content-ue24beac9"><strong><span>Action 1 (橙色)</span></strong><span>：点击日期选择器。这是一个合理的动作，但不够具体。</span></li><li id="user-content-uc775b561"><strong><span>Action 2 (绿色)</span></strong><span>：在搜索栏中输入“Terra - Eataly”。这是一个非常直接且高效的行动，能快速定位目标餐厅。</span></li><li id="user-content-u0f3d2ee4"><strong><span>Action 3 (红色)</span></strong><span>：导航回OpenTable首页。这是一个错误的动作，因为当前已经在主页上，这会浪费步骤。</span></li></ul></ul></ul><ol start="3"><li id="user-content-ub369e6e3"><strong><span>右侧：LLM Critic (批评模型)</span></strong></li></ol><ul><ul><li id="user-content-u4d5d88fd"><span>这是“自我监督”的关键。它是一个</span><strong><span>与主模型（Actor）完全相同的基础LLM模型</span></strong><span>（例如，都是LLaMA-3 70B）。它不负责执行任务，只负责“思考”和“评判”。</span></li><li id="user-content-u4fb8bf56"><span>它接收主模型提出的三个动作，并基于其自身的“直觉”对它们进行分析和排序。</span></li><li id="user-content-u5603b4e8"><strong><span>批评模型的思考</span></strong><span>（灰色框）：</span></li></ul></ul><div><p id="user-content-u53ffa838"><span>“在分析当前浏览器状态后，我注意到我们正在OpenTable网站上。此页面显示了带有可用预订时间的餐厅列表。页面上的相关元素是……”<br></span><span>“最有可能成功的命令是在搜索栏中输入搜索词‘Terra - Eataly’。”</span></p></div><ul><ul><li id="user-content-u7147fa5b"><span>这个思考过程表明，批评模型能够理解当前环境（在主页上，有搜索栏），并能评估不同动作的合理性。它认为</span><strong><span>Action 2</span></strong><span> 是最佳选择。</span></li></ul></ul><ol start="4"><li id="user-content-u6575e193"><strong><span>结果：动作排序与反馈</span></strong></li></ol><ul><ul><li id="user-content-uf7c0a3b2"><span>批评模型的判断结果体现在</span><strong><span>右侧的重新排列</span></strong><span>上：</span></li></ul></ul><ul><ul><ul><li id="user-content-u5a636958"><strong><span>Action 2 (绿色)</span></strong><span> 被排在第一位，因为它被判定为“最有可能成功的命令”。</span></li><li id="user-content-ucd8ef47a"><strong><span>Action 1 (橙色)</span></strong><span> 被排在第二位。</span></li><li id="user-content-u8eced427"><strong><span>Action 3 (红色)</span></strong><span> 被排在第三位，因为它是一个无效或低效的动作。</span></li></ul></ul></ul><ul><ul><li id="user-content-ufe45a758"><span>这个排序就是所谓的“</span><strong><span>AI过程监督</span></strong><span>”（AI Process Feedback）。它为每个动作提供了一个</span><strong><span>中间奖励</span></strong><span>（intermediate reward），即使该动作最终没有导致任务成功。</span></li></ul></ul><h4 id="user-content-z7poa"><strong><span>核心意义与应用</span></strong></h4><ul><li id="user-content-uc61f3294"><strong><span>解决稀疏奖励问题</span></strong><span>：在真实的网页环境中，只有在任务完成时才能获得“成功/失败”的最终奖励。如果一个动作（如Action 3）在早期就犯了错误，整个轨迹都会失败，模型无法知道是哪个具体的动作导致了失败。AI过程监督通过批评模型提供的即时反馈，解决了这个问题。它告诉MCTS：“虽然你没成功，但你的Action 2是正确的，而Action 3是错的。”</span></li><li id="user-content-uee5ba3b8"><strong><span>指导MCTS搜索</span></strong><span>：在MCTS的“选择”阶段，这个排序可以作为启发式信息，优先探索那些被批评模型评为“好”的动作路径，从而加速搜索过程。</span></li><li id="user-content-u34aa9121"><strong><span>构建训练数据</span></strong><span>：在训练阶段，这个排序结果被用来计算一个加权的Q值（见公式(10)），以构建用于DPO训练的“偏好对”。例如，在某个节点上，如果Action 2被排在第一，而Action 3被排在最后，那么这对动作就可以构成一个“优选” vs “劣选”的对比对，用于训练主模型。</span></li></ul><p id="user-content-ua2077dfd"><span>总而言之，图4生动地展示了Agent Q如何利用一个</span><strong><span>相同的、强大的LLM作为“自我批评者”</span></strong><span>，来为复杂的网页交互任务提供细粒度的、实时的反馈，从而极大地提升了学习效率和最终性能。</span></p></details>
<h4>扩展</h4>
<ul>
<li><strong>动作生成</strong>：使用基础LLM策略（<code class="notranslate">πθ</code>）在当前节点（网页状态）上采样K个可能的动作（如 <code class="notranslate">CLICK[ELEMENT ID]</code>, <code class="notranslate">TYPE[CONTENT]</code>）。</li>
<li><strong>执行</strong>：选择一个动作在浏览器环境中执行，进入下一个网页状态，创建新的子节点。</li>
</ul>
<h4>模拟</h4>
<ul>
<li><strong>过程</strong>：从新创建的叶节点开始，使用当前的策略模型 <code class="notranslate">πθ</code> 进行rollout，直到任务成功或失败。</li>
<li><strong>目的</strong>：快速评估该分支的最终奖励R（成功时R=1，失败时R=0）。</li>
</ul>
<h4>回溯</h4>
<ul>
<li><strong>更新</strong>：将模拟得到的最终奖励（R=1/0）从叶节点反向传播到根节点。</li>
<li><strong>更新内容</strong>：更新路径上每个 (state, action) 对的访问次数<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/0401a1721d8ef91fc180e0f0b9f1cb2ab2f28397cacc63a2185cfa2886daa715/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62663039303732376639343963616239393762653833363132633033343630352e737667"><img src="https://camo.githubusercontent.com/0401a1721d8ef91fc180e0f0b9f1cb2ab2f28397cacc63a2185cfa2886daa715/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62663039303732376639343963616239393762653833363132633033343630352e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/bf090727f949cab997be83612c034605.svg" style="max-width: 100%;"></a>和MCTS的平均<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/402d5c473cd9ec957f3d7c4e4e59a83377c303c738f2ff12bd0fb4664efc44c4/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f31666239646135333332623065366363323266393837306462633362306635372e737667"><img src="https://camo.githubusercontent.com/402d5c473cd9ec957f3d7c4e4e59a83377c303c738f2ff12bd0fb4664efc44c4/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f31666239646135333332623065366363323266393837306462633362306635372e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/1fb9da5332b0e6cc22f9870dbc3b0f57.svg" style="max-width: 100%;"></a>：
<ul>
<li><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e0746cf5bc14d7692bae7f16494b39d9d550570f236122e113d5e2b156c4d072/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62383631313036333862626563313434396162643361393136333866383066312e737667"><img src="https://camo.githubusercontent.com/e0746cf5bc14d7692bae7f16494b39d9d550570f236122e113d5e2b156c4d072/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62383631313036333862626563313434396162643361393136333866383066312e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/b86110638bbec1449abd3a91638f80f1.svg" style="max-width: 100%;"></a></li>
<li><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/91b7e5a01b34ea86561f743f351617070ff720807633f194e87e9358a34b59fb/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f39343265393764393662356433643833643165613462396337343461376461632e737667"><img src="https://camo.githubusercontent.com/91b7e5a01b34ea86561f743f351617070ff720807633f194e87e9358a34b59fb/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f39343265393764393662356433643833643165613462396337343461376461632e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/942e97d96b5d3d83d1ea4b9c744a7dac.svg" style="max-width: 100%;"></a></li>
</ul>
</li>
</ul>
<h2>DPO（Direct Preference Optimization）</h2>
<h3>为什么选用DPO</h3>
<ul>
<li>强化微调 (Reinforced Fine-Tuning, RFT)简单但性能通常低于标准RL。</li>
<li>DPO是经典RLHF优化流程的一种离线RL替代方案，无需在线采样。</li>
</ul>
<h3>DPO工作原理</h3>
<ul>
<li><strong>作用</strong>：训练时，LLM会通过微调（fine-tuning）的方式，调整其所有参数 <code class="notranslate">θ</code>，朝着使损失函数 DPO loss 最小化的方向进行优化。</li>
<li><strong>范围</strong>：DPO loss 是对整个<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/fbb5d62c1809581330ca0d09f8dd1dec8ad22709d67e79f62dbeff325e47c962/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63363161386233383765316362366334303630386634616536356436663661362e737667"><img src="https://camo.githubusercontent.com/fbb5d62c1809581330ca0d09f8dd1dec8ad22709d67e79f62dbeff325e47c962/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63363161386233383765316362366334303630386634616536356436663661362e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/c61a8b387e1cb6c40608f4ae65d6f6a6.svg" style="max-width: 100%;"></a> 的 log-likelihood（plan/thought/env/expl 部分全部拼在一起）做梯度更新的，因此虽然MCTS的数据针对env action，但DPO最终可以调整LLM的plan、thought和explanation。</li>
<li><strong>数据来源</strong>：使用MCTS生成的轨迹，构造偏好对（preferred vs. dispreferred）。</li>
</ul>
<h4>训练时优化的不是单独的Env Action</h4>
<p>被训练的策略模型（policy）即 LLM with parameters，输出是一个完整的 step action <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/fbb5d62c1809581330ca0d09f8dd1dec8ad22709d67e79f62dbeff325e47c962/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63363161386233383765316362366334303630386634616536356436663661362e737667"><img src="https://camo.githubusercontent.com/fbb5d62c1809581330ca0d09f8dd1dec8ad22709d67e79f62dbeff325e47c962/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63363161386233383765316362366334303630386634616536356436663661362e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/c61a8b387e1cb6c40608f4ae65d6f6a6.svg" style="max-width: 100%;"></a> ：</p>
<ul>
<li>第一步：<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/333b8eef73ea1eb3e77c3c89502af7e9e59fa5ac2eb10985bf2a56872170fd33/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f33313032623135353937306631663561613937356639616330616536616366642e737667"><img src="https://camo.githubusercontent.com/333b8eef73ea1eb3e77c3c89502af7e9e59fa5ac2eb10985bf2a56872170fd33/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f33313032623135353937306631663561613937356639616330616536616366642e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/3102b155970f1f5aa975f9ac0ae6acfd.svg" style="max-width: 100%;"></a></li>
<li>之后步骤：<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1b090854e77ae245d9bc2e3ae0a2f887618f8512455f068e0cd68b2adf1ed9fc/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f31393462616565343333623434393134626531376463646234386364306366342e737667"><img src="https://camo.githubusercontent.com/1b090854e77ae245d9bc2e3ae0a2f887618f8512455f068e0cd68b2adf1ed9fc/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f31393462616565343333623434393134626531376463646234386364306366342e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/194baee433b44914be17dcdb48cd0cf4.svg" style="max-width: 100%;"></a></li>
</ul>
<p>在优化时，DPO loss 是对整个 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/fbb5d62c1809581330ca0d09f8dd1dec8ad22709d67e79f62dbeff325e47c962/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63363161386233383765316362366334303630386634616536356436663661362e737667"><img src="https://camo.githubusercontent.com/fbb5d62c1809581330ca0d09f8dd1dec8ad22709d67e79f62dbeff325e47c962/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63363161386233383765316362366334303630386634616536356436663661362e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/c61a8b387e1cb6c40608f4ae65d6f6a6.svg" style="max-width: 100%;"></a> 的 log-likelihood（plan/thought/env/expl 部分全部拼在一起）做梯度更新的。</p>
<h5>公式 (1) 说明</h5>
<ul>
<li>公式 (1) 是论文中用于定义<strong>复合动作</strong>（composite action）的联合似然性（joint likelihood）的核心，解释了模型<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d4c1e2393794ed173f5736cf428e2342a2d961866aa1133eb0dc2935da2cf291/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f66336333363033653664376537366132363439333164656166366638303837662e737667"><img src="https://camo.githubusercontent.com/d4c1e2393794ed173f5736cf428e2342a2d961866aa1133eb0dc2935da2cf291/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f66336333363033653664376537366132363439333164656166366638303837662e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/f3c3603e6d7e76a264931deaf6f8087f.svg" style="max-width: 100%;"></a>在给定历史<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7809d16bdb5f155a540bb48af7e76758f0031cb85f88a33d9a453996c8ff9f03/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63613537643136616366343431323731653465383238336633343461336637302e737667"><img src="https://camo.githubusercontent.com/7809d16bdb5f155a540bb48af7e76758f0031cb85f88a33d9a453996c8ff9f03/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63613537643136616366343431323731653465383238336633343461336637302e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/ca57d16acf441271e4e8283f344a3f70.svg" style="max-width: 100%;"></a>的情况下，生成一个完整动作<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/0ec1caba2f0cb936b1481148eea8a4b45d4aae3956435f66424721f81cd5cecb/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f61393163316237353739356361623531663432616131393033356161383332342e737667"><img src="https://camo.githubusercontent.com/0ec1caba2f0cb936b1481148eea8a4b45d4aae3956435f66424721f81cd5cecb/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f61393163316237353739356361623531663432616131393033356161383332342e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/a91c1b75795cab51f42aa19035aa8324.svg" style="max-width: 100%;"></a>的概率是如何计算的。</li>
<li>该公式的结果<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c1baf15747bec784f7d0fef9a840f9338938aed816ec4beda44f3a56115f6c92/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f65636166386561636433666335356432623437616166636638663538336334302e737667"><img src="https://camo.githubusercontent.com/c1baf15747bec784f7d0fef9a840f9338938aed816ec4beda44f3a56115f6c92/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f65636166386561636433666335356432623437616166636638663538336334302e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/ecaf8eacd3fc55d2b47aafcf8f583c40.svg" style="max-width: 100%;"></a>将在后续的DPO优化中使用。</li>
</ul>
<details><summary id="user-content-u4428fbe9"><span>【相关原文】</span><span>3.1. Agent Formulation, page 6</span></summary><div><p id="user-content-u9453dfd4"><span>We denote the step action </span><span id="user-content-ierdj"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3269e632d5c3de59bb2a7b21021113aec855f7f641f65f189eba305c82259d4f/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62633138323734326332623162633162343166626662373562386136353531342e737667"><img src="https://camo.githubusercontent.com/3269e632d5c3de59bb2a7b21021113aec855f7f641f65f189eba305c82259d4f/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62633138323734326332623162633162343166626662373562386136353531342e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/bc182742c2b1bc1b41fbfb75b8a65514.svg" style="max-width: 100%;"></a></span><span> as a tuple of plan, thought, environment and explanation actions for the first step and thought, environment and explanation actions for subsequent steps. When optimizing models we consider the joint likelihood </span><span id="user-content-ajo35"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8e519f8866f0f066c686bbfa87b7b97d024b89317c17eee6b4abcc4b7c6faa2b/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f35393332316435643431336339313038336263633261386362663164373263642e737667"><img src="https://camo.githubusercontent.com/8e519f8866f0f066c686bbfa87b7b97d024b89317c17eee6b4abcc4b7c6faa2b/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f35393332316435643431336339313038336263633261386362663164373263642e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/59321d5d413c91083bcc2a8cbf1d72cd.svg" style="max-width: 100%;"></a></span></p><p id="user-content-u13d34b08"><span>for the initial action and </span><span id="user-content-ymmen"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/a30ff78b41455c6f2dca6f2f4f3b4e62bf761c905bea58316cfb3404fea72a5d/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f61626363643665356264306638303737353638383939626433636632333865322e737667"><img src="https://camo.githubusercontent.com/a30ff78b41455c6f2dca6f2f4f3b4e62bf761c905bea58316cfb3404fea72a5d/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f61626363643665356264306638303737353638383939626433636632333865322e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/abccd6e5bd0f8077568899bd3cf238e2.svg" style="max-width: 100%;"></a></span></p><p id="user-content-ue9a5fc96"><span>for subsequent actions, unlike some prior works Zhai et al. (2024), which down-weight the reasoning likelihood.</span></p></div></details>
1. 初始动作的联合似然 (Initial Action)
<p>对于第一步（t=1），模型需要同时生成四个组件。公式 (1) 将这四个部分的似然性相加：</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7822c838dccbc4a1f3d76dffaf144547bf73d84a51b6619c49c8043a99ea0a32/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f66383363383833303531386330323834313965343533376637363837636135332e737667"><img src="https://camo.githubusercontent.com/7822c838dccbc4a1f3d76dffaf144547bf73d84a51b6619c49c8043a99ea0a32/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f66383363383833303531386330323834313965343533376637363837636135332e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/f83c8830518c028419e4537f7687ca53.svg" style="max-width: 100%;"></a></p>
<details><summary id="user-content-u99898270"><span>t=1时似然公式的参数说明</span></summary><ul><li id="user-content-uf41c05ce"><span id="user-content-pbmnh"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e36c785d05b9730c8608719344e2fbf779c79785ade6bcbb937004d8c8ae8e74/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f35356636353664356239633266303431326333306663393461313230656531622e737667"><img src="https://camo.githubusercontent.com/e36c785d05b9730c8608719344e2fbf779c79785ade6bcbb937004d8c8ae8e74/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f35356636353664356239633266303431326333306663393461313230656531622e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/55f656d5b9c2f0412c30fc94a120ee1b.svg" style="max-width: 100%;"></a></span><span>: </span></li></ul><ul><ul><li id="user-content-uc0524659"><span>生成</span><strong><span>计划</span></strong><span>（Plan）的概率。</span></li><li id="user-content-ucfab8118"><span>在第一步，模型根据用户指令和初始网页状态</span><span id="user-content-yovuq"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/10ff3836c645eba38db73ae061b226809741ae39f2e9dbec364e90b68a0e98f2/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f61323062343737336531306237326631666237633762366132376439613664662e737667"><img src="https://camo.githubusercontent.com/10ff3836c645eba38db73ae061b226809741ae39f2e9dbec364e90b68a0e98f2/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f61323062343737336531306237326631666237633762366132376439613664662e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/a20b4773e10b72f1fb7c7b6a27d9a6df.svg" style="max-width: 100%;"></a></span><span>，生成一个高层次的、分步骤的任务执行方案。</span></li></ul></ul><ul><li id="user-content-u7dde6514"><span id="user-content-qfnzs"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b55725cb527592dfc297d240d17abcf5ea717d774acdd0becb0103c57308cb3e/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f38623764616332326661373362306466383539663239626530613466333139332e737667"><img src="https://camo.githubusercontent.com/b55725cb527592dfc297d240d17abcf5ea717d774acdd0becb0103c57308cb3e/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f38623764616332326661373362306466383539663239626530613466333139332e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/8b7dac22fa73b0df859f29be0a4f3193.svg" style="max-width: 100%;"></a></span><span>：</span></li></ul><ul><ul><li id="user-content-u59fdbcc7"><span>生成</span><strong><span>推理</span></strong><span>（Thought）的概率。</span></li><li id="user-content-u51e331b3"><span>模型在知道计划后，生成下一步的具体思考过程，解释为什么选择某个动作。</span></li></ul></ul><ul><li id="user-content-u8d573d02"><span id="user-content-boqwp"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/13cfc7bbee52bad4996ce925f25f90800118dcfa1bdf710a8318c99ef9cd45f8/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37313533616639613961373330623835366161393365313539376633393736632e737667"><img src="https://camo.githubusercontent.com/13cfc7bbee52bad4996ce925f25f90800118dcfa1bdf710a8318c99ef9cd45f8/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37313533616639613961373330623835366161393365313539376633393736632e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/7153af9a9a730b856aa93e1597f3976c.svg" style="max-width: 100%;"></a></span><span>：</span></li></ul><ul><ul><li id="user-content-u4e899019"><span>生成</span><strong><span>环境动作</span></strong><span>（Environment Action）的概率。</span></li><li id="user-content-u12ce523c"><span>这是真正与浏览器交互的指令（如 </span><code class="notranslate"><span>"CLICK[123]"</span></code><span>）。它的生成依赖于当前的状态</span><span id="user-content-zmmdn"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/10ff3836c645eba38db73ae061b226809741ae39f2e9dbec364e90b68a0e98f2/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f61323062343737336531306237326631666237633762366132376439613664662e737667"><img src="https://camo.githubusercontent.com/10ff3836c645eba38db73ae061b226809741ae39f2e9dbec364e90b68a0e98f2/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f61323062343737336531306237326631666237633762366132376439613664662e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/a20b4773e10b72f1fb7c7b6a27d9a6df.svg" style="max-width: 100%;"></a></span><span>、已生成的推理 </span><span id="user-content-yxza9"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e62a49736268dc597848fdda72b2f3c09bd54e72b49ed7735c118f46a500418c/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f35353037643938343233323039616538366362323363383038326433616365322e737667"><img src="https://camo.githubusercontent.com/e62a49736268dc597848fdda72b2f3c09bd54e72b49ed7735c118f46a500418c/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f35353037643938343233323039616538366362323363383038326433616365322e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/5507d98423209ae86cb23c8082d3ace2.svg" style="max-width: 100%;"></a></span><span>和计划</span><span id="user-content-xsd4v"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9367617ee248fa2d9f62a2bf4ec5bf7408a335a0bad3c2218e88ffe9e3869156/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f36666366303637393561616663323632386365623462373863343161663131302e737667"><img src="https://camo.githubusercontent.com/9367617ee248fa2d9f62a2bf4ec5bf7408a335a0bad3c2218e88ffe9e3869156/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f36666366303637393561616663323632386365623462373863343161663131302e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/6fcf06795aafc2628ceb4b78c41af110.svg" style="max-width: 100%;"></a></span><span>。</span></li></ul></ul><ul><li id="user-content-u80183486"><span id="user-content-sefnl"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/31fc14fba17b492bf121a722258b7dd616d191ed098d24cff28b841e275db028/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f65656664663361346264333362383231633631336534616361653238336463632e737667"><img src="https://camo.githubusercontent.com/31fc14fba17b492bf121a722258b7dd616d191ed098d24cff28b841e275db028/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f65656664663361346264333362383231633631336534616361653238336463632e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/eefdf3a4bd33b821c613e4acae283dcc.svg" style="max-width: 100%;"></a></span><span>：</span></li></ul><ul><ul><li id="user-content-uda13f8f4"><span>生成</span><strong><span>解释</span></strong><span>（Explanation）的概率。</span></li><li id="user-content-uca628dcc"><span>模型在生成了环境动作后，会提供一个解释，说明这个动作的意图是什么。</span></li></ul></ul></details>
<blockquote>
<p>这里使用的是对数似然性 (log π)，而不是直接的似然性 (π)。这是为了数值稳定性和方便数学运算（将乘法转换为加法）。</p>
</blockquote>
<ol start="2">
<li><strong>后续动作的联合似然 (Subsequent Actions)</strong></li>
</ol>
<p>对于第 t 步（t &gt; 1），模型不再需要生成“计划”，因为它已经有一个初步的计划。因此，后续动作的联合似然性只包含三个部分：</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/2e98166067b2269d63dc853490c853d690a08d882b53b2cffe06e4d6e8837148/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f32353031306635306161626161653137326134373933386638646232386335662e737667"><img src="https://camo.githubusercontent.com/2e98166067b2269d63dc853490c853d690a08d882b53b2cffe06e4d6e8837148/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f32353031306635306161626161653137326134373933386638646232386335662e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/25010f50aabaae172a47938f8db28c5f.svg" style="max-width: 100%;"></a></p>
<details><summary id="user-content-u97084ef1"><span>t&gt;1时似然公式的参数说明</span></summary><ul><li id="user-content-u60e72b3e"><span id="user-content-otnir"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/40d49e3c375349ca92bc050f6633c8e4c2c0d6acb21e75a48c1e9878aa24b40c/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37613539313031383464666337376431376639623065633366613063663037652e737667"><img src="https://camo.githubusercontent.com/40d49e3c375349ca92bc050f6633c8e4c2c0d6acb21e75a48c1e9878aa24b40c/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37613539313031383464666337376431376639623065633366613063663037652e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/7a5910184dfc77d17f9b0ec3fa0cf07e.svg" style="max-width: 100%;"></a></span><span>：根据当前的历史</span><span id="user-content-xyqmj"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ea3432fcc5265cfaacbfcc1584d8f48f822c9d6412d63a6cf9afcf1940e04eb2/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f38353235613637346133653161316531316366663734346462346165313530662e737667"><img src="https://camo.githubusercontent.com/ea3432fcc5265cfaacbfcc1584d8f48f822c9d6412d63a6cf9afcf1940e04eb2/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f38353235613637346133653161316531316366663734346462346165313530662e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/8525a674a3e1a1e11cff744db4ae150f.svg" style="max-width: 100%;"></a></span><span>（包括之前的动作和当前的网页状态），生成当前步骤的</span><strong><span>thought</span></strong><span>。 </span></li><li id="user-content-ud1d4f919"><span id="user-content-nvife"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/04fab64522a656831ea4d16a2fdd9c998209db900117499060c14fecbfa32ddb/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f33353038626663333837313161633163356166376637316334343337643135622e737667"><img src="https://camo.githubusercontent.com/04fab64522a656831ea4d16a2fdd9c998209db900117499060c14fecbfa32ddb/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f33353038626663333837313161633163356166376637316334343337643135622e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/3508bfc38711ac1c5af7f71c4437d15b.svg" style="max-width: 100%;"></a></span><span>：根据当前历史</span><span id="user-content-nsbye"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7809d16bdb5f155a540bb48af7e76758f0031cb85f88a33d9a453996c8ff9f03/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63613537643136616366343431323731653465383238336633343461336637302e737667"><img src="https://camo.githubusercontent.com/7809d16bdb5f155a540bb48af7e76758f0031cb85f88a33d9a453996c8ff9f03/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63613537643136616366343431323731653465383238336633343461336637302e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/ca57d16acf441271e4e8283f344a3f70.svg" style="max-width: 100%;"></a></span><span>和刚刚生成的推理</span><span id="user-content-yrtpq"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c06a94ab6accfecacb28c1940ff8591932d60910e0615b455309b23ff1c4fb6e/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f30633831376561633339393937663464373235376338316265363433326332612e737667"><img src="https://camo.githubusercontent.com/c06a94ab6accfecacb28c1940ff8591932d60910e0615b455309b23ff1c4fb6e/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f30633831376561633339393937663464373235376338316265363433326332612e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/0c817eac39997f4d7257c81be6432c2a.svg" style="max-width: 100%;"></a></span><span>，生成</span><strong><span>env action</span></strong><span>。 </span></li><li id="user-content-u4189e91b"><span id="user-content-hkg2e"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/438daf0eaecb43c71f569a1436d427075c10c4d2dfd8be3f0a9444a40e8a6099/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f30363035353632333731643239613734623665656632376465333433646434342e737667"><img src="https://camo.githubusercontent.com/438daf0eaecb43c71f569a1436d427075c10c4d2dfd8be3f0a9444a40e8a6099/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f30363035353632333731643239613734623665656632376465333433646434342e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/0605562371d29a74b6eef27de343dd44.svg" style="max-width: 100%;"></a></span><span>$: 根据当前历史</span><span id="user-content-ojri8"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7809d16bdb5f155a540bb48af7e76758f0031cb85f88a33d9a453996c8ff9f03/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63613537643136616366343431323731653465383238336633343461336637302e737667"><img src="https://camo.githubusercontent.com/7809d16bdb5f155a540bb48af7e76758f0031cb85f88a33d9a453996c8ff9f03/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63613537643136616366343431323731653465383238336633343461336637302e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/ca57d16acf441271e4e8283f344a3f70.svg" style="max-width: 100%;"></a></span><span>、生成的环境动作</span><span id="user-content-ydfdk"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/be3adf24a690e23a5f1d432ae9f49b9c15d10143af3d4118f465e252d994cf3d/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62383961363130326138623336396333363434306631333439366330643063332e737667"><img src="https://camo.githubusercontent.com/be3adf24a690e23a5f1d432ae9f49b9c15d10143af3d4118f465e252d994cf3d/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62383961363130326138623336396333363434306631333439366330643063332e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/b89a6102a8b369c36440f13496c0d0c3.svg" style="max-width: 100%;"></a></span><span>和推理</span><span id="user-content-l4awq"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f63dab81b6a66e56391b4362b3c95f08e898154116b1968a484a5a2ce2dcb412/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f34333164373136376139393131383335636230373431353361646131653263342e737667"><img src="https://camo.githubusercontent.com/f63dab81b6a66e56391b4362b3c95f08e898154116b1968a484a5a2ce2dcb412/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f34333164373136376139393131383335636230373431353361646131653263342e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/431d7167a9911835cb074153ada1e2c4.svg" style="max-width: 100%;"></a></span><span>，生成</span><strong><span>explanation</span></strong><span>。</span></li></ul></details>
<h4>从env action的偏好传导到其他部分</h4>
<p>MCTS的偏好对针对于env action候选；在此基础上，DPO从env action偏好传导到其他部分。</p>
<ol>
<li><strong>数据构造</strong>
<ul>
<li>MCTS rollout 给出的是“(history <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/7809d16bdb5f155a540bb48af7e76758f0031cb85f88a33d9a453996c8ff9f03/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63613537643136616366343431323731653465383238336633343461336637302e737667"><img src="https://camo.githubusercontent.com/7809d16bdb5f155a540bb48af7e76758f0031cb85f88a33d9a453996c8ff9f03/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63613537643136616366343431323731653465383238336633343461336637302e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/ca57d16acf441271e4e8283f344a3f70.svg" style="max-width: 100%;"></a>, env action <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3b4ddcf3f998da7caeb677c230a8c137374de3c4268075f98aa2aa2a8f87aa20/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f35383631316538316634386330633532663461363665313332346533643762622e737667"><img src="https://camo.githubusercontent.com/3b4ddcf3f998da7caeb677c230a8c137374de3c4268075f98aa2aa2a8f87aa20/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f35383631316538316634386330633532663461363665313332346533643762622e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/58611e81f48c0c52f4a66e1324e3d7bb.svg" style="max-width: 100%;"></a>) 的偏好”。</li>
<li>但训练时我们不单独输入 env action，而是把 <strong>完整的 composite action</strong>（包含当时生成的 plan/thought/env/expl）当作候选轨迹。</li>
</ul>
</li>
<li><strong>DPO Loss 计算</strong></li>
</ol>
<ul>
<li>单步DPO：
<ul>
<li><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/8008b73756c7c5d882eeddc6b1a53a778a115cc45a0b8f13e7ad16f39ba3055d/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f66373833663266393232613136353563396566333239663834323434353266662e737667"><img src="https://camo.githubusercontent.com/8008b73756c7c5d882eeddc6b1a53a778a115cc45a0b8f13e7ad16f39ba3055d/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f66373833663266393232613136353563396566333239663834323434353266662e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/f783f2f922a1655c9ef329f8424452ff.svg" style="max-width: 100%;"></a></li>
</ul>
</li>
</ul>
<details><summary id="user-content-ucd70aea2"><span>单步 DPO loss 参数说明</span></summary><ul><li id="user-content-u889a90c1"><span id="user-content-wchbf"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/465a3ccd770bf8fcb9f29c0bf2a7675b54668f5863b478a29f4591fcf8cc5924/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37396538623731313135393035373965663564343763303231323231643733612e737667"><img src="https://camo.githubusercontent.com/465a3ccd770bf8fcb9f29c0bf2a7675b54668f5863b478a29f4591fcf8cc5924/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37396538623731313135393035373965663564343763303231323231643733612e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/79e8b7111590579ef5d47c021221d73a.svg" style="max-width: 100%;"></a></span><span>：训练时要最小化这个损失函数。</span></li><li id="user-content-ude84e9f6"><span id="user-content-vgqls"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9d4b6379073d6fd9c51670e55e97d02db3f203df2d84992b3d80f4b7ea258e68/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37666263643632303534626438333737306334353038383431616539633961342e737667"><img src="https://camo.githubusercontent.com/9d4b6379073d6fd9c51670e55e97d02db3f203df2d84992b3d80f4b7ea258e68/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37666263643632303534626438333737306334353038383431616539633961342e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/7fbcd62054bd83770c4508841ae9c9a4.svg" style="max-width: 100%;"></a></span><span>：被训练的策略模型（policy），即 LLM with parameters。</span></li></ul><ul><ul><li id="user-content-uf7cd4687"><span id="user-content-flsr6"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/230093b6e945f4e0fff5fa066dcc85e8eb966a7a0c833dad3400046dd4a3c357/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f65643561346161356530393265333033613639633630383538326337306462392e737667"><img src="https://camo.githubusercontent.com/230093b6e945f4e0fff5fa066dcc85e8eb966a7a0c833dad3400046dd4a3c357/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f65643561346161356530393265333033613639633630383538326337306462392e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/ed5a4aa5e092e303a69c608582c70db9.svg" style="max-width: 100%;"></a></span><span>：被训练的参数，它输出给定历史</span><span id="user-content-mjwu8"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/811c7d988e66c190e8dd125c1fa0b05ebdbbcd383d88250d224a92064bd9fe42/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f65633536366565343661633638303764626165333463303434363762373235622e737667"><img src="https://camo.githubusercontent.com/811c7d988e66c190e8dd125c1fa0b05ebdbbcd383d88250d224a92064bd9fe42/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f65633536366565343661633638303764626165333463303434363762373235622e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/ec566ee46ac6807dbae34c04467b725b.svg" style="max-width: 100%;"></a></span><span>下，生成某个 composite action </span><span id="user-content-svhlt"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/434f5a89839b08d496f883834fa8e0a1d425c4450b00eef2cfc63fa80d5aff4c/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f32303937353138643232376265633762666337623664383232373866646261312e737667"><img src="https://camo.githubusercontent.com/434f5a89839b08d496f883834fa8e0a1d425c4450b00eef2cfc63fa80d5aff4c/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f32303937353138643232376265633762666337623664383232373866646261312e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/2097518d227bec7bfc7b6d82278fdba1.svg" style="max-width: 100%;"></a></span><span>的概率。</span></li></ul></ul><ul><li id="user-content-u69951107"><span id="user-content-nizgw"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b29b2ab89895ef0cba63207c223844ed9135aa48d958f4d3f920baa7c0a1aa9f/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37303830623837386165333061366363376137343333363133656264306232642e737667"><img src="https://camo.githubusercontent.com/b29b2ab89895ef0cba63207c223844ed9135aa48d958f4d3f920baa7c0a1aa9f/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37303830623837386165333061366363376137343333363133656264306232642e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/7080b878ae30a6cc7a7433613ebd0b2d.svg" style="max-width: 100%;"></a></span><span>：参考模型（reference policy）。通常就是初始的 SFT 模型，用来稳定训练，防止策略无限制偏离原始分布。</span></li><li id="user-content-u5af2de1d"><span id="user-content-jbnjb"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9cf7cfaafd75b906ea6afa9162251a4c74781cbcf87051d1eb07cf6562b20c25/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f35393033663533663466386336326639396466373765346533636362633465332e737667"><img src="https://camo.githubusercontent.com/9cf7cfaafd75b906ea6afa9162251a4c74781cbcf87051d1eb07cf6562b20c25/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f35393033663533663466386336326639396466373765346533636362633465332e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/5903f53f4f8c62f99df77e4e3ccbc4e3.svg" style="max-width: 100%;"></a></span><span>：数据集，包含很多个偏好对 (preference pairs)。每个样本形如 </span><span id="user-content-fdtxx"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/e565ebecb5d22dca7c2e88f67d7e7d2350dd0b9e8c890554a2b9cb31444aae03/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f31626639366533643237636666353831383237376231313735643435363831642e737667"><img src="https://camo.githubusercontent.com/e565ebecb5d22dca7c2e88f67d7e7d2350dd0b9e8c890554a2b9cb31444aae03/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f31626639366533643237636666353831383237376231313735643435363831642e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/1bf96e3d27cff5818277b1175d45681d.svg" style="max-width: 100%;"></a></span><span>。</span></li></ul><ul><ul><li id="user-content-u114f3825"><span id="user-content-lwjrw"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/811c7d988e66c190e8dd125c1fa0b05ebdbbcd383d88250d224a92064bd9fe42/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f65633536366565343661633638303764626165333463303434363762373235622e737667"><img src="https://camo.githubusercontent.com/811c7d988e66c190e8dd125c1fa0b05ebdbbcd383d88250d224a92064bd9fe42/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f65633536366565343661633638303764626165333463303434363762373235622e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/ec566ee46ac6807dbae34c04467b725b.svg" style="max-width: 100%;"></a></span><span>：当时的历史（环境状态 + 已有动作）。</span></li><li id="user-content-u218e346e"><span id="user-content-xwnyp"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/220267af2f8f269c594f669ca677494fbd4131c5a8ba3bc1c074720c9e380de1/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f66373961363131316136656435663731303732383936343764643632636233662e737667"><img src="https://camo.githubusercontent.com/220267af2f8f269c594f669ca677494fbd4131c5a8ba3bc1c074720c9e380de1/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f66373961363131316136656435663731303732383936343764643632636233662e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/f79a6111a6ed5f7107289647dd62cb3f.svg" style="max-width: 100%;"></a></span><span>：较优的动作（来自 MCTS Q 值高的 env action 轨迹）。</span></li><li id="user-content-ud805af8a"><span id="user-content-vzjab"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/76d51494afa7d937e8d639eb0665fe88787c45c7e04dc45ad65dbe82297f7e0b/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f34643938363962316631333038313361653033653432376263373937646334332e737667"><img src="https://camo.githubusercontent.com/76d51494afa7d937e8d639eb0665fe88787c45c7e04dc45ad65dbe82297f7e0b/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f34643938363962316631333038313361653033653432376263373937646334332e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/4d9869b1f130813ae03e427bc797dc43.svg" style="max-width: 100%;"></a></span><span>：较劣的动作。</span></li></ul></ul><ul><li id="user-content-u6a77cd52"><span id="user-content-xsix1"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/6eb9c297816e3353cb9eb1c0df0496d3e80fbe8708543cc06ae36d7f021ef1c2/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63643933316433656261396238383133306465333938383163633931663830332e737667"><img src="https://camo.githubusercontent.com/6eb9c297816e3353cb9eb1c0df0496d3e80fbe8708543cc06ae36d7f021ef1c2/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63643933316433656261396238383133306465333938383163633931663830332e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/cd931d3eba9b88130de39881cc91f803.svg" style="max-width: 100%;"></a></span><span>：对数据集里的所有偏好对取平均（数学上的“期望”）。所以最终 loss 是在整个训练集上的平均损失。</span></li><li id="user-content-u3acc0de5"><span id="user-content-zupdi"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5274f9c77a5b01e0bd71a18eccd0b8917effd091edd0bb032defff07c476a449/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f36313030313538383032653732326138386331356566633130316663323735622e737667"><img src="https://camo.githubusercontent.com/5274f9c77a5b01e0bd71a18eccd0b8917effd091edd0bb032defff07c476a449/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f36313030313538383032653732326138386331356566633130316663323735622e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/6100158802e722a88c15efc101fc275b.svg" style="max-width: 100%;"></a></span><span>：缩放系数，控制 policy 与参考模型之间的偏差有多敏感。</span></li></ul><ul><ul><li id="user-content-u026a7af4"><span id="user-content-g21st"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5274f9c77a5b01e0bd71a18eccd0b8917effd091edd0bb032defff07c476a449/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f36313030313538383032653732326138386331356566633130316663323735622e737667"><img src="https://camo.githubusercontent.com/5274f9c77a5b01e0bd71a18eccd0b8917effd091edd0bb032defff07c476a449/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f36313030313538383032653732326138386331356566633130316663323735622e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/6100158802e722a88c15efc101fc275b.svg" style="max-width: 100%;"></a></span><span> 大 → 更强调区分优劣动作；</span></li><li id="user-content-u9504861c"><span id="user-content-m47tc"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/5274f9c77a5b01e0bd71a18eccd0b8917effd091edd0bb032defff07c476a449/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f36313030313538383032653732326138386331356566633130316663323735622e737667"><img src="https://camo.githubusercontent.com/5274f9c77a5b01e0bd71a18eccd0b8917effd091edd0bb032defff07c476a449/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f36313030313538383032653732326138386331356566633130316663323735622e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/6100158802e722a88c15efc101fc275b.svg" style="max-width: 100%;"></a></span><span> 小 → 更温和地调整。</span></li></ul></ul><ul><li id="user-content-u1c8e02f0"><span id="user-content-qgbui"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/423738b3b93fbca6900cb5e86694a59f330134e6a85fc98c61d74abba55ebd22/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f34306538363963316431333939373535336566306366393736653039623262352e737667"><img src="https://camo.githubusercontent.com/423738b3b93fbca6900cb5e86694a59f330134e6a85fc98c61d74abba55ebd22/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f34306538363963316431333939373535336566306366393736653039623262352e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/40e869c1d13997553ef0cf976e09b2b5.svg" style="max-width: 100%;"></a></span><span>：当前模型在历史 </span><span id="user-content-slses"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3cfc05a27696485f910dc5f33fb56c9db5b372a4b0be37dddf72752ab79540b9/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37613562383936376433376438346235383939326363386163316262633263362e737667"><img src="https://camo.githubusercontent.com/3cfc05a27696485f910dc5f33fb56c9db5b372a4b0be37dddf72752ab79540b9/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37613562383936376433376438346235383939326363386163316262633263362e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/7a5b8967d37d84b58992cc8ac1bbc2c6.svg" style="max-width: 100%;"></a></span><span> 下输出优动作 </span><span id="user-content-cfpwe"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/220267af2f8f269c594f669ca677494fbd4131c5a8ba3bc1c074720c9e380de1/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f66373961363131316136656435663731303732383936343764643632636233662e737667"><img src="https://camo.githubusercontent.com/220267af2f8f269c594f669ca677494fbd4131c5a8ba3bc1c074720c9e380de1/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f66373961363131316136656435663731303732383936343764643632636233662e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/f79a6111a6ed5f7107289647dd62cb3f.svg" style="max-width: 100%;"></a></span><span> 的概率。</span></li><li id="user-content-u41f4a739"><span id="user-content-eeksp"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/b525f99e9e9cbe906030e4af8f238802e7bf28005a61279471fbdff97b8d5487/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f39613639306136303463316430323363653065356231326662353436663231302e737667"><img src="https://camo.githubusercontent.com/b525f99e9e9cbe906030e4af8f238802e7bf28005a61279471fbdff97b8d5487/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f39613639306136303463316430323363653065356231326662353436663231302e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/9a690a604c1d023ce0e5b12fb546f210.svg" style="max-width: 100%;"></a></span><span>：同理，是劣动作的概率。</span></li><li id="user-content-ub86b588a"><span id="user-content-cqsg4"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/f46acb18f9cfc4dc3931d45bb7001d79ef34934eb81377f890f7359666821fb2/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f30376531656466373839633964643765613762313566316536376339383132652e737667"><img src="https://camo.githubusercontent.com/f46acb18f9cfc4dc3931d45bb7001d79ef34934eb81377f890f7359666821fb2/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f30376531656466373839633964643765613762313566316536376339383132652e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/07e1edf789c9dd7ea7b15f1e67c9812e.svg" style="max-width: 100%;"></a></span><span>：这里的 </span><span id="user-content-ctgob"><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/573e98e2bdf96530bd02829fddc70fcd6ca0c58e4a7365bcbb4ffe23e8cdbf72/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37383864663162613334346233303932646566373539306431626536623464342e737667"><img src="https://camo.githubusercontent.com/573e98e2bdf96530bd02829fddc70fcd6ca0c58e4a7365bcbb4ffe23e8cdbf72/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37383864663162613334346233303932646566373539306431626536623464342e737667" data-canonical-src="https://cdn.nlark.com/yuque/__latex/788df1ba344b3092def7590d1be6b4d4.svg" style="max-width: 100%;"></a></span><span> 是 sigmoid 函数。log-sigmoid 的形式和对比学习类似，确保：</span></li></ul><ul><ul><li id="user-content-uab88fc25"><span>如果优动作相对参考模型的概率比劣动作更大 → loss 变小；</span></li><li id="user-content-ua4e273d8"><span>如果模型错误地更偏向劣动作 → loss 变大，强迫模型纠正。</span></li></ul></ul></details>
<ul>
<li>轨迹级DPO：
<ul>
<li><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/eb1df00490b3c8279dde6ead99da34b479f3abbefd43d875aa6a8b1ce9494fc4/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f38366237333833643330346335343135643839626630663635343262626264612e737667"><img src="https://camo.githubusercontent.com/eb1df00490b3c8279dde6ead99da34b479f3abbefd43d875aa6a8b1ce9494fc4/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f38366237333833643330346335343135643839626630663635343262626264612e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/86b7383d304c5415d89bf0f6542bbbda.svg" style="max-width: 100%;"></a></li>
</ul>
</li>
<li>虽然偏好标签来自 env action，但训练信号被应用到了 <strong>plan/thought/expl + env</strong> 整体序列的 likelihood。</li>
<li>采用无策略重放缓冲区，缓解计算资源，避免独立参考模型。</li>
</ul>
<ol start="3">
<li><strong>条件传导</strong>
<ul>
<li>因为 env action 的选择是<strong>依赖于 plan/thought/expl 的上下文</strong>（模型是自回归的），如果 thought/expl 给出的“上下文”更有助于生成更优的 env action，它们的概率分布就会被调整。</li>
<li>换句话说，DPO 在优化 env action 的同时，<strong>顺带推动模型去产生那些“能更好引导 env action” 的 plan/thought/expl</strong>。</li>
</ul>
</li>
</ol>
<h5>打个比方</h5>
<p>假设有两个 rollout：</p>
<ul>
<li><strong>坏轨迹</strong>：
<ul>
<li>thought: “我应该立即点确认”</li>
<li>env action: <code class="notranslate">CLICK [wrong button]</code></li>
</ul>
</li>
<li><strong>好轨迹</strong>：
<ul>
<li>thought: “先检查餐厅是否有空位”</li>
<li>env action: <code class="notranslate">CLICK [availability button]</code></li>
</ul>
</li>
</ul>
<p>MCTS 会给 <strong>第二条 env action</strong> 更高的 Q 值。<br>
→ DPO 偏好对就标记第二条轨迹更优。<br>
→ 优化时，整个序列 (thought + env + explanation) 的 log-likelihood 都会被提升。<br>
→ 于是模型学到“这种 thought 更可能导致好 env action”，以后也更倾向生成这种思路。</p>
<h2>通过强化学习提升零样本性能</h2>
<ul>
<li><strong>目标</strong>：利用MCTS生成的轨迹数据，通过DPO离线训练，提升模型的内在能力。</li>
<li><strong>理论基础</strong>：Theorem 1 表明，若偏好对（preference pairs）基于最优Q值生成，则DPO优化等价于最优RL策略。</li>
<li><strong>偏好对构造</strong>：
<ul>
<li>在MCTS树同一父节点（状态）的每个节点，比较不同子节点（动作）的Q值。</li>
<li>如果两个动作的Q值差异超过阈值 <a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ea2fa2ead107f55a33167d25ecf56f4752917beea94df69db56423761a57171f/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f35396332663164326461613561303364336235356234373764666530376332372e737667"><img src="https://camo.githubusercontent.com/ea2fa2ead107f55a33167d25ecf56f4752917beea94df69db56423761a57171f/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f35396332663164326461613561303364336235356234373764666530376332372e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/59c2f1d2daa5a03d3b55b477dfe07c27.svg" style="max-width: 100%;"></a>，则构造一个偏好对。</li>
<li><strong>最终Q值</strong>：采用加权平均<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/cc064c3dac9e47faf011f47cb195d973dea331933c1339b1ba276ec3d6c21fe7/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f33326333636131663430623838313364633065383330666535356366316334372e737667"><img src="https://camo.githubusercontent.com/cc064c3dac9e47faf011f47cb195d973dea331933c1339b1ba276ec3d6c21fe7/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f33326333636131663430623838313364633065383330666535356366316334372e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/32c3ca1f40b8813dc0e830fe55cf1c47.svg" style="max-width: 100%;"></a>
<ul>
<li><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/03d44c46f0668c18755542e23d680f1f9e586860dd71b48c8034a027cb488325/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f38663735323366613131636630333139333136643437666262386563376661312e737667"><img src="https://camo.githubusercontent.com/03d44c46f0668c18755542e23d680f1f9e586860dd71b48c8034a027cb488325/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f38663735323366613131636630333139333136643437666262386563376661312e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/8f7523fa11cf0319316d47fbb8ec7fa1.svg" style="max-width: 100%;"></a>：通过MCTS回溯得到的。</li>
<li><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1c3ed0e35ba1560697e2e6effbd3bdd74a8cd81cae41bb07506c96059cd0f642/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62643234616537643133396130376134653232363361383039363637633463322e737667"><img src="https://camo.githubusercontent.com/1c3ed0e35ba1560697e2e6effbd3bdd74a8cd81cae41bb07506c96059cd0f642/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62643234616537643133396130376134653232363361383039363637633463322e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/bd24ae7d139a07a4e2263a809667c4c2.svg" style="max-width: 100%;"></a>：通过AI过程监督得到的。</li>
</ul>
</li>
<li>偏好对<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/80cd0e768fca3488add2d27a494d8ecd1411dd224dbc6604d253782ddeafd1a8/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f33316335353166313134373331366136666465326462316266373634333664642e737667"><img src="https://camo.githubusercontent.com/80cd0e768fca3488add2d27a494d8ecd1411dd224dbc6604d253782ddeafd1a8/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f33316335353166313134373331366136666465326462316266373634333664642e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/31c551f1147316a6fde2db1bf76436dd.svg" style="max-width: 100%;"></a>：当处于<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/999dadb92ebe6601b013503f16ca1b4f159b76d6d6c6e1c6b86a7ccf49a1d162/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63643764613635353037326362363036643836366165643062383335343533352e737667"><img src="https://camo.githubusercontent.com/999dadb92ebe6601b013503f16ca1b4f159b76d6d6c6e1c6b86a7ccf49a1d162/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f63643764613635353037326362363036643836366165643062383335343533352e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/cd7da655072cb606d866aed0b8354535.svg" style="max-width: 100%;"></a>状态时，应该选择<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/af68f6ba83d6cee8fcf2e273f2bf4f8ab4163736e95a8c753bcfbc2f34336eaa/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62326134646530343666386239306262616362386237353065396166376130642e737667"><img src="https://camo.githubusercontent.com/af68f6ba83d6cee8fcf2e273f2bf4f8ab4163736e95a8c753bcfbc2f34336eaa/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f62326134646530343666386239306262616362386237353065396166376130642e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/b2a4de046f8b90bbacb8b750e9af7a0d.svg" style="max-width: 100%;"></a>，而不是<a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/33754bd0cdad998dd988a36b6525eb62135c32326b34df554a76d95346d90f1e/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37323730643238306133666565636263326463623338356131346234326532622e737667"><img src="https://camo.githubusercontent.com/33754bd0cdad998dd988a36b6525eb62135c32326b34df554a76d95346d90f1e/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f5f5f6c617465782f37323730643238306133666565636263326463623338356131346234326532622e737667" alt="image" data-canonical-src="https://cdn.nlark.com/yuque/__latex/7270d280a3feecbc2dcb385a14b42e2b.svg" style="max-width: 100%;"></a>。</li>
</ul>
</li>
<li><strong>结果</strong>：训练后的模型（Agent Q）即使在不使用MCTS的情况下（零样本），其性能也远超基线。</li>
</ul>
<h3>图表说明</h3>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/1bbe32b4bc74c761e4b81513c40c0bad6a88d020ae75dbcc9fca05d45727b671/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313735363230333739363637392d64633066653430382d353462652d346165302d393830632d6335626362633338663432322e706e67"><img src="https://camo.githubusercontent.com/1bbe32b4bc74c761e4b81513c40c0bad6a88d020ae75dbcc9fca05d45727b671/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313735363230333739363637392d64633066653430382d353462652d346165302d393830632d6335626362633338663432322e706e67" alt="图1：我们使用蒙特卡洛树搜索（MCTS）来指导轨迹收集，并通过直接偏好优化（DPO）迭代地提升模型性能。我们从左侧开始，从数据集的任务列表中采样一个用户查询。我们使用UCB1作为启发式方法，迭代地扩展搜索树，以平衡不同动作的探索与利用。我们将树中每个节点累积获得的奖励存储起来，在本图中，更深的绿色表示更高的奖励，更深的红色表示更低的奖励。为了构建偏好数据集，我们计算MCTS平均Q值和由反馈语言模型生成的分数的加权得分，以构造用于DPO的对比对。策略被优化，并且可以被迭代地改进。" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1756203796679-dc0fe408-54be-4ae0-980c-c5bcbc38f422.png" style="max-width: 100%;"></a></p>
<details><summary id="user-content-u969dd33e"><span>【AI生成】图1解释</span></summary><p id="user-content-uab7c21ee"><span>这张图是理解Agent Q框架核心思想的“全景图”。它清晰地展示了</span><strong><span>训练阶段</span></strong><span>（Training Phase）的整个流程，特别是如何将</span><strong><span>MCTS</span></strong><span>和</span><strong><span>DPO</span></strong><span>结合起来。</span></p><h4 id="user-content-dvgwj"><strong><span>整体流程概览</span></strong></h4><p id="user-content-ub6a1e640"><span>整个过程是一个循环：</span></p><ol><li id="user-content-u1c08d6a0"><strong><span>起点</span></strong><span>：从一个具体的任务开始（如“在OpenTable上预订餐厅”）。</span></li><li id="user-content-u7404d9fa"><strong><span>MCTS探索</span></strong><span>：使用MCTS算法在可能的行动路径上进行探索，生成大量的成功和失败的交互轨迹。</span></li><li id="user-content-uae55f086"><strong><span>数据构建</span></strong><span>：利用MCTS探索过程中产生的信息（Q值、AI反馈），构建一个“偏好对”数据集。</span></li><li id="user-content-ufe71b1be"><strong><span>DPO训练</span></strong><span>：使用这个偏好对数据集，通过DPO算法对基础LLM模型进行微调，使其学习到更优的决策策略。</span></li><li id="user-content-u92c45450"><strong><span>迭代</span></strong><span>：用新训练好的模型再次进行MCTS探索，如此循环往复，不断迭代优化模型。</span></li></ol><h4 id="user-content-qulsy"><strong><span>详细分解</span></strong></h4><ol><li id="user-content-uc107a26a"><strong><span>左侧：MCTS 搜索树 (The Search Tree)</span></strong></li></ol><ul><ul><li id="user-content-u6bc151ba"><strong><span>根节点 (Root Node)</span></strong><span>：代表初始状态，即用户的任务指令（“Booking a Reservation on Open Table”）。</span></li><li id="user-content-ub7fa5806"><strong><span>树枝 (Branches)</span></strong><span>：代表了在执行任务时可能采取的不同动作序列。</span></li><li id="user-content-u4e6a3f9c"><strong><span>节点 (Nodes)</span></strong><span>：代表了网页上的一个具体状态或决策点。例如，“导航到错误的餐厅”、“选择错误的日期”等。</span></li><li id="user-content-uf348f21a"><strong><span>颜色编码 (Color Coding)</span></strong><span>：</span></li></ul></ul><ul><ul><ul><li id="user-content-u1c98919c"><strong><span>深绿色 (Darker Green)</span></strong><span>：表示该路径（或该节点）的累积奖励高，意味着这条路径更有可能通向成功。</span></li><li id="user-content-u0177c0d3"><strong><span>深红色 (Darker Red)</span></strong><span>：表示该路径（或该节点）的累积奖励低，意味着这条路径很可能导致失败。</span></li><li id="user-content-u748ac48c"><span>这个颜色编码直观地展示了MCTS如何通过反复模拟和回溯，为每条路径“打分”，从而引导搜索向更成功的方向发展。</span></li></ul></ul></ul><ul><ul><li id="user-content-ub3886e3a"><strong><span>路径示例</span></strong><span>：</span></li></ul></ul><ul><ul><ul><li id="user-content-u19eb3822"><strong><span>成功路径 (SUCCESS)</span></strong><span>：</span><code class="notranslate"><span>1 -&gt; 3 -&gt; 5 -&gt; 6 -&gt; 7</span></code><span>。这条路径经过一系列正确的操作，最终完成了预订，因此被标记为“SUCCESS”。</span></li><li id="user-content-u015128d3"><strong><span>失败路径 (FAILURE)</span></strong><span>：</span><code class="notranslate"><span>1 -&gt; 2</span></code><span> 和 </span><code class="notranslate"><span>3 -&gt; 4</span></code><span>。这些路径因为选择了错误的餐厅或日期而失败，因此被标记为“FAILURE”。</span></li></ul></ul></ul><ol start="2"><li id="user-content-u92979bb8"><strong><span>右侧：AI过程监督 (AI Process Supervision)</span></strong></li></ol><ul><ul><li id="user-content-ub64d72f0"><span>这个部分展示了MCTS是如何在探索过程中获得“中间奖励”的。</span></li><li id="user-content-u134459a1"><strong><span>背景</span></strong><span>：在真实的网页环境中，只有在任务完成时才会得到最终的“成功/失败”奖励（稀疏奖励）。这使得模型很难判断中间步骤的好坏（信用分配问题）。</span></li><li id="user-content-u66361722"><strong><span>解决方案 - AI反馈</span></strong><span>：作者引入了一个“反馈语言模型”（Feedback Language Model），它也是一个LLM（可能是和主模型相同的模型）。</span></li><li id="user-content-u7c0c21c8"><strong><span>工作方式</span></strong><span>：</span></li></ul></ul><ol><ol><ol><li id="user-content-u25ec07bc"><span>当MCTS到达一个决策点（比如要点击哪个按钮）时，它会从主模型那里采样出多个可能的动作（K个）。</span></li><li id="user-content-u29fbbb75"><span>然后，</span><strong><span>反馈语言模型</span></strong><span>会被询问：“在当前情况下，你认为这些动作哪个更有助于完成任务？”</span></li><li id="user-content-u2bbfc14d"><span>反馈模型会根据其自身的“直觉”对这些动作进行排序和打分。</span></li></ol></ol></ol><ul><ul><li id="user-content-ub69b03c4"><strong><span>目的</span></strong><span>：这个由AI生成的“过程监督”分数（Process Feedback Score）作为一个</span><strong><span>中间奖励</span></strong><span>，被用来指导MCTS的搜索。它让MCTS能“提前知道”某个动作是否看起来“正确”，从而优先探索那些被AI认为是好主意的路径。</span></li></ul></ul><ol start="3"><li id="user-content-u3dafa1d5"><strong><span>连接与整合 (Connecting the Dots)</span></strong></li></ol><ul><ul><li id="user-content-ua3d3b457"><span>图中的箭头表明了MCTS搜索的结果（即所有探索过的路径及其Q值、AI反馈分数）被用来构建一个“偏好对”数据集。</span></li><li id="user-content-uada7184b"><span>具体来说，对于搜索树中的每一个节点，如果两个子节点的Q值差异足够大（超过阈值 </span><code class="notranslate"><span>θthreshold</span></code><span>），就会形成一个“优选”（preferred）和“劣选”（dispreferred）的对比对。</span></li><li id="user-content-u01a5a799"><span>这个对比对的数据集就是</span><strong><span>DPO算法的输入</span></strong><span>。DPO会利用这些数据来训练主模型，让模型学会在相同的历史下，生成“优选”响应的概率远高于“劣选”响应的概率。</span></li></ul></ul><h4 id="user-content-iopwx"><strong><span>总结</span></strong></h4><p id="user-content-u23816a0e"><span>图1的核心意义在于，它展示了一个</span><strong><span>闭环的、自我强化的学习系统</span></strong><span>：</span></p><ul><li id="user-content-ufedc035f"><strong><span>MCTS</span></strong><span> 是一个强大的“探索者”和“数据生成器”，它通过结合</span><strong><span>AI过程监督</span></strong><span>来解决稀疏奖励的问题，生成高质量的训练数据。</span></li><li id="user-content-u6614ad93"><strong><span>DPO</span></strong><span> 是一个高效的“学习者”，它利用MCTS生成的“谁更好”的偏好数据，直接优化模型的参数。</span></li><li id="user-content-u0faeca6b"><span>两者结合，使得模型能够从自己的探索经验中持续学习，不断提升在复杂、多步骤任务中的决策能力。</span></li></ul></details>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9939dfa1205f6cfdb91e37745239c67b5fbcf9fe39ccfab5d83ac461d66eadc5/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313735363230353133363737342d65353966653038382d653263392d343839342d623762372d3539383664653939623533632e706e67"><img src="https://camo.githubusercontent.com/9939dfa1205f6cfdb91e37745239c67b5fbcf9fe39ccfab5d83ac461d66eadc5/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313735363230353133363737342d65353966653038382d653263392d343839342d623762372d3539383664653939623533632e706e67" alt="" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1756205136774-e59fe088-e2c9-4894-b7b7-5986de99b53c.png" style="max-width: 100%;"></a></p>
<details><summary id="user-content-u4416c6c0"><span>【AI生成】算法1解释</span></summary><div><p id="user-content-u178298e4"><strong><span>算法1：MCTS引导的直接偏好优化（MCTS Guided Direct Preference Optimization）</span></strong></p><p id="user-content-ud7731271"><strong><span>输入</span></strong><span>：</span><code class="notranslate"><span>πθ0</span></code><span>：初始LLM策略，</span><code class="notranslate"><span>DT</span></code><span>：代理必须在环境中完成的任务数据集，</span><code class="notranslate"><span>N</span></code><span>：迭代次数，</span><code class="notranslate"><span>B</span></code><span>：每次迭代的样本数，</span><code class="notranslate"><span>T</span></code><span>：MCTS树深度，</span><code class="notranslate"><span>ℬ</span></code><span>：重放缓冲区（replay buffer），</span><code class="notranslate"><span>θthreshold</span></code><span>：公式(10)中的值阈值，</span><code class="notranslate"><span>K</span></code><span>：为MCTS采样的动作数量</span></p><p id="user-content-u6dbff0de"><strong><span>输出</span></strong><span>：</span><code class="notranslate"><span>πθN</span></code><span>，训练好的LLM策略</span></p><p id="user-content-u2536243f"><strong><span>for i = 1 to N do</span></strong><span><br></span><span>    </span><code class="notranslate"><span>πref ← πθi</span></code><span>, </span><code class="notranslate"><span>πθi ← πθi−1</span></code><span><br></span><span>    从 </span><code class="notranslate"><span>DT</span></code><span> 中采样一批大小为 </span><code class="notranslate"><span>B</span></code><span> 的任务<br></span><span>    </span><strong><span>for each task in batch do</span></strong><span><br></span><span>        初始化根节点 </span><code class="notranslate"><span>h0</span></code><span><br></span><span>        </span><strong><span>for t = 1 to T do</span></strong><span><br></span><span>            </span><strong><span>选择（Selection）</span></strong><span>：使用树策略（UCB1；7）从根节点遍历到叶节点<br></span><span>            </span><strong><span>轨迹回放（Trajectory Rollout）</span></strong><span>：从选定节点的轨迹开始，使用 </span><code class="notranslate"><span>πθi</span></code><span> 回放轨迹，直到达到终止状态<br></span><span>            </span><strong><span>回溯（Backpropagation）</span></strong><span>：自底向上地反向传播价值估计（8）<br></span><span>        </span><strong><span>end for</span></strong><span><br></span><span>        从回放中收集轨迹并存储在重放缓冲区 </span><code class="notranslate"><span>ℬ</span></code><span> 中<br></span><span>    </span><strong><span>end for</span></strong><span><br></span><span>    构造偏好对 </span><code class="notranslate"><span>DP = {(ht, awt, alt)}T−1 t=1</span></code><span>，其中 </span><code class="notranslate"><span>ht ∼ DP</span></code><span>。对于每个步骤 </span><code class="notranslate"><span>t</span></code><span> 的节点，比较每对子节点，并构造生成的动作对 </span><code class="notranslate"><span>(aw, al)</span></code><span>，如果采取该动作的价值差异 </span><code class="notranslate"><span>|Q(ht, aw) - Q(ht, al)| &gt; θthreshold</span></code><span>，其中 </span><code class="notranslate"><span>Q(ht, aw)</span></code><span> 和 </span><code class="notranslate"><span>Q(ht, al)</span></code><span> 是通过公式(10)计算的。<br></span><span>    使用公式(5)中的DPO目标函数，结合 </span><code class="notranslate"><span>DP</span></code><span> 和 </span><code class="notranslate"><span>πref</span></code><span>，优化LLM策略 </span><code class="notranslate"><span>πθi</span></code><span><br></span><strong><span>end for</span></strong></p></div><hr id="user-content-wkyzw"><p id="user-content-u0f720212"><span>这个算法是Agent Q框架的核心，它将</span><strong><span>MCTS搜索</span></strong><span>和</span><strong><span>DPO训练</span></strong><span>紧密地结合在一起，形成一个</span><strong><span>迭代优化</span></strong><span>的闭环。它的主要目标是利用MCTS探索产生的“经验”来不断改进LLM策略。</span></p><h4 id="user-content-zwuut"><strong><span>整体流程概览</span></strong></h4><p id="user-content-u8b5dbf61"><span>整个过程是一个循环，共进行 </span><code class="notranslate"><span>N</span></code><span> 次迭代：</span></p><ol><li id="user-content-u43032446"><strong><span>准备阶段</span></strong><span>：</span></li></ol><ul><ul><li id="user-content-u8c3b53b7"><span>在每次迭代 </span><code class="notranslate"><span>i</span></code><span> 开始时，将上一次迭代训练好的模型 </span><code class="notranslate"><span>πθi−1</span></code><span> 设置为当前的参考模型 </span><code class="notranslate"><span>πref</span></code><span>。</span></li><li id="user-content-u1bfc7dbe"><span>将当前要训练的模型初始化为 </span><code class="notranslate"><span>πθi</span></code><span>，它在本次迭代中会根据新数据被更新。</span></li></ul></ul><ol start="2"><li id="user-content-ua8609158"><strong><span>数据收集阶段（MCTS）</span></strong><span>：</span></li></ol><ul><ul><li id="user-content-u6253f200"><span>从任务数据集 </span><code class="notranslate"><span>DT</span></code><span> 中随机抽取一批任务（</span><code class="notranslate"><span>B</span></code><span> 个）。</span></li><li id="user-content-u54e55d6a"><span>对于每一个任务，执行一个完整的MCTS搜索过程（见第5.1节）：</span></li></ul></ul><ul><ul><ul><li id="user-content-u1db96cf7"><strong><span>选择（Selection）</span></strong><span>：使用UCB1公式从根节点（任务初始状态）向下遍历，直到找到一个叶节点（未探索的决策点）。</span></li><li id="user-content-u901acf35"><strong><span>轨迹回放（Trajectory Rollout）</span></strong><span>：从这个叶节点开始，使用当前的LLM策略 </span><code class="notranslate"><span>πθi</span></code><span> 进行模拟（rollout），直到任务成功或失败，得到一个完整的交互轨迹。</span></li><li id="user-content-u675b5bd5"><strong><span>回溯（Backpropagation）</span></strong><span>：将这个轨迹的最终奖励（R=1/0）从叶节点反向传播到根节点，更新路径上所有 </span><code class="notranslate"><span>(状态, 动作)</span></code><span> 对的访问次数 </span><code class="notranslate"><span>N(ht, ait)</span></code><span> 和平均Q值 </span><code class="notranslate"><span>Q(ht, ait)</span></code><span>。</span></li></ul></ul></ul><ul><ul><li id="user-content-u1e77d369"><span>所有这些由MCTS生成的轨迹都被收集起来，并存入一个</span><strong><span>重放缓冲区</span></strong><span> </span><code class="notranslate"><span>ℬ</span></code><span>。</span></li></ul></ul><ol start="3"><li id="user-content-u4a354b77"><strong><span>数据处理与训练阶段（DPO）</span></strong><span>：</span></li></ol><ul><ul><li id="user-content-ub5b15a93"><strong><span>构建偏好对</span></strong><span>：从重放缓冲区 </span><code class="notranslate"><span>ℬ</span></code><span> 中提取数据，为DPO训练构建“对比对”。具体来说，对于MCTS树中每个时间步 </span><code class="notranslate"><span>t</span></code><span> 的节点 </span><code class="notranslate"><span>ht</span></code><span>，会比较其所有子节点（即不同的可能动作）。如果两个动作 </span><code class="notranslate"><span>aw</span></code><span> 和 </span><code class="notranslate"><span>al</span></code><span> 的Q值差异超过预设的阈值 </span><code class="notranslate"><span>θthreshold</span></code><span>，就认为 </span><code class="notranslate"><span>aw</span></code><span> 是更优的，从而构成一个“优选” vs “劣选”的偏好对 </span><code class="notranslate"><span>(ht, aw, al)</span></code><span>。</span></li><li id="user-content-ub76b676e"><strong><span>模型训练</span></strong><span>：使用这些构建好的偏好对数据集 </span><code class="notranslate"><span>DP</span></code><span> 和参考模型 </span><code class="notranslate"><span>πref</span></code><span>，通过DPO的损失函数（公式5）来优化当前的LLM策略 </span><code class="notranslate"><span>πθi</span></code><span>。这一步会更新模型的内部参数 </span><code class="notranslate"><span>θ</span></code><span>。</span></li></ul></ul><h4 id="user-content-ljnph"><strong><span>关键细节解析</span></strong></h4><ul><li id="user-content-ub3b96873"><strong><span>重放缓冲区（Replay Buffer </span></strong><code class="notranslate"><span>ℬ</span></code><strong><span>）</span></strong><span>：这是一个非常重要的组件。它允许算法在一次MCTS搜索中收集大量轨迹，并在后续的多个DPO训练迭代中重复使用这些数据，从而提高了数据利用效率，避免了每次都重新进行昂贵的MCTS搜索。</span></li><li id="user-content-u230a6789"><code class="notranslate"><span>πref</span></code><strong><span> 的作用</span></strong><span>：</span><code class="notranslate"><span>πref</span></code><span> 是一个固定的参考模型，在本次迭代中不会被改变。它在DPO的损失函数中作为“基准”，用来衡量新策略 </span><code class="notranslate"><span>πθi</span></code><span> 相对于旧策略的改进程度，防止模型在训练过程中偏离太远（out-of-distribution drift）。</span></li><li id="user-content-u0c527fc8"><code class="notranslate"><span>θthreshold</span></code><strong><span> 的作用</span></strong><span>：这个阈值用于过滤掉那些Q值差异很小、难以判断优劣的行动对。只保留那些差距明显的对比对，可以提高训练信号的质量，使DPO学习到更清晰的偏好。</span></li><li id="user-content-u85969121"><strong><span>迭代优化</span></strong><span>：这是算法的精髓。经过一轮DPO训练后，模型 </span><code class="notranslate"><span>πθi</span></code><span> 会变得更强。在下一轮迭代中，这个更强的模型 </span><code class="notranslate"><span>πθi</span></code><span> 又会被用作新的 </span><code class="notranslate"><span>πθi−1</span></code><span> 来进行MCTS搜索。这意味着新的MCTS搜索会基于一个更聪明的“大脑”进行，从而能探索出更优的路径，产生更高质量的训练数据，进而让模型变得更强大。这个正向循环是Agent Q性能提升的关键。</span></li></ul><h4 id="user-content-kr1bp"><strong><span>总结</span></strong></h4><p id="user-content-u5cf76302"><span>算法1完美地诠释了论文的核心思想：“</span><strong><span>用搜索来指导学习，用学习来增强搜索</span></strong><span>”。MCTS作为一个强大的探索者，负责生成高质量的数据（轨迹和偏好对）；DPO作为一个高效的学习者，负责利用这些数据来优化模型。两者通过迭代的方式相互促进，共同推动LLM策略的持续进化。</span></p></details></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://qiakachi.github.io">QiakaChi's Note</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","QiakaChi/qiakachi.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}



</script>


</html>
