<!DOCTYPE html>
<html data-color-mode="light" data-dark-theme="dark" data-light-theme="light" lang="zh-CN">
<head>
    <meta content="text/html; charset=utf-8" http-equiv="content-type" />
    <meta name="viewport" content="width=device-width,initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <link href='https://mirrors.sustech.edu.cn/cdnjs/ajax/libs/Primer/21.0.7/primer.css' rel='stylesheet' />
    <style>body[data-ui-pending] #content {opacity:0;transition:opacity 0.3s ease;}</style><script>document.documentElement.setAttribute('data-ui-pending','true');</script><link rel='stylesheet' href='assets/GmeekBaseTheme.css'><script src='assets/GmeekCustomizeCss.js' defer></script><script src='https://blog.meekdai.com/Gmeek/plugins/GmeekTOC.js'></script><script src='https://blog.meekdai.com/Gmeek/plugins/lightbox.js'></script>
    <link rel="icon" href="https://avatars.githubusercontent.com/u/98450248?v=4"><script>
        let theme = localStorage.getItem("meek_theme") || "light";
        document.documentElement.setAttribute("data-color-mode", theme);
    </script>
<meta name="description" content="https://arxiv.org/abs/2510.18234

> 参考：  
[https://www.bilibili.com/video/BV1YssezhEhj/](https://www.bilibili.com/video/BV1YssezhEhj/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=0b90e25774e5586a51f60079f57d9588)
>
> [https://zhuanlan.zhihu.com/p/81714646334](https://zhuanlan.zhihu.com/p/81714646334)
>

# 1 引言
![](https://cdn.nlark.com/yuque/0/2025/png/39039688/1764147741549-12f7948c-c086-4e36-a0e8-fc74408fd262.png)

当前的LLMs在处理长文本时，计算复杂度与序列长度呈二次方增长，导致巨大的计算挑战。">
<meta property="og:title" content="[论文笔记] DeepSeek-OCR: Contexts Optical Compression">
<meta property="og:description" content="https://arxiv.org/abs/2510.18234

> 参考：  
[https://www.bilibili.com/video/BV1YssezhEhj/](https://www.bilibili.com/video/BV1YssezhEhj/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&vd_source=0b90e25774e5586a51f60079f57d9588)
>
> [https://zhuanlan.zhihu.com/p/81714646334](https://zhuanlan.zhihu.com/p/81714646334)
>

# 1 引言
![](https://cdn.nlark.com/yuque/0/2025/png/39039688/1764147741549-12f7948c-c086-4e36-a0e8-fc74408fd262.png)

当前的LLMs在处理长文本时，计算复杂度与序列长度呈二次方增长，导致巨大的计算挑战。">
<meta property="og:type" content="article">
<meta property="og:url" content="https://qiakachi.github.io/post/%5B-lun-wen-bi-ji-%5D%20DeepSeek-OCR-%20Contexts%20Optical%20Compression.html">
<meta property="og:image" content="https://avatars.githubusercontent.com/u/98450248?v=4">
<title>[论文笔记] DeepSeek-OCR: Contexts Optical Compression</title>



</head>
<style>
body{box-sizing: border-box;min-width: 200px;max-width: 900px;margin: 20px auto;padding: 45px;font-size: 16px;font-family: sans-serif;line-height: 1.25;}
#header{display:flex;padding-bottom:8px;border-bottom: 1px solid var(--borderColor-muted, var(--color-border-muted));margin-bottom: 16px;}
#footer {margin-top:64px; text-align: center;font-size: small;}

</style>

<style>
.postTitle{margin: auto 0;font-size:40px;font-weight:bold;}
.title-right{display:flex;margin:auto 0 0 auto;}
.title-right .circle{padding: 14px 16px;margin-right:8px;}
#postBody{border-bottom: 1px solid var(--color-border-default);padding-bottom:36px;}
#postBody hr{height:2px;}
#cmButton{height:48px;margin-top:48px;}
#comments{margin-top:64px;}
.g-emoji{font-size:24px;}
@media (max-width: 600px) {
    body {padding: 8px;}
    .postTitle{font-size:24px;}
}

</style>




<body>
    <div id="header">
<h1 class="postTitle">[论文笔记] DeepSeek-OCR: Contexts Optical Compression</h1>
<div class="title-right">
    <a href="https://qiakachi.github.io" id="buttonHome" class="btn btn-invisible circle" title="首页">
        <svg class="octicon" width="16" height="16">
            <path id="pathHome" fill-rule="evenodd"></path>
        </svg>
    </a>
    
    <a href="https://github.com/QiakaChi/qiakachi.github.io/issues/10" target="_blank" class="btn btn-invisible circle" title="Issue">
        <svg class="octicon" width="16" height="16">
            <path id="pathIssue" fill-rule="evenodd"></path>
        </svg>
    </a>
    

    <a class="btn btn-invisible circle" onclick="modeSwitch();" title="切换主题">
        <svg class="octicon" width="16" height="16" >
            <path id="themeSwitch" fill-rule="evenodd"></path>
        </svg>
    </a>

</div>
</div>
    <div id="content">
<div class="markdown-body" id="postBody"><p><a href="https://arxiv.org/abs/2510.18234" rel="nofollow">https://arxiv.org/abs/2510.18234</a></p>
<blockquote>
<p>参考：<br>
<a href="https://www.bilibili.com/video/BV1YssezhEhj/?spm_id_from=333.1007.top_right_bar_window_custom_collection.content.click&amp;vd_source=0b90e25774e5586a51f60079f57d9588" rel="nofollow">https://www.bilibili.com/video/BV1YssezhEhj/</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/81714646334" rel="nofollow">https://zhuanlan.zhihu.com/p/81714646334</a></p>
</blockquote>
<h1>1 引言</h1>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ba21d265684f7471718cc17a99c3cbac94c9c62e317526a4b9192c9feec1c529/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134373734313534392d31326637393438632d633038362d346533362d613065382d6663373434303866643236322e706e67"><img src="https://camo.githubusercontent.com/ba21d265684f7471718cc17a99c3cbac94c9c62e317526a4b9192c9feec1c529/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134373734313534392d31326637393438632d633038362d346533362d613065382d6663373434303866643236322e706e67" alt="" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1764147741549-12f7948c-c086-4e36-a0e8-fc74408fd262.png" style="max-width: 100%;"></a></p>
<p>当前的LLMs在处理长文本时，计算复杂度与序列长度呈二次方增长，导致巨大的计算挑战。二次方增长是由Attention的计算方式导致的，除非改进结构，否则很难避免。</p>
<p>目前主流的VLMs为了缓解这个问题，使用了如双塔架构、基于分块的方法和自适应分辨率编码等架构，但都仍然存在着不足，例如Vary等双塔架构<strong>难以支持 pipeline parallel</strong>，基于分块的方法和自适应分辨率编码都会<strong>生成过多的vision tokens</strong>，或者<strong>激活内存消耗极大</strong>，引发GPU显存溢出。</p>
<p>针对这个问题，deepseek-ocr的作者提出：能否把文本用图片的形式压缩，以减少token的数量？如果能够在压缩token的情况下依然能保存和压缩前几乎相同的信息量，就说明这个方法是可行的。值得补充的是，ICLR 2023 的《Language modelling with pixels》可能是第一篇讨论如何把文本变成图像然后用ViT理解它的paper，并且由于不需要基于character模型中过长的sequence，从而解决了vocabulary的瓶颈。DeepSeek-OCR在理论方面可能不算突破，但具有一定工程价值。</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/462d785f09892bf02178112457183351cfffb5e5da678eecfcc4296f47d38143/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134373737353039302d34633331333063662d343363662d343938312d613962372d6562313538323164656333642e706e67"><img src="https://camo.githubusercontent.com/462d785f09892bf02178112457183351cfffb5e5da678eecfcc4296f47d38143/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134373737353039302d34633331333063662d343363662d343938312d613962372d6562313538323164656333642e706e67" alt="" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1764147775090-4c3130cf-43cf-4981-a9b7-eb15821dec3d.png" style="max-width: 100%;"></a></p>
<h1>2 架构</h1>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/66099400429d138e0fe99a6d7719ee9f4884103f8b74375e7f3f764aa6f0b10a/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134373837343635332d32666163386332372d653663642d346632372d613037372d3032656163623234373935302e706e67"><img src="https://camo.githubusercontent.com/66099400429d138e0fe99a6d7719ee9f4884103f8b74375e7f3f764aa6f0b10a/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134373837343635332d32666163386332372d653663642d346632372d613037372d3032656163623234373935302e706e67" alt="" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1764147874653-2fac8c27-e6cd-4f27-a077-02eacb247950.png" style="max-width: 100%;"></a></p>
<p>架构图看上去是输入image、输出文字的工作，但DeepSeek-OCR最终想讨论的其实是能否通过压缩输入token数来缓解计算开销的问题，所以在image输入之前，其实还有一个工作就是把text转化为image，然后才是对image做处理。</p>
<p>DeepSeek-OCR由encoder和decoder两个部分组成。encoder是光学压缩的重点，分为SAM和CLIP，中间包括了卷积层进行下采样。</p>
<p>Encoder的网络架构，基于一作（Haoran Wei）于23年发表的paper《Vary: Scaling up the Vision Vocabulary for Large Vision-Language Models》。Vary的核心想法是用SAM和CLIP这2个image encoder抽取feature，最后融合起来。DeepSeek-OCR的改进在于，把Vary的并行结构改成了串行结构，先用SAM和卷积层对image做处理，再把输出送到CLIP进行处理。</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/3398a78ac9f4e597befdd8cadb9140bc0c1525924a00979f1b6c638b0c94ab93/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134373838333537332d35396332363532352d343764612d343133642d386130662d3861636239333532336235622e706e67"><img src="https://camo.githubusercontent.com/3398a78ac9f4e597befdd8cadb9140bc0c1525924a00979f1b6c638b0c94ab93/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134373838333537332d35396332363532352d343764612d343133642d386130662d3861636239333532336235622e706e67" alt="" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1764147883573-59c26525-47da-413d-8a0f-8acb93523b5b.png" style="max-width: 100%;"></a></p>
<h2>2.1 DeepEncoder</h2>
<h3>2.1.1 SAM-base</h3>
<p>SAM-base是视觉感知特征提取模块。</p>
<p>原始图像被切分为n个16*16的patch（图像）后，输入给SAM。SAM使用window attention来提取局部特征，SAM输出vision tokens。</p>
<p>这里的SAM-base并不是原版的SAM，而是何恺明提出的ViTDet：把global attention改成了local attention（也就是window attention），这样做的好处是在尽可能高的输入分辨率下，用尽可能少的activation memory和vision tokens，同时不损失关键信息。</p>
<p>因为不同的文档和图像复杂度差异巨大，DeepSeek-OCR 希望在“清晰度”和“计算成本”之间自由切换，从而在不牺牲性能的前提下实现更高效的推理，所以引入了多分辨率支持。对于简单的图，就用小的分辨率转换为较少的tokens；对于复杂的图，就转换为较多的tokens。这个多分辨率支持的机制与24年的 NeurIPS《Visual Autoregressive Modeling》也很相似，这篇paper采用了“由粗到细”的多尺度预测方式——先低分辨率生成，再逐步提升清晰度。同时也展示了相同的数据压缩路径：512×512 图像可编码为 64 个 token，256×256 甚至能压到 32 个 token。</p>
<h3>2.1.2 Conv(down-sample)</h3>
<p>下采样用2 stride=2的卷积层，把SAM输出的特征图的空间分辨率缩小16倍。</p>
<blockquote>
<p>stride=2：卷积核每次移动 2 个像素，相当于<strong>隔一个取一个</strong>；输出的宽、高都会变成原来的一半，面积缩小为原来的 1/4。</p>
<p>2个卷积层：(1/4)×(1/4)=1/16</p>
</blockquote>
<p>如果输入是SAM提供的 4096个 vision tokens，经过下采样后vision tokens可以降到256个。通过减少tokens，使整体激活内存处于可控状态，让高分辨率图像处理变得可行。</p>
<h3>2.1.3 CLIP-large</h3>
<p>CLIP是视觉知识特征提取模块。SAM获取下采样后的vision tokens，通过global attention让每个vision token和所有其他token全局交互，这样既有SAM的局部信息计算，又有CLIP提供的全局信息计算。</p>
<h3>2.1.4 总结</h3>
<p>通过上述的设计，DeepEncoder能够<strong>处理高分辨率图像</strong>，且<strong>在高分辨率下激活内存低</strong>，这是因为SAM 使用 window attention，每个窗口独立计算，只在局部区域传播信息，因此降低了激活内存，不会因为分辨率升高而让注意力矩阵爆炸。另外，通过卷积层的下采样，也<strong>减少了vision token数量</strong>，可以将4096个tokens可以减少到256个。</p>
<h2>2.2 DeepSeekMoE</h2>
<p>decoder则使用了DeepSeek-MoE模型。</p>
<p>DeepSeek-MoE输入是：</p>
<ol>
<li>encoder输出的视觉特征向量序列；</li>
<li>文字prompt，指示模型要执行哪种 OCR / 文档解析任务。</li>
</ol>
<p>输出是下一个token的概率分布（文本序列）。</p>
<p>参数量而言，虽然MoE是3B的模型，但推理时实际激活参数只有570M，激活了64个路由专家中的6个，以及2个共享专家。</p>
<h1>3 Training</h1>
<h2>3.1 Training Data</h2>
<p>训练模型的数据约70%是OCR数据，OCR1.0主要是如场景图像OCR和文档OCR等传统的OCR任务，这其中包含了互联网的多语言 PDF 数据，使DeepSeek-OCR自然具备了多语言识别的能力。OCR2.0包括了图表、化学公式和平面几何解析数据等复杂的人工图像。</p>
<h2>3.2 Training Pipelines</h2>
<p>首先，单独训练DeepEncoder，做next-token prediction。</p>
<p>然后，用Pipeline Parallelism训练deepseek-ocr整体，使用了HAI-LLM 框架。在这一阶段里冻结了SAM和卷积层的参数，继续训练CLIP和Decoder部分。</p>
<p>最后，如果需要进一步训练gundam-master模式的话，就在已预训练的DeepSeek-OCR模型上使用600万条采样数据继续训练。Gundam-Master模式是其中一种超高分辨率动态输入模式，用于处理如报纸、大幅面PDF等复杂的文档，其核心设计目标是在保持高OCR精度的同时，避免因单图分辨率过大而导致显存溢出或训练不稳定。</p>
<h1>4 分析</h1>
<h2>4.1 Vision-text Compression Study</h2>
<p>回到一开始提出的问题：一张图像（视觉表示）到底能压缩多少文字信息而不丢失内容？实验的输入是不同数量级的text，用precision衡量模型输出的text和真实text的匹配度。实验分为2种情况，分别是将text压缩后输出64个vision tokens，以及压缩后输出100个vision tokens。</p>
<p>实验用了Fox做了benchmark。当600-700个text tokens的文档压缩为64个vision tokens后，压缩率达到10.5倍，精度仍在96.5%；而如果是压缩率为10倍以下的情况精度可达97%。但是当压缩比超过10倍时，性能开始下降。</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/4f2e42e1d1143ecf757105fb32b8e0005ed2d385a54e0c5d89f650ae806cc2ab/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383235303637382d36326166373634372d353333642d343831392d613239392d3262323931373336336239372e706e67"><img src="https://camo.githubusercontent.com/4f2e42e1d1143ecf757105fb32b8e0005ed2d385a54e0c5d89f650ae806cc2ab/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383235303637382d36326166373634372d353333642d343831392d613239392d3262323931373336336239372e706e67" alt="" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1764148250678-62af7647-533d-4819-a299-2b2917363b97.png" style="max-width: 100%;"></a></p>
<h2>4.2 OCR Practical Performance</h2>
<p>这个实验用了OmniDocBench做benchmark。</p>
<p>实验把当前OCR模型分为3类。第1类是传统的OCR模型，是多步骤的pipeline，比如第一步做layout detection（哪些地方有文字，哪些地方是图表），第二步是对每个部分做文字识别，这里每一步都可能出错，且上一步的错误可能会影响下一步的精度。第2类是end2end的主流的VLM，其中表现最好的是小红书的dots.ocr。第3类就是DeepSeek-OCR系列模型。</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/9b2a415f4ab109cae1f99f5cd8449c2997ce6d1883b3cae9b61f80b5d400697d/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383236383434342d61656438373937632d366166312d343135322d396235652d6365366336613638613831652e706e67"><img src="https://camo.githubusercontent.com/9b2a415f4ab109cae1f99f5cd8449c2997ce6d1883b3cae9b61f80b5d400697d/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383236383434342d61656438373937632d366166312d343135322d396235652d6365366336613638613831652e706e67" alt="" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1764148268444-aed8797c-6af1-4152-9b5e-ce6c6a68a81e.png" style="max-width: 100%;"></a></p>
<p>下图横坐标表示平均使用的tokens，越靠右越少；纵坐标表示模型的表现（edit performance），越往上越好。DeepSeek-OCR虽然性能上没有非常夸张地超过SOTA，但是它能以更少的vision token 达到近似或以微弱优势超过主流 OCR 模型的精度。</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/c4eaa78ddc4bb51b2f480a4dbd6c929acce015b6284a93393956fa3f66b0b9ce/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383237333438302d33653161373930642d393038632d343463322d623334382d3339616136623834396661652e706e67"><img src="https://camo.githubusercontent.com/c4eaa78ddc4bb51b2f480a4dbd6c929acce015b6284a93393956fa3f66b0b9ce/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383237333438302d33653161373930642d393038632d343463322d623334382d3339616136623834396661652e706e67" alt="" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1764148273480-3e1a790d-908c-44c2-b348-39aa6b849fae.png" style="max-width: 100%;"></a></p>
<h2>4.3 遗忘机制</h2>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/d4caa910df7545d2ff94dd14225a0ac900c993ce18b09f8373df25f4bdf44e70/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383238373131312d39343037343937362d393733302d343061372d393535352d3964326635386661343661312e706e67"><img src="https://camo.githubusercontent.com/d4caa910df7545d2ff94dd14225a0ac900c993ce18b09f8373df25f4bdf44e70/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383238373131312d39343037343937362d393733302d343061372d393535352d3964326635386661343661312e706e67" alt="" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1764148287111-94074976-9730-40a7-9555-9d2f58fa46a1.png" style="max-width: 100%;"></a></p>
<p>这篇paper的创新点并不只在OCR，或者说，这篇paper在做完OCR的工作后升华到了人类记忆的遗忘机制，认为光学压缩可以类比遗忘机制，而这种遗忘机制可以处理长context。（或许可以从这篇paper学习一下如何拔高立意hh）</p>
<p>注意论文讨论的压缩对象是token，而不单纯是information。如果是information的话，对context进行summary或者摘要也可以做到“压缩”。vision token可以被下采样，而text token难以做到：一句话如果少了10%的单词，就已经难以理解了。</p>
<p>讨论：对于最近的对话，可以把它转换为较高分辨率的picture；随着对话的进行，该对话逐渐变得古老，我们可以压缩这个古老的picture，为memory腾出更多的空间。这就类似于人类的记忆，对于刚刚发生的事情还能记住很多细节，而童年的回忆只剩下一个模糊的轮廓。</p>
<h1>5 讨论</h1>
<p>论文虽然论证了deepseek-ocr压缩的效果很好，但这里主要讨论一下Encoder压缩是否真实。这个问题在社区受到一定程度的质疑。</p>
<ol>
<li>
<p>未具体讨论 decoder 的贡献</p>
<p>我在使用deepseek-ocr的时候发现，输入的图片内容是“汉字的序顺并不定一影阅响读”，但是OCR的推理结果是“并不一定一影响阅读”。在InternVL上也会出现类似的情况（见后），这或许是end2end的OCR模型都存在的问题：不仅是识别字符，还包括了语言理解或者纠正机制。我个人觉得这是decoder在自动补全语言信息的体现。</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/afe43bdb422aa059d0b7fc599b7c33d0d5613370a413277619370d97e56d65a2/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383338313131382d32343331363562382d656139642d343139312d623530382d3133316234343464616464622e706e67"><img src="https://camo.githubusercontent.com/afe43bdb422aa059d0b7fc599b7c33d0d5613370a413277619370d97e56d65a2/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383338313131382d32343331363562382d656139642d343139312d623530382d3133316234343464616464622e706e67" alt="" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1764148381118-243165b8-ea9d-4191-b508-131b444daddb.png" style="max-width: 100%;"></a></p>
<p>如果 vision token 缺少某些细节，decoder 完全可能通过语言学prior自动补上，使得结果看起来几乎无误。这是因为LLM 具有强语言先验（Language Priors），例如猜字、补全句子、根据上下文恢复缺失的信息。所以重建精度高不代表encoder存储了全部信息。</p>
</li>
<li>
<p>缺少 stage 1 的消融实验</p>
<p>论文中，Stage1用轻量decoder训练encoder；Stage2在此基础上训练decoder，并报告了Satge 2之后的模型效果，但唯独没有报告Stage1的encoder效果。如果压缩是真实的，那么Stage1应该表现很好。但文章中没给出对应的结果，所以：无法证明encoder压缩是真实的，或者说，压缩能力是来自encoder的高质量表征，还是来自 Stage2中强decoder的语言补全能力，无法被直接区分。</p>
<p>如果这里要做消融实验的话，我的一个想法是：在相同视觉压缩率（如64或100vision tokens）下，分别测量 Stage1和Stage2的OCR精度。若Stage1已经能够在高压缩率下取得接近Stage2的性能，则说明encoder的压缩能力占主导；反之，若Stage1明显劣于Stage2，则表明最终的精度有相当部分来自decoder的语言先验与补全能力，而非encoder本身的压缩表示质量。</p>
</li>
</ol>
<hr>
<h1>【附录】end2end的OCR模型“并不定一影阅响读”的测试情况</h1>
<p>InternVL3.5</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/de83ab79ffa7ad68cdf65079183f1bda059b994df3de3bd513674bfe89d87a9a/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383438393530342d34376536393830662d633339372d343936362d383862382d3338303835336263643833372e706e67"><img src="https://camo.githubusercontent.com/de83ab79ffa7ad68cdf65079183f1bda059b994df3de3bd513674bfe89d87a9a/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383438393530342d34376536393830662d633339372d343936362d383862382d3338303835336263643833372e706e67" alt="" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1764148489504-47e6980f-c397-4966-88b8-380853bcd837.png" style="max-width: 100%;"></a></p>
<p>Qwen3-VL</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/ba596714654823156ea10deffadacdebaa643b01b1f11d9ce86747db9163db67/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383439363538322d61343834646230302d356134612d343563392d613732332d3333623461646366383533662e706e67"><img src="https://camo.githubusercontent.com/ba596714654823156ea10deffadacdebaa643b01b1f11d9ce86747db9163db67/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383439363538322d61343834646230302d356134612d343563392d613732332d3333623461646366383533662e706e67" alt="" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1764148496582-a484db00-5a4a-45c9-a723-33b4adcf853f.png" style="max-width: 100%;"></a></p>
<p>Dots.ocr</p>
<p><a target="_blank" rel="noopener noreferrer nofollow" href="https://camo.githubusercontent.com/08c69dbe0572a24e4a577b3d353115df7fc04db2f55e8e43a594b9b660c4737a/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383530303339312d36333835643865662d643934642d346338302d383365352d3962316261633930623832312e706e67"><img src="https://camo.githubusercontent.com/08c69dbe0572a24e4a577b3d353115df7fc04db2f55e8e43a594b9b660c4737a/68747470733a2f2f63646e2e6e6c61726b2e636f6d2f79757175652f302f323032352f706e672f33393033393638382f313736343134383530303339312d36333835643865662d643934642d346338302d383365352d3962316261633930623832312e706e67" alt="" data-canonical-src="https://cdn.nlark.com/yuque/0/2025/png/39039688/1764148500391-6385d8ef-d94d-4c80-83e5-9b1bac90b821.png" style="max-width: 100%;"></a></p></div>
<div style="font-size:small;margin-top:8px;float:right;"></div>

<button class="btn btn-block" type="button" onclick="openComments()" id="cmButton">评论</button>
<div class="comments" id="comments"></div>

</div>
    <div id="footer"><div id="footer1">Copyright © <span id="copyrightYear"></span> <a href="https://qiakachi.github.io">QiakaChi's Note</a></div>
<div id="footer2">
    <span id="runday"></span><span>Powered by <a href="https://meekdai.com/Gmeek.html" target="_blank">Gmeek</a></span>
</div>

<script>
var now=new Date();
document.getElementById("copyrightYear").innerHTML=now.getFullYear();

if(""!=""){
    var startSite=new Date("");
    var diff=now.getTime()-startSite.getTime();
    var diffDay=Math.floor(diff/(1000*60*60*24));
    document.getElementById("runday").innerHTML="网站运行"+diffDay+"天"+" • ";
}
</script></div>
</body>
<script>
var IconList={'sun': 'M8 10.5a2.5 2.5 0 100-5 2.5 2.5 0 000 5zM8 12a4 4 0 100-8 4 4 0 000 8zM8 0a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0V.75A.75.75 0 018 0zm0 13a.75.75 0 01.75.75v1.5a.75.75 0 01-1.5 0v-1.5A.75.75 0 018 13zM2.343 2.343a.75.75 0 011.061 0l1.06 1.061a.75.75 0 01-1.06 1.06l-1.06-1.06a.75.75 0 010-1.06zm9.193 9.193a.75.75 0 011.06 0l1.061 1.06a.75.75 0 01-1.06 1.061l-1.061-1.06a.75.75 0 010-1.061zM16 8a.75.75 0 01-.75.75h-1.5a.75.75 0 010-1.5h1.5A.75.75 0 0116 8zM3 8a.75.75 0 01-.75.75H.75a.75.75 0 010-1.5h1.5A.75.75 0 013 8zm10.657-5.657a.75.75 0 010 1.061l-1.061 1.06a.75.75 0 11-1.06-1.06l1.06-1.06a.75.75 0 011.06 0zm-9.193 9.193a.75.75 0 010 1.06l-1.06 1.061a.75.75 0 11-1.061-1.06l1.06-1.061a.75.75 0 011.061 0z', 'moon': 'M9.598 1.591a.75.75 0 01.785-.175 7 7 0 11-8.967 8.967.75.75 0 01.961-.96 5.5 5.5 0 007.046-7.046.75.75 0 01.175-.786zm1.616 1.945a7 7 0 01-7.678 7.678 5.5 5.5 0 107.678-7.678z', 'sync': 'M1.705 8.005a.75.75 0 0 1 .834.656 5.5 5.5 0 0 0 9.592 2.97l-1.204-1.204a.25.25 0 0 1 .177-.427h3.646a.25.25 0 0 1 .25.25v3.646a.25.25 0 0 1-.427.177l-1.38-1.38A7.002 7.002 0 0 1 1.05 8.84a.75.75 0 0 1 .656-.834ZM8 2.5a5.487 5.487 0 0 0-4.131 1.869l1.204 1.204A.25.25 0 0 1 4.896 6H1.25A.25.25 0 0 1 1 5.75V2.104a.25.25 0 0 1 .427-.177l1.38 1.38A7.002 7.002 0 0 1 14.95 7.16a.75.75 0 0 1-1.49.178A5.5 5.5 0 0 0 8 2.5Z', 'home': 'M6.906.664a1.749 1.749 0 0 1 2.187 0l5.25 4.2c.415.332.657.835.657 1.367v7.019A1.75 1.75 0 0 1 13.25 15h-3.5a.75.75 0 0 1-.75-.75V9H7v5.25a.75.75 0 0 1-.75.75h-3.5A1.75 1.75 0 0 1 1 13.25V6.23c0-.531.242-1.034.657-1.366l5.25-4.2Zm1.25 1.171a.25.25 0 0 0-.312 0l-5.25 4.2a.25.25 0 0 0-.094.196v7.019c0 .138.112.25.25.25H5.5V8.25a.75.75 0 0 1 .75-.75h3.5a.75.75 0 0 1 .75.75v5.25h2.75a.25.25 0 0 0 .25-.25V6.23a.25.25 0 0 0-.094-.195Z', 'github': 'M8 0c4.42 0 8 3.58 8 8a8.013 8.013 0 0 1-5.45 7.59c-.4.08-.55-.17-.55-.38 0-.27.01-1.13.01-2.2 0-.75-.25-1.23-.54-1.48 1.78-.2 3.65-.88 3.65-3.95 0-.88-.31-1.59-.82-2.15.08-.2.36-1.02-.08-2.12 0 0-.67-.22-2.2.82-.64-.18-1.32-.27-2-.27-.68 0-1.36.09-2 .27-1.53-1.03-2.2-.82-2.2-.82-.44 1.1-.16 1.92-.08 2.12-.51.56-.82 1.28-.82 2.15 0 3.06 1.86 3.75 3.64 3.95-.23.2-.44.55-.51 1.07-.46.21-1.61.55-2.33-.66-.15-.24-.6-.83-1.23-.82-.67.01-.27.38.01.53.34.19.73.9.82 1.13.16.45.68 1.31 2.69.94 0 .67.01 1.3.01 1.49 0 .21-.15.45-.55.38A7.995 7.995 0 0 1 0 8c0-4.42 3.58-8 8-8Z'};
var utterancesLoad=0;

let themeSettings={
    "dark": ["dark","moon","#00f0ff","dark-blue"],
    "light": ["light","sun","#ff5000","github-light"],
    "auto": ["auto","sync","","preferred-color-scheme"]
};
function changeTheme(mode, icon, color, utheme){
    document.documentElement.setAttribute("data-color-mode",mode);
    document.getElementById("themeSwitch").setAttribute("d",value=IconList[icon]);
    document.getElementById("themeSwitch").parentNode.style.color=color;
    if(utterancesLoad==1){utterancesTheme(utheme);}
}
function modeSwitch(){
    let currentMode=document.documentElement.getAttribute('data-color-mode');
    let newMode = currentMode === "light" ? "dark" : currentMode === "dark" ? "auto" : "light";
    localStorage.setItem("meek_theme", newMode);
    if(themeSettings[newMode]){
        changeTheme(...themeSettings[newMode]);
    }
}
function utterancesTheme(theme){
    const message={type:'set-theme',theme: theme};
    const iframe=document.getElementsByClassName('utterances-frame')[0];
    iframe.contentWindow.postMessage(message,'https://utteranc.es');
}
if(themeSettings[theme]){changeTheme(...themeSettings[theme]);}
console.log("\n %c Gmeek last https://github.com/Meekdai/Gmeek \n","padding:5px 0;background:#02d81d;color:#fff");
</script>

<script>
document.getElementById("pathHome").setAttribute("d",IconList["home"]);
document.getElementById("pathIssue").setAttribute("d",IconList["github"]);



function openComments(){
    cm=document.getElementById("comments");
    cmButton=document.getElementById("cmButton");
    cmButton.innerHTML="loading";
    span=document.createElement("span");
    span.setAttribute("class","AnimatedEllipsis");
    cmButton.appendChild(span);

    script=document.createElement("script");
    script.setAttribute("src","https://utteranc.es/client.js");
    script.setAttribute("repo","QiakaChi/qiakachi.github.io");
    script.setAttribute("issue-term","title");
    
    if(localStorage.getItem("meek_theme")=="dark"){script.setAttribute("theme","dark-blue");}
    else if(localStorage.getItem("meek_theme")=="light") {script.setAttribute("theme","github-light");}
    else{script.setAttribute("theme","preferred-color-scheme");}
    
    script.setAttribute("crossorigin","anonymous");
    script.setAttribute("async","");
    cm.appendChild(script);

    int=self.setInterval("iFrameLoading()",200);
}

function iFrameLoading(){
    var utterances=document.getElementsByClassName('utterances');
    if(utterances.length==1){
        if(utterances[0].style.height!=""){
            utterancesLoad=1;
            int=window.clearInterval(int);
            document.getElementById("cmButton").style.display="none";
            console.log("utterances Load OK");
        }
    }
}



</script>


</html>
